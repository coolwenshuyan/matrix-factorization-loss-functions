#!/usr/bin/env python3
# evaluate\evaluate_optimal_parameters.py
"""
ä½¿ç”¨æœ€ä¼˜å‚æ•°è¿›è¡Œæ¨¡å‹è¯„ä¼°å¹¶ä¿å­˜ç»“æœ - æ”¯æŒæ‰¹é‡æŸå¤±å‡½æ•°è¯„ä¼°
"""
import os
import sys
import json
import time
import numpy as np
from pathlib import Path
import pandas as pd
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)  # ä¸Šä¸€çº§ç›®å½•æ‰æ˜¯é¡¹ç›®æ ¹ç›®å½•
sys.path.append(project_root)

print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
print(f"å½“å‰ç›®å½•: {current_dir}")

# å¯¼å…¥æ¨¡å— - å¿…é¡»åœ¨æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.pathä¹‹å
from utils.sms_notification import send_sms_notification
from data.data_manager import DataManager
from src.models.mf_sgd import MatrixFactorizationSGD
from src.models.initializers import NormalInitializer
from src.models.regularizers import L2Regularizer
# å¯¼å…¥æ‰€æœ‰æŸå¤±å‡½æ•°
from src.losses.standard import L1Loss, L2Loss
from src.losses.hpl import HybridPiecewiseLoss
from src.losses.robust import HuberLoss, LogcoshLoss
from src.losses.sigmoid import SigmoidLikeLoss

# å¯¼å…¥æ ‡å‡†evaluationæ¨¡å—
# from src.evaluation import (
#     MAE, RMSE, MSE, R2Score,
#     HitRate, Precision, Recall, MAP, NDCG, MRR,
#     CatalogCoverage, UserCoverage, Diversity,
#     Novelty, Serendipity, MetricFactory
# )
from src.evaluation.ranking import TopKGenerator
from src.evaluation.evaluator import ModelEvaluator

print("æˆåŠŸå¯¼å…¥æ‰€æœ‰æ¨¡å—")


class OptimalParameterEvaluator:
    """ä½¿ç”¨æœ€ä¼˜å‚æ•°è¯„ä¼°æ¨¡å‹æ€§èƒ½"""

    def __init__(self, dataset_name, dataset_file):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.dataset_name = dataset_name
        self.dataset_file = dataset_file
        self.data_manager = self._setup_data_manager()

        # è·å–æ•°æ®é›†æ˜¾ç¤ºåç§°
        self.dataset_display_name = self._get_dataset_display_name()

        # å›ºå®šä¿å­˜åˆ°å½“å‰ç›®å½•çš„evaluation_resultsä¸­ï¼Œä»¥æ•°æ®é›†åå­—å‘½å
        self.results_dir = Path(current_dir) / 'evaluation_results' / self.dataset_name
        self.results_dir.mkdir(parents=True, exist_ok=True)
        print(f"ç»“æœå°†ä¿å­˜åˆ°: {self.results_dir}")

    def _get_dataset_display_name(self):
        """è·å–æ•°æ®é›†æ˜¾ç¤ºåç§°"""
        if hasattr(self.data_manager.dataset, 'name'):
            return self.data_manager.dataset.name
        else:
            # ç”Ÿæˆæ˜¾ç¤ºåç§°çš„æ˜ å°„
            display_names = {
                'ml100k': 'MovieLens 100K',
                'movielens100k': 'MovieLens 100K',
                'netflix': 'Netflix',
                'filmtrust': 'FilmTrust',
                'ciaodvd': 'CiaoDVD',
                'epinions': 'Epinions',
                'amazon': 'Amazon',
                'movielens1m': 'MovieLens 1M',
                'movietweetings': 'MovieTweetings'
            }
            return display_names.get(self.dataset_name.lower(), self.dataset_name.title())

    def _setup_data_manager(self):
        """è®¾ç½®æ•°æ®ç®¡ç†å™¨"""
        print(f"åŠ è½½æ•°æ®é›†: {self.dataset_name} ({self.dataset_file})")

        # åˆ›å»ºé…ç½®
        config = {
            'random_seed': 42,
            'train_ratio': 0.8,
            'val_ratio': 0.1,
            'test_ratio': 0.1,
            'center_data': True,
            'ensure_user_in_train': True
        }

        # åˆ›å»ºæ•°æ®ç®¡ç†å™¨
        data_manager = DataManager(config)

        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ï¼ˆé“¾å¼è°ƒç”¨ï¼‰
        data_manager.load_dataset_from_path(self.dataset_file, self.dataset_name).preprocess()

        # æ‰“å°æ•°æ®æ‘˜è¦
        data_manager.print_summary()

        # è·å–æ•°æ®é›†åˆ’åˆ†
        train_data, val_data, test_data = data_manager.get_splits()

        print(f"æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"  è®­ç»ƒé›†: {len(train_data)} æ¡è®°å½•")
        print(f"  éªŒè¯é›†: {len(val_data)} æ¡è®°å½•")
        print(f"  æµ‹è¯•é›†: {len(test_data)} æ¡è®°å½•")

        return data_manager

    def load_config_from_file(self, config_file):
        """ä»é…ç½®æ–‡ä»¶åŠ è½½å®Œæ•´é…ç½®ä¿¡æ¯"""
        print(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
        with open(config_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # æå–æ•°æ®é›†ä¿¡æ¯
        dataset_info = data.get('dataset_info', {})
        dataset_name = dataset_info.get('dataset_name', '')
        dataset_display_name = dataset_info.get('dataset_display_name', '')

        # æå–æœ€ä¼˜é…ç½®
        if 'results' in data and 'best_config' in data['results']:
            best_config = data['results']['best_config']
            best_score = data['results'].get('best_score', 0)

            # æŸ¥æ‰¾æœ€ä½³è¯•éªŒçš„trial_id
            trial_id = 'N/A'
            if 'optimizer_results' in data['results'] and 'all_trials' in data['results']['optimizer_results']:
                all_trials = data['results']['optimizer_results']['all_trials']
                for trial in all_trials:
                    if abs(trial.get('score', 0) - best_score) < 1e-10:  # æµ®ç‚¹æ•°æ¯”è¾ƒ
                        trial_id = trial.get('trial_id', 'N/A')
                        break
        elif 'best_config' in data:
            best_config = data['best_config']
            trial_id = data.get('best_trial_id', 'N/A')
        else:
            raise ValueError("æ— æ³•ä»æ–‡ä»¶ä¸­æå–æœ€ä¼˜é…ç½®")

        return {
            'dataset_name': dataset_name,
            'dataset_display_name': dataset_display_name,
            'best_config': best_config,
            'trial_id': trial_id
        }

    @staticmethod
    def get_available_loss_functions():
        """è·å–å¯ç”¨çš„æŸå¤±å‡½æ•°åˆ—è¡¨"""
        return {
            'l1': {'class': L1Loss, 'name': 'L1æŸå¤± (Mean Absolute Error)', 'params': []},
            'l2': {'class': L2Loss, 'name': 'L2æŸå¤± (Mean Squared Error)', 'params': []},
            'hpl': {'class': HybridPiecewiseLoss, 'name': 'HPLæ··åˆåˆ†æ®µæŸå¤±',
                   'params': ['delta1', 'delta2', 'l_max', 'c_sigmoid']},
            'huber': {'class': HuberLoss, 'name': 'HuberæŸå¤±', 'params': ['delta']},
            'logcosh': {'class': LogcoshLoss, 'name': 'LogCoshæŸå¤±', 'params': []},
            'sigmoid': {'class': SigmoidLikeLoss, 'name': 'Sigmoid-likeæŸå¤±',
                       'params': ['alpha', 'l_max']}
        }

    @staticmethod
    def display_loss_functions():
        """æ˜¾ç¤ºå¯ç”¨çš„æŸå¤±å‡½æ•°"""
        loss_functions = OptimalParameterEvaluator.get_available_loss_functions()
        print("\n" + "="*60)
        print("ğŸ“Š å¯ç”¨æŸå¤±å‡½æ•°åˆ—è¡¨")
        print("="*60)
        for i, (key, info) in enumerate(loss_functions.items(), 1):
            print(f"[{i}] {key.upper()}: {info['name']}")
        print(f"[0] è¿è¡Œæ‰€æœ‰æŸå¤±å‡½æ•°")
        print("="*60)

    def create_model(self, config, loss_type='l2'):
        """åˆ›å»ºæ¨¡å‹ - æ·»åŠ é”™è¯¯å¤„ç†"""
        try:
            # è·å–æŸå¤±å‡½æ•°ä¿¡æ¯
            loss_functions = self.get_available_loss_functions()

            if loss_type.lower() not in loss_functions:
                raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°ç±»å‹: {loss_type}")

            loss_info = loss_functions[loss_type.lower()]
            loss_class = loss_info['class']

            # éªŒè¯é…ç½®å‚æ•°
            required_params = loss_info.get('params', [])
            missing_params = []

            # æ ¹æ®æŸå¤±å‡½æ•°ç±»å‹åˆ›å»ºå®ä¾‹ï¼Œæ·»åŠ å‚æ•°éªŒè¯
            if loss_type.lower() == 'hpl':
                # éªŒè¯HPLæ‰€éœ€å‚æ•°
                hpl_params = ['delta1', 'delta2', 'l_max', 'c_sigmoid']
                for param in hpl_params:
                    if param not in config:
                        missing_params.append(param)

                if missing_params:
                    print(f"è­¦å‘Š: HPLæŸå¤±å‡½æ•°ç¼ºå°‘å‚æ•° {missing_params}ï¼Œä½¿ç”¨é»˜è®¤å€¼")

                loss_function = loss_class(
                    delta1=config.get('delta1', 0.5),
                    delta2=config.get('delta2', 1.5),
                    l_max=config.get('l_max', 4.0),
                    c_sigmoid=config.get('c_sigmoid', 1.0)
                )
            elif loss_type.lower() == 'huber':
                if 'delta' not in config:
                    print("è­¦å‘Š: HuberæŸå¤±å‡½æ•°ç¼ºå°‘deltaå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼1.0")
                loss_function = loss_class(delta=config.get('delta', 1.0))
            elif loss_type.lower() == 'sigmoid':
                sigmoid_params = ['alpha', 'l_max']
                for param in sigmoid_params:
                    if param not in config:
                        missing_params.append(param)

                if missing_params:
                    print(f"è­¦å‘Š: SigmoidæŸå¤±å‡½æ•°ç¼ºå°‘å‚æ•° {missing_params}ï¼Œä½¿ç”¨é»˜è®¤å€¼")

                loss_function = loss_class(
                    alpha=config.get('alpha', 1.0),
                    l_max=config.get('l_max', 3.0)
                )
            elif loss_type.lower() == 'l1':
                loss_function = loss_class(epsilon=config.get('epsilon', 1e-8))
            else:  # l2, logcoshç­‰æ— å‚æ•°æŸå¤±å‡½æ•°
                loss_function = loss_class()

            # è·å–ç”¨æˆ·å’Œç‰©å“æ•°é‡
            stats = self.data_manager.get_statistics()
            n_users = stats['n_users']
            n_items = stats['n_items']

            # éªŒè¯åŸºæœ¬é…ç½®å‚æ•°
            if 'latent_factors' not in config:
                print("è­¦å‘Š: ç¼ºå°‘latent_factorså‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼20")
            if 'learning_rate' not in config:
                print("è­¦å‘Š: ç¼ºå°‘learning_rateå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼0.01")
            if 'lambda_reg' not in config:
                print("è­¦å‘Š: ç¼ºå°‘lambda_regå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼0.01")

            print(f"\nå…³é”®ç»Ÿè®¡ä¿¡æ¯:")
            print(f"ç”¨æˆ·æ•°: {stats['n_users']}")
            print(f"ç‰©å“æ•°: {stats['n_items']}")
            print(f"ç¨€ç–åº¦: {stats['sparsity']:.4f}")
            print(f"å¹³å‡è¯„åˆ†: {stats['rating_mean']:.2f}")
            print(f"ä½¿ç”¨æŸå¤±å‡½æ•°: {loss_info['name']}")

            # åˆ›å»ºæ¨¡å‹
            model = MatrixFactorizationSGD(
                n_users=n_users,
                n_items=n_items,
                n_factors=config.get('latent_factors', 20),
                learning_rate=config.get('learning_rate', 0.01),
                loss_function=loss_function,
                regularizer=L2Regularizer(lambda_reg=config.get('lambda_reg', 0.01)),
                use_bias=True,
                global_mean=self.data_manager.global_mean or 0.0
            )

            # åˆå§‹åŒ–æ¨¡å‹å‚æ•°
            initializer = NormalInitializer(mean=0, std=0.1)
            model.initialize_parameters(initializer)

            return model

        except Exception as e:
            print(f"åˆ›å»ºæ¨¡å‹æ—¶å‡ºé”™: {e}")
            raise

    def train_and_evaluate(self, model, n_epochs=50):
        """è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ - ä½¿ç”¨æ ‡å‡†evaluationæ¨¡å—"""
        try:
            print("="*60)
            print("å¼€å§‹è®­ç»ƒå’Œè¯„ä¼°")
            #  æ·»åŠ è°ƒè¯•æ­¥éª¤
            print("\n  é¢„è®­ç»ƒè°ƒè¯•...")
            self.debug_recommendation_generation(model)

            if not hasattr(self.data_manager, 'train_data') or self.data_manager.train_data is None:
                raise ValueError("è®­ç»ƒæ•°æ®æœªåŠ è½½")

            if not hasattr(self.data_manager, 'val_data') or self.data_manager.val_data is None:
                raise ValueError("éªŒè¯æ•°æ®æœªåŠ è½½")

            # æ‰“å°æ¨¡å‹é…ç½®å‚æ•°
            print("\n" + "="*60)
            print("æ¨¡å‹é…ç½®å‚æ•°:")
            print("="*60)
            print(f"ç”¨æˆ·æ•°é‡: {model.n_users}")
            print(f"ç‰©å“æ•°é‡: {model.n_items}")
            print(f"æ½œåœ¨å› å­æ•°: {model.n_factors}")
            print(f"å­¦ä¹ ç‡: {model.learning_rate}")
            print(f"ä½¿ç”¨åå·®é¡¹: {model.use_bias}")
            print(f"å…¨å±€å‡å€¼: {model.global_mean}")
            print(f"æ•°æ®ç±»å‹: {model.dtype}")

            # æŸå¤±å‡½æ•°ä¿¡æ¯
            if hasattr(model, 'loss_function') and model.loss_function is not None:
                print(f"æŸå¤±å‡½æ•°: {model.loss_function.__class__.__name__}")
                if hasattr(model.loss_function, 'get_config'):
                    loss_config = model.loss_function.get_config()
                    for key, value in loss_config.items():
                        if key not in ['name', 'class']:
                            print(f"  {key}: {value}")
            else:
                print("æŸå¤±å‡½æ•°: é»˜è®¤MSE")

            # æ­£åˆ™åŒ–å™¨ä¿¡æ¯
            if hasattr(model, 'regularizer') and model.regularizer is not None:
                print(f"æ­£åˆ™åŒ–å™¨: {model.regularizer.__class__.__name__}")
                if hasattr(model.regularizer, 'lambda_reg'):
                    print(f"  æ­£åˆ™åŒ–ç³»æ•°: {model.regularizer.lambda_reg}")
            else:
                print("æ­£åˆ™åŒ–å™¨: æ— ")

            print(f"è®­ç»ƒè½®æ•°: {n_epochs}")
            print("="*60)

            start_time = time.time()

            # è®­ç»ƒæ¨¡å‹
            model.fit(
                train_data=self.data_manager.train_data,
                val_data=self.data_manager.val_data,
                n_epochs=n_epochs,
                verbose=1,
                early_stopping_patience=None  # ç¦ç”¨æ—©åœ
            )

            train_time = time.time() - start_time
            print(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {train_time:.2f}ç§’")

            # è®­ç»ƒåå†æ¬¡è°ƒè¯•
            print("\n è®­ç»ƒåè°ƒè¯•...")
            self.debug_recommendation_generation(model)
            # è¯„ä¼°æ¨¡å‹
            print("è¯„ä¼°æ¨¡å‹...")
            evaluation_results = self.evaluate_model(model)
            evaluation_results['training_time'] = train_time

            return evaluation_results

        except Exception as e:
            print(f"è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

            # è¿”å›åŸºæœ¬ç»“æœï¼Œé¿å…ç¨‹åºå´©æºƒ
            return {
                'training_time': 0,
                'mae': float('inf'),
                'rmse': float('inf'),
                'error': str(e)
            }

    def evaluate_model(self, model):
        """å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ - ä½¿ç”¨æ ‡å‡†evaluationæ¨¡å—"""
        print("ä½¿ç”¨æ ‡å‡†evaluationæ¨¡å—è¯„ä¼°æ¨¡å‹...")

        try:
            # åˆ›å»ºModelEvaluatorå®ä¾‹
            model_evaluator = ModelEvaluator(
                model=model,
                test_data=self.data_manager.test_data,
                k_values=[5, 10, 20],
                metrics=['mae', 'rmse', 'mse', 'r2', 'hr', 'precision', 'recall', 'map', 'ndcg', 'mrr'],
                n_users_sample=100  # ğŸ”§ å…ˆé™åˆ¶ä¸º100ä¸ªç”¨æˆ·è¿›è¡Œæµ‹è¯•
            )

            # æ‰§è¡Œå®Œæ•´è¯„ä¼°
            results = model_evaluator.evaluate_all()

            # è®¡ç®—é¢å¤–çš„å¤šæ ·æ€§æŒ‡æ ‡
            additional_results = self._calculate_diversity_metrics(model)
            results.update(additional_results)

            return results

        except Exception as e:
            print(f"è¯„ä¼°æ¨¡å‹æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

            # è¿”å›åŸºæœ¬ç»“æœ
            return {
                'mae': float('inf'),
                'rmse': float('inf'),
                'error': str(e)
            }

    def _calculate_diversity_metrics(self, model):
        """è®¡ç®—å¤šæ ·æ€§ç›¸å…³æŒ‡æ ‡ - ä¿®å¤ç‰ˆæœ¬"""
        results = {}

        try:
            # ğŸ”§ åˆ›å»ºTopKGeneratorå®ä¾‹
            topk_gen = TopKGenerator(model)

            # è·å–æµ‹è¯•é›†ç”¨æˆ·ï¼ˆé™åˆ¶æ•°é‡ä»¥æé«˜æ•ˆç‡ï¼‰
            unique_users = np.unique(self.data_manager.test_data[:, 0].astype(int))

            if len(unique_users) > 200:  # é™åˆ¶ç”¨æˆ·æ•°é‡
                np.random.seed(42)
                unique_users = np.random.choice(unique_users, 200, replace=False)

            print(f"è¯„ä¼° {len(unique_users)} ä¸ªç”¨æˆ·çš„å¤šæ ·æ€§æŒ‡æ ‡...")

            # è·å–è®­ç»ƒé›†ç”¨æˆ·ç‰©å“
            train_user_items = self._get_train_user_items()

            # ç”Ÿæˆæ¨èåˆ—è¡¨
            all_recommendations = []
            successful_users = 0
            total_items = model.n_items

            for user_id in unique_users:
                try:
                    # è·å–è¯¥ç”¨æˆ·çš„å·²çŸ¥ç‰©å“
                    known_items = set(train_user_items.get(user_id, []))

                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥å€™é€‰ç‰©å“æ•°é‡
                    candidate_items = total_items - len(known_items)

                    if candidate_items < 20:  # å¦‚æœå€™é€‰ç‰©å“å¤ªå°‘
                        # ä¸æ’é™¤å·²çŸ¥ç‰©å“ï¼Œæˆ–åªæ’é™¤éƒ¨åˆ†
                        print(f"ç”¨æˆ·{user_id}å€™é€‰ç‰©å“ä¸è¶³({candidate_items})ï¼Œé™ä½æ’é™¤ç­–ç•¥")
                        exclude_items = None  # ä¸æ’é™¤ä»»ä½•ç‰©å“
                    else:
                        exclude_items = known_items

                    # ç”Ÿæˆæ¨è
                    user_recs = topk_gen.generate_top_k_for_user(
                        user_id=user_id,
                        k=10,
                        exclude_items=exclude_items
                    )

                    if len(user_recs) >= 5:  # ç¡®ä¿è‡³å°‘æœ‰5ä¸ªæ¨è
                        all_recommendations.extend(user_recs)
                        successful_users += 1
                    else:
                        print(f"ç”¨æˆ·{user_id}æ¨èæ•°é‡ä¸è¶³: {len(user_recs)}")
                except Exception as e:
                    print(f"ä¸ºç”¨æˆ·{user_id}ç”Ÿæˆæ¨èå¤±è´¥: {e}")
                    continue

            print(f"æˆåŠŸä¸º {successful_users}/{len(unique_users)} ä¸ªç”¨æˆ·ç”Ÿæˆæ¨è")

            # ğŸ”§ å…³é”®æ£€æŸ¥ï¼šå¦‚æœæˆåŠŸç”¨æˆ·å¤ªå°‘ï¼Œè¿”å›0
            if successful_users < len(unique_users) * 0.1:  # å°‘äº10%çš„ç”¨æˆ·æˆåŠŸ
                print("âš ï¸ æ¨èç”ŸæˆæˆåŠŸç‡è¿‡ä½ï¼Œå¤šæ ·æ€§æŒ‡æ ‡è®¾ä¸º0")
                return {metric: 0.0 for metric in ['catalog_coverage', 'user_coverage', 'diversity', 'novelty', 'serendipity']}

            # å‡†å¤‡è¯„ä¼°æ•°æ®
            stats = self.data_manager.get_statistics()
            additional_data = {
                'recommendations': {user_id: all_recommendations[i*10:(i+1)*10]
                                  for i, user_id in enumerate(unique_users[:successful_users])},
                'n_items': stats['n_items'],
                'n_users': successful_users,
                'train_data': self.data_manager.train_data
            }

            if not all_recommendations:
                print("âš ï¸ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ¨èï¼Œå¤šæ ·æ€§æŒ‡æ ‡è®¾ä¸º0")
                return {metric: 0.0 for metric in ['catalog_coverage', 'user_coverage', 'diversity', 'novelty', 'serendipity']}


            # è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡
            diversity_metrics = ['catalog_coverage', 'user_coverage', 'diversity', 'novelty', 'serendipity']

            for metric_name in diversity_metrics:
                try:
                    # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®æŒ‡æ ‡ç±»å‹æä¾›åˆé€‚çš„æ•°æ®
                    if metric_name == 'catalog_coverage':
                        # è®¡ç®—ç›®å½•è¦†ç›–ç‡
                        unique_items = set(all_recommendations)
                        results[metric_name] = len(unique_items) / stats['n_items']

                    elif metric_name == 'user_coverage':
                        # è®¡ç®—ç”¨æˆ·è¦†ç›–ç‡
                        results[metric_name] = successful_users / len(unique_users)

                    elif metric_name == 'diversity':
                        # è®¡ç®—å¤šæ ·æ€§ï¼ˆç†µï¼‰
                        from collections import Counter
                        if all_recommendations:
                            item_counts = Counter(all_recommendations)
                            total_recs = len(all_recommendations)
                            diversity = 0.0
                            for count in item_counts.values():
                                p = count / total_recs
                                if p > 0:
                                    diversity -= p * np.log2(p)
                            max_diversity = np.log2(len(set(all_recommendations))) if len(set(all_recommendations)) > 1 else 1
                            results[metric_name] = diversity / max_diversity if max_diversity > 0 else 0.0
                        else:
                            results[metric_name] = 0.0

                    elif metric_name == 'novelty':
                        # è®¡ç®—æ–°é¢–æ€§
                        if all_recommendations:
                            from collections import Counter
                            item_popularity = Counter()
                            for row in self.data_manager.train_data:
                                item_id = int(row[1])
                                item_popularity[item_id] += 1

                            total_interactions = len(self.data_manager.train_data)
                            novelty_scores = []

                            for item in all_recommendations:
                                popularity = item_popularity.get(item, 0)
                                novelty = -np.log2((popularity + 1) / (total_interactions + len(item_popularity)))
                                novelty_scores.append(novelty)

                            if novelty_scores:
                                max_novelty = -np.log2(1 / (total_interactions + len(item_popularity)))
                                results[metric_name] = np.mean(novelty_scores) / max_novelty if max_novelty > 0 else 0
                            else:
                                results[metric_name] = 0.0
                        else:
                            results[metric_name] = 0.0

                    elif metric_name == 'serendipity':
                        # æ„å¤–æ€§ï¼ˆç®€åŒ–ä¸ºæ–°é¢–æ€§å’Œå¤šæ ·æ€§çš„ä¹˜ç§¯ï¼‰
                        results[metric_name] = results.get('novelty', 0) * results.get('diversity', 0)

                    print(f"âœ… {metric_name}: {results[metric_name]:.4f}")

                except Exception as e:
                    print(f"âŒ è®¡ç®—{metric_name}æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
                    results[metric_name] = 0.0

        except Exception as e:
            print(f"è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            # è®¾ç½®é»˜è®¤å€¼
            for metric_name in ['catalog_coverage', 'user_coverage', 'diversity', 'novelty', 'serendipity']:
                results[metric_name] = 0.0

        return results

    def _get_train_user_items(self):
        """è·å–è®­ç»ƒé›†ä¸­ç”¨æˆ·çš„ç‰©å“"""
        user_items = {}
        for row in self.data_manager.train_data:
            user_id = int(row[0])
            item_id = int(row[1])
            if user_id not in user_items:
                user_items[user_id] = []
            user_items[user_id].append(item_id)
        return user_items

    def save_results(self, results, config, loss_type='l2', trial_id='N/A'):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        # åˆ›å»ºç»“æœå­—å…¸
        save_data = {
            'dataset_info': {
                'dataset_name': self.dataset_name,
                'dataset_display_name': self.dataset_display_name,
                'dataset_file': self.dataset_file,
            },
            'model_config': config,
            'loss_type': loss_type,
            'best_trial_id': trial_id,
            'evaluation_results': results,
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }

        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        filename = f"{self.dataset_name}_{loss_type}_evaluation_{time.strftime('%Y%m%d_%H%M%S')}.json"
        save_path = self.results_dir / filename

        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)

        print(f"ç»“æœå·²ä¿å­˜è‡³: {save_path}")

        # åŒæ—¶ä¿å­˜ä¸ºCSVæ ¼å¼ï¼Œæ–¹ä¾¿åç»­åˆ†æ
        results_df = pd.DataFrame([results])
        results_df['loss_type'] = loss_type
        results_df['trial_id'] = trial_id

        # æ·»åŠ é…ç½®å‚æ•°åˆ°DataFrame
        for param, value in config.items():
            results_df[f'config_{param}'] = value

        csv_path = self.results_dir / f"{self.dataset_name}_{loss_type}_evaluation_{time.strftime('%Y%m%d_%H%M%S')}.csv"
        results_df.to_csv(csv_path, index=False)

        print(f"CSVç»“æœå·²ä¿å­˜è‡³: {csv_path}")

        return save_path

    def save_combined_results(self, all_results):
        """ä¿å­˜æ‰€æœ‰æŸå¤±å‡½æ•°çš„åˆå¹¶ç»“æœ"""
        # åˆ›å»ºç»¼åˆç»“æœDataFrame
        combined_df = pd.DataFrame()

        for loss_type, results in all_results.items():
            results_copy = results.copy()
            results_copy['loss_type'] = loss_type
            combined_df = pd.concat([combined_df, pd.DataFrame([results_copy])], ignore_index=True)

        # ä¿å­˜ç»¼åˆCSVæ–‡ä»¶
        combined_csv_path = self.results_dir / f"{self.dataset_name}_all_losses_evaluation_{time.strftime('%Y%m%d_%H%M%S')}.csv"
        combined_df.to_csv(combined_csv_path, index=False)
        print(f"\nç»¼åˆç»“æœCSVå·²ä¿å­˜è‡³: {combined_csv_path}")

        # ä¿å­˜ç»¼åˆJSONæ–‡ä»¶
        combined_json = {
            'dataset_info': {
                'dataset_name': self.dataset_name,
                'dataset_display_name': self.dataset_display_name,
                'dataset_file': self.dataset_file,
            },
            'results_by_loss': all_results,
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }

        combined_json_path = self.results_dir / f"{self.dataset_name}_all_losses_evaluation_{time.strftime('%Y%m%d_%H%M%S')}.json"
        with open(combined_json_path, 'w', encoding='utf-8') as f:
            json.dump(combined_json, f, indent=2, ensure_ascii=False)
        print(f"ç»¼åˆç»“æœJSONå·²ä¿å­˜è‡³: {combined_json_path}")


    def debug_recommendation_generation(self, model):
        """è°ƒè¯•æ¨èç”Ÿæˆè¿‡ç¨‹"""
        print("\n è°ƒè¯•æ¨èç”Ÿæˆ...")

        # éšæœºé€‰æ‹©5ä¸ªç”¨æˆ·æµ‹è¯•
        test_users = np.random.choice(
            np.unique(self.data_manager.test_data[:, 0].astype(int)),
            min(5, len(np.unique(self.data_manager.test_data[:, 0]))),
            replace=False
        )

        train_user_items = self._get_train_user_items()

        for user_id in test_users:
            print(f"\nç”¨æˆ· {user_id}:")

            # æ£€æŸ¥è®­ç»ƒé›†äº¤äº’
            known_items = train_user_items.get(user_id, [])
            print(f"  è®­ç»ƒé›†å·²çŸ¥ç‰©å“æ•°: {len(known_items)}")
            print(f"  å€™é€‰ç‰©å“æ•°: {model.n_items - len(known_items)}")

            # å°è¯•ç”Ÿæˆæ¨è
            try:
                topk_gen = TopKGenerator(model)

                # å…ˆå°è¯•æ’é™¤å·²çŸ¥ç‰©å“
                recs_excluded = topk_gen.generate_top_k_for_user(
                    user_id=user_id,
                    k=10,
                    exclude_items=set(known_items)
                )
                print(f"  æ’é™¤å·²çŸ¥ç‰©å“åæ¨èæ•°: {len(recs_excluded)}")

                # å†å°è¯•ä¸æ’é™¤
                recs_all = topk_gen.generate_top_k_for_user(
                    user_id=user_id,
                    k=10,
                    exclude_items=None
                )
                print(f"  ä¸æ’é™¤ä»»ä½•ç‰©å“æ¨èæ•°: {len(recs_all)}")

                if len(recs_excluded) < 5:
                    print(f"  âš ï¸ è¯¥ç”¨æˆ·æ¨èç”Ÿæˆæœ‰é—®é¢˜ï¼")

            except Exception as e:
                print(f"  âŒ æ¨èç”Ÿæˆå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    # è·å–ç”¨æˆ·è¾“å…¥
    print("="*60)
    print("æœ€ä¼˜å‚æ•°è¯„ä¼°å·¥å…· - æ”¯æŒæ‰¹é‡æŸå¤±å‡½æ•°è¯„ä¼°")
    print("="*60)

    # è®°å½•å®éªŒå¼€å§‹æ—¶é—´
    experiment_start_time = time.time()

    # 1. è¾“å…¥é…ç½®æ–‡ä»¶è·¯å¾„
    config_file = input("è¯·è¾“å…¥é…ç½®æ–‡ä»¶çš„å®Œæ•´è·¯å¾„: ").strip()
    if not os.path.exists(config_file):
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {config_file}")
        return

    # 2. é€‰æ‹©æŸå¤±å‡½æ•°
    OptimalParameterEvaluator.display_loss_functions()
    loss_functions = OptimalParameterEvaluator.get_available_loss_functions()
    loss_keys = list(loss_functions.keys())

    while True:
        try:
            choice = input(f"è¯·é€‰æ‹©æŸå¤±å‡½æ•° (0-{len(loss_keys)}, é»˜è®¤0è¿è¡Œæ‰€æœ‰): ").strip()
            if not choice:  # ç”¨æˆ·ç›´æ¥æŒ‰å›è½¦ï¼Œé»˜è®¤è¿è¡Œæ‰€æœ‰
                choice = "0"

            choice_idx = int(choice)
            if choice_idx == 0:
                selected_losses = loss_keys  # è¿è¡Œæ‰€æœ‰æŸå¤±å‡½æ•°
                break
            elif 1 <= choice_idx <= len(loss_keys):
                selected_losses = [loss_keys[choice_idx - 1]]  # è¿è¡Œå•ä¸ªæŸå¤±å‡½æ•°
                break
            else:
                print(f"è¯·è¾“å…¥0åˆ°{len(loss_keys)}ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

    # 3. è¾“å…¥æ•°æ®é›†æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    dataset_file = input("è¯·è¾“å…¥æ•°æ®é›†æ–‡ä»¶çš„å®Œæ•´è·¯å¾„: ").strip()
    if not os.path.exists(dataset_file):
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {dataset_file}")
        return

    try:
        # å…ˆåŠ è½½é…ç½®æ–‡ä»¶è·å–åŸºæœ¬ä¿¡æ¯ï¼ˆä¸åˆ›å»ºæ•°æ®ç®¡ç†å™¨ï¼‰
        print(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
        with open(config_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # æå–æ•°æ®é›†ä¿¡æ¯
        dataset_info = data.get('dataset_info', {})
        dataset_name = dataset_info.get('dataset_name', '')
        dataset_display_name = dataset_info.get('dataset_display_name', '')

        # æå–æœ€ä¼˜é…ç½®å’Œtrial_id
        if 'results' in data and 'best_config' in data['results']:
            best_config = data['results']['best_config']
            best_score = data['results'].get('best_score', 0)

            # æŸ¥æ‰¾æœ€ä½³è¯•éªŒçš„trial_id
            trial_id = 'N/A'
            if 'optimizer_results' in data['results'] and 'all_trials' in data['results']['optimizer_results']:
                all_trials = data['results']['optimizer_results']['all_trials']
                for trial in all_trials:
                    if abs(trial.get('score', 0) - best_score) < 1e-10:  # æµ®ç‚¹æ•°æ¯”è¾ƒ
                        trial_id = trial.get('trial_id', 'N/A')
                        break
        elif 'best_config' in data:
            best_config = data['best_config']
            trial_id = data.get('best_trial_id', 'N/A')
        else:
            raise ValueError("æ— æ³•ä»æ–‡ä»¶ä¸­æå–æœ€ä¼˜é…ç½®")

        # 4. è®¾ç½®è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ä¸ºtrial_idï¼‰
        default_epochs = trial_id if isinstance(trial_id, int) else 50
        n_epochs = int(input(f"è¯·è¾“å…¥è®­ç»ƒè½®æ•° (é»˜è®¤: {default_epochs}): ").strip() or str(default_epochs))

        config_info = {
            'dataset_name': dataset_name,
            'dataset_display_name': dataset_display_name,
            'best_config': best_config,
            'trial_id': trial_id
        }

        print("\nä»é…ç½®æ–‡ä»¶ä¸­è¯»å–çš„ä¿¡æ¯:")
        print(f"æ•°æ®é›†åç§°: {config_info['dataset_name']}")
        print(f"æ•°æ®é›†æ˜¾ç¤ºåç§°: {config_info['dataset_display_name']}")
        print(f"æœ€ä¼˜è¿­ä»£æ¬¡æ•° (trial_id): {config_info['trial_id']}")
        print(f"é€‰æ‹©çš„æŸå¤±å‡½æ•°: {', '.join(selected_losses) if len(selected_losses) > 1 else loss_functions[selected_losses[0]]['name']}")
        print(f"æœ€ä¼˜å‚æ•°é…ç½®:")
        for key, value in config_info['best_config'].items():
            print(f"  {key}: {value}")

        # åˆ›å»ºæ­£å¼çš„è¯„ä¼°å™¨
        evaluator = OptimalParameterEvaluator(
            config_info['dataset_name'],
            dataset_file
        )

        # å­˜å‚¨æ‰€æœ‰æŸå¤±å‡½æ•°çš„ç»“æœ
        all_results = {}

        # å¯¹æ¯ä¸ªé€‰æ‹©çš„æŸå¤±å‡½æ•°è¿›è¡Œè¯„ä¼°
        for loss_type in selected_losses:
            print(f"\n{'='*80}")
            print(f"å¼€å§‹è¯„ä¼°æŸå¤±å‡½æ•°: {loss_functions[loss_type]['name']}")
            print(f"{'='*80}")

            # åˆ›å»ºæ¨¡å‹
            model = evaluator.create_model(
                config_info['best_config'],
                loss_type
            )

            # è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
            results = evaluator.train_and_evaluate(model, n_epochs)

            # æ‰“å°è¯„ä¼°ç»“æœ
            print("\nè¯„ä¼°ç»“æœ:")
            print("-" * 60)

            # è¯„åˆ†é¢„æµ‹æŒ‡æ ‡
            print("è¯„åˆ†é¢„æµ‹æŒ‡æ ‡:")
            for metric in ['mae', 'rmse', 'mse', 'r2']:
                if metric in results:
                    print(f"  {metric.upper()}: {results[metric]:.4f}")

            # æ’åºæŒ‡æ ‡
            print("\næ’åºæŒ‡æ ‡:")
            for k in [5, 10, 20]:
                print(f"  @{k}:")
                for metric in ['hr', 'precision', 'recall', 'map', 'ndcg']:
                    key = f'{metric}@{k}'
                    if key in results:
                        print(f"    {metric.upper()}: {results[key]:.4f}")

            # MRRæŒ‡æ ‡
            if 'mrr' in results:
                print(f"  MRR: {results['mrr']:.4f}")

            # å¤šæ ·æ€§å’Œè¦†ç›–åº¦æŒ‡æ ‡
            print("\nå¤šæ ·æ€§å’Œè¦†ç›–åº¦æŒ‡æ ‡:")
            for metric in ['catalog_coverage', 'user_coverage', 'diversity', 'novelty', 'serendipity']:
                if metric in results:
                    print(f"  {metric.replace('_', ' ').title()}: {results[metric]:.4f}")

            print(f"\nè®­ç»ƒæ—¶é—´: {results.get('training_time', 0):.2f} ç§’")

            # ä¿å­˜å•ä¸ªæŸå¤±å‡½æ•°çš„ç»“æœ
            evaluator.save_results(
                results,
                config_info['best_config'],
                loss_type,
                config_info['trial_id']
            )

            # å­˜å‚¨ç»“æœç”¨äºåç»­åˆå¹¶
            all_results[loss_type] = results

            # å¦‚æœæ˜¯å¤šä¸ªæŸå¤±å‡½æ•°è¯„ä¼°ï¼Œå‘é€å•ä¸ªæŸå¤±å‡½æ•°å®Œæˆé€šçŸ¥
            if len(selected_losses) > 1:
                single_loss_message = f"âœ… æŸå¤±å‡½æ•° {loss_functions[loss_type]['name']} è¯„ä¼°å®Œæˆ\n" \
                                     f"æ•°æ®é›†: {config_info['dataset_display_name']}\n" \
                                     f"RMSE: {results.get('rmse', 0):.4f}\n" \
                                     f"MAE: {results.get('mae', 0):.4f}\n" \
                                     f"HR@10: {results.get('hr@10', 0):.4f}\n" \
                                     f"è®­ç»ƒæ—¶é—´: {results.get('training_time', 0):.2f}ç§’"

                try:
                    send_sms_notification(single_loss_message)
                    print(f"âœ… {loss_type.upper()} å®Œæˆé€šçŸ¥å·²å‘é€")
                except Exception as e:
                    print(f"âš ï¸ å‘é€ {loss_type.upper()} å®Œæˆé€šçŸ¥å¤±è´¥: {e}")

        # å¦‚æœè¯„ä¼°äº†å¤šä¸ªæŸå¤±å‡½æ•°ï¼Œä¿å­˜åˆå¹¶ç»“æœ
        if len(selected_losses) > 1:
            print(f"\n{'='*80}")
            print("ä¿å­˜æ‰€æœ‰æŸå¤±å‡½æ•°çš„ç»¼åˆç»“æœ...")
            evaluator.save_combined_results(all_results)

            # æ‰“å°æ‰€æœ‰æŸå¤±å‡½æ•°çš„å¯¹æ¯”æ‘˜è¦
            print(f"\n{'='*80}")
            print("æŸå¤±å‡½æ•°æ€§èƒ½å¯¹æ¯”æ‘˜è¦:")
            print(f"{'='*80}")

            # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
            print(f"{'æŸå¤±å‡½æ•°':<15} {'RMSE':<10} {'MAE':<10} {'HR@10':<12} {'NDCG@10':<10} {'è®­ç»ƒæ—¶é—´(ç§’)':<12}")
            print("-" * 80)

            for loss_type in selected_losses:
                if loss_type in all_results:
                    r = all_results[loss_type]
                    print(f"{loss_type.upper():<15} "
                          f"{r.get('rmse', 0):<10.4f} "
                          f"{r.get('mae', 0):<10.4f} "
                          f"{r.get('hr@10', 0):<12.4f} "
                          f"{r.get('ndcg@10', 0):<10.4f} "
                          f"{r.get('training_time', 0):<12.2f}")

        print(f"\n{'='*80}")
        print("æ‰€æœ‰è¯„ä¼°å®Œæˆ!")
        print(f"ç»“æœä¿å­˜åœ¨: {evaluator.results_dir}")

        # è®¡ç®—æ€»å®éªŒæ—¶é—´
        experiment_end_time = time.time()
        total_experiment_time = experiment_end_time - experiment_start_time

        # å‘é€å®éªŒå®Œæˆæ€»ç»“é€šçŸ¥
        if len(selected_losses) == 1:
            # å•ä¸ªæŸå¤±å‡½æ•°çš„å®Œæˆé€šçŸ¥
            loss_type = selected_losses[0]
            results = all_results[loss_type]
            completion_message = f"ğŸ‰ æœ€ä¼˜å‚æ•°è¯„ä¼°å®éªŒå®Œæˆ!\n" \
                               f"æ•°æ®é›†: {config_info['dataset_display_name']}\n" \
                               f"æŸå¤±å‡½æ•°: {loss_functions[loss_type]['name']}\n" \
                               f"RMSE: {results.get('rmse', 0):.4f}\n" \
                               f"MAE: {results.get('mae', 0):.4f}\n" \
                               f"HR@10: {results.get('hr@10', 0):.4f}\n" \
                               f"NDCG@10: {results.get('ndcg@10', 0):.4f}\n" \
                               f"æ€»è€—æ—¶: {total_experiment_time/60:.1f}åˆ†é’Ÿ"
        else:
            # å¤šä¸ªæŸå¤±å‡½æ•°çš„å®Œæˆé€šçŸ¥ - æ‰¾åˆ°æœ€ä½³è¡¨ç°
            best_loss_type = ""
            best_rmse = float('inf')
            for loss_type in selected_losses:
                if loss_type in all_results:
                    rmse = all_results[loss_type].get('rmse', float('inf'))
                    if rmse < best_rmse:
                        best_rmse = rmse
                        best_loss_type = loss_type

            completion_message = f"ğŸ‰ æ‰¹é‡è¯„ä¼°å®éªŒå®Œæˆ!\n" \
                               f"æ•°æ®é›†: {config_info['dataset_display_name']}\n" \
                               f"è¯„ä¼°äº† {len(selected_losses)} ä¸ªæŸå¤±å‡½æ•°\n" \
                               f"æœ€ä½³æŸå¤±å‡½æ•°: {loss_functions[best_loss_type]['name']}\n" \
                               f"æœ€ä½³RMSE: {best_rmse:.4f}\n" \
                               f"æ€»è€—æ—¶: {total_experiment_time/60:.1f}åˆ†é’Ÿ\n" \
                               f"ç»“æœå·²ä¿å­˜åˆ°: {evaluator.results_dir.name}"

        try:
            send_sms_notification(completion_message)
            print("âœ… å®éªŒå®Œæˆé€šçŸ¥å·²å‘é€")
        except Exception as e:
            print(f"âš ï¸ å‘é€å®Œæˆé€šçŸ¥å¤±è´¥: {e}")

    except Exception as e:
        print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

        # å‘é€é”™è¯¯é€šçŸ¥
        error_message = f"âŒ æœ€ä¼˜å‚æ•°è¯„ä¼°å®éªŒå‡ºé”™!\n" \
                       f"æ•°æ®é›†: {config_info.get('dataset_display_name', 'æœªçŸ¥') if 'config_info' in locals() else 'æœªçŸ¥'}\n" \
                       f"é”™è¯¯ä¿¡æ¯: {str(e)[:100]}...\n" \
                       f"å‘ç”Ÿæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}"

        try:
            send_sms_notification(error_message)
            print("âœ… é”™è¯¯é€šçŸ¥å·²å‘é€")
        except Exception as sms_e:
            print(f"âš ï¸ å‘é€é”™è¯¯é€šçŸ¥å¤±è´¥: {sms_e}")


if __name__ == "__main__":
    main()
