#!/usr/bin/env python3
# ä¿®å¤ç‰ˆæœ¬ï¼šOptimal parameter experiment\Optimal_parameter_experiment.py
"""
HPLæŸå¤±å‡½æ•°ä¸“ç”¨ä¼˜åŒ–å®éªŒ - ä¿®å¤ç‰ˆæœ¬

ä¿®å¤å†…å®¹ï¼š
1. ä¿®å¤ best_trial å¯¹è±¡å±æ€§è®¿é—®é—®é¢˜
2. æ”¹è¿›ç»“æœä¿å­˜é€»è¾‘
3. å¢å¼ºé”™è¯¯å¤„ç†
"""
from pathlib import Path
import sys
import os
import numpy as np
import time
import json
import glob
import traceback
from pathlib import Path
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# å¯¼å…¥æ¨¡å—
try:
    from src.hyperopt.space import ParameterSpace
    from src.hyperopt.samplers import LatinHypercubeSampler
    from src.hyperopt.constraints import ConstraintManager
    from src.hyperopt.optimizer import HyperOptimizer
    from src.hyperopt.tracker import ExperimentTracker
    print("æˆåŠŸå¯¼å…¥hyperoptæ¨¡å—")
except ImportError as e:
    print(f"å¯¼å…¥hyperoptæ¨¡å—å¤±è´¥: {e}")

try:
    from data.data_manager import DataManager
    from src.models.mf_sgd import MatrixFactorizationSGD
    from src.models.initializers import NormalInitializer
    from src.models.regularizers import L2Regularizer
    from src.losses.hpl import HybridPiecewiseLoss
    from src.evaluation.metrics import (
        RMSE, MAE, MSE, R2Score, HitRate, Precision, Recall,
        MAP, NDCG, MRR, CatalogCoverage, UserCoverage,
        Diversity, Novelty, MetricFactory
    )
    print("æˆåŠŸå¯¼å…¥æ•°æ®å’Œæ¨¡å‹æ¨¡å—")
except ImportError as e:
    print(f"å¯¼å…¥æ•°æ®/æ¨¡å‹æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

# åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥çŸ­ä¿¡é€šçŸ¥æ¨¡å—
try:
    from utils.sms_notification import send_sms_notification
except ImportError:
    print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥çŸ­ä¿¡é€šçŸ¥æ¨¡å—")
    def send_sms_notification(message):
        print(f"é€šçŸ¥: {message}")


class SafeTrial:
    """å®‰å…¨çš„è¯•éªŒç»“æœåŒ…è£…å™¨"""
    def __init__(self, config, score):
        self.config = config
        self.score = score

    def to_dict(self):
        return {'config': self.config, 'score': self.score}


class DatasetAwareResultManager:
    """æ•°æ®é›†æ„ŸçŸ¥çš„ç»“æœç®¡ç†å™¨"""

    DATASET_CONFIGS = {
        'movielens100k': {
            'name': 'MovieLens100K',
            'size': '100K',
            'version': '2020-12-02',
            'short_name': 'ml100k'
        },
        'movielens1m': {
            'name': 'MovieLens1M',
            'size': '1M',
            'version': '2021-01-01',
            'short_name': 'ml1m'
        },
        'amazon_movies': {
            'name': 'Amazon Movies',
            'size': 'Various',
            'version': '2023',
            'short_name': 'amazon_mv'
        }
    }

    def __init__(self, dataset_name='movielens100k', dataset_file=None, base_dir=None):
        self.dataset_name = dataset_name
        self.dataset_file = dataset_file

        if base_dir is None:
            current_script_dir = os.path.dirname(os.path.abspath(__file__))
            base_dir = os.path.join(current_script_dir, 'results')

        self.base_dir = Path(base_dir)

        self.dataset_info = self.DATASET_CONFIGS.get(dataset_name, {
            'name': dataset_name.title(),
            'size': 'Unknown',
            'version': 'Unknown',
            'short_name': dataset_name.lower()
        })

        self.dataset_dir = self.base_dir / self.dataset_info['short_name']
        self.dataset_dir.mkdir(parents=True, exist_ok=True)
        self._create_subdirectories()

        print(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•: {self.dataset_dir}")

    def _create_subdirectories(self):
        """åˆ›å»ºå¿…è¦çš„å­ç›®å½•"""
        subdirs = ['hpl_optimization', 'models', 'reports', 'logs']
        for subdir in subdirs:
            (self.dataset_dir / subdir).mkdir(exist_ok=True)

    def generate_filename(self, experiment_type, file_format='json', include_timestamp=True):
        """ç”Ÿæˆæ•°æ®é›†ç›¸å…³çš„æ–‡ä»¶å"""
        timestamp = time.strftime("%Y%m%d_%H%M%S") if include_timestamp else ""
        parts = [self.dataset_info['short_name'], experiment_type, timestamp]
        parts = [part for part in parts if part]
        filename = "_".join(parts)
        return f"{filename}.{file_format}"

    def get_save_path(self, experiment_type, file_format='json'):
        """è·å–å®Œæ•´çš„ä¿å­˜è·¯å¾„"""
        subdir_map = {
            'hpl_optimization': 'hpl_optimization',
            'enhanced_hpl_optimization': 'hpl_optimization',
            'model_evaluation': 'reports',
            'training_log': 'logs'
        }
        subdir = subdir_map.get(experiment_type, 'hpl_optimization')
        filename = self.generate_filename(experiment_type, file_format)
        return self.dataset_dir / subdir / filename

    def save_experiment_results(self, results, experiment_type, metadata=None):
        """ä¿å­˜å®éªŒç»“æœï¼ŒåŒ…å«å®Œæ•´çš„æ•°æ®é›†ä¿¡æ¯å’Œæ‰€æœ‰é…ç½®å‚æ•°"""
        try:
            save_data = {
                'dataset_info': {
                    'dataset_name': self.dataset_name,
                    'dataset_file': self.dataset_file,
                    'dataset_display_name': self.dataset_info['name'],
                    'dataset_size': self.dataset_info['size'],
                    'dataset_version': self.dataset_info['version']
                },
                'experiment_info': {
                    'experiment_type': experiment_type,
                    'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                    'experiment_id': f"{experiment_type}_{time.strftime('%Y%m%d_%H%M%S')}",
                    'python_version': sys.version,
                    'numpy_version': np.__version__,
                    'random_seed': 42
                },
                'results': results,
                'reproduction_info': {
                    'command': f"python {os.path.basename(__file__)}",
                    'working_directory': os.getcwd(),
                    'environment': {
                        'PYTHONPATH': os.environ.get('PYTHONPATH', ''),
                        'project_root': project_root
                    }
                }
            }

            if metadata:
                save_data['metadata'] = metadata

            save_path = self.get_save_path(experiment_type, 'json')
            clean_data = self._convert_numpy_types(save_data)

            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(clean_data, f, indent=2, ensure_ascii=False)

            print(f"\nğŸ“ å®éªŒç»“æœå·²ä¿å­˜åˆ°: {save_path}")
            print(f"   æ•°æ®é›†: {self.dataset_info['name']}")
            print(f"   å®éªŒç±»å‹: {experiment_type}")

            return save_path

        except Exception as e:
            print(f"ä¿å­˜å®éªŒç»“æœå¤±è´¥: {e}")
            traceback.print_exc()
            return None

    def _convert_numpy_types(self, obj):
        """é€’å½’è½¬æ¢numpyç±»å‹"""
        if isinstance(obj, dict):
            return {key: self._convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        elif hasattr(obj, 'item'):
            return obj.item()
        elif hasattr(obj, 'tolist'):
            return obj.tolist()
        else:
            return obj


class OptimizationMonitor:
    """ç»Ÿä¸€çš„ä¼˜åŒ–ç›‘æ§å™¨"""

    def __init__(self, result_manager, experiment_type='optimization'):
        self.result_manager = result_manager
        self.experiment_type = experiment_type
        self.trial_history = []
        self.best_score_history = []

        log_filename = f"{experiment_type}_log_{time.strftime('%Y%m%d_%H%M%S')}.txt"
        self.log_path = result_manager.dataset_dir / 'logs' / log_filename

        self.log_file = open(self.log_path, 'w', encoding='utf-8')
        self.log_file.write(f"ä¼˜åŒ–æ—¥å¿— - {result_manager.dataset_info['name']}\n")
        self.log_file.write(f"ä¿å­˜è·¯å¾„: {result_manager.dataset_dir}\n")
        self.log_file.write(f"å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        self.log_file.write("="*60 + "\n\n")
        self.log_file.flush()

    def update(self, trial):
        """æ›´æ–°ç›‘æ§ä¿¡æ¯"""
        # ç¡®ä¿trialæ˜¯SafeTrialå¯¹è±¡
        if not isinstance(trial, SafeTrial):
            if hasattr(trial, 'config') and hasattr(trial, 'score'):
                trial = SafeTrial(trial.config, trial.score)
            elif isinstance(trial, dict):
                trial = SafeTrial(trial.get('config', {}), trial.get('score', float('inf')))
            else:
                print(f"è­¦å‘Šï¼šæ— æ³•å¤„ç†çš„trialç±»å‹: {type(trial)}")
                return

        self.trial_history.append(trial)

        if not self.best_score_history:
            self.best_score_history.append(trial.score)
        else:
            self.best_score_history.append(
                min(self.best_score_history[-1], trial.score)
            )

        self.log_file.write(f"è¯•éªŒ {len(self.trial_history)}: åˆ†æ•°={trial.score:.4f}, é…ç½®={trial.config}\n")
        self.log_file.flush()

        if len(self.trial_history) % 10 == 0:
            progress_msg = f"è¿›åº¦: {len(self.trial_history)} æ¬¡è¯•éªŒå®Œæˆï¼Œå½“å‰æœ€ä½³: {min(self.best_score_history):.4f}"
            print(progress_msg)
            self.log_file.write(f"\n{progress_msg}\n\n")
            self.log_file.flush()

    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¹¶ä¿å­˜"""
        report_data = {
            'total_trials': len(self.trial_history),
            'best_score': min(self.best_score_history) if self.best_score_history else None,
            'best_trial': min(self.trial_history, key=lambda x: x.score).to_dict() if self.trial_history else None,
            'score_history': self.best_score_history,
            'convergence_analysis': self._analyze_convergence(),
            'all_trials': [trial.to_dict() for trial in self.trial_history]
        }

        # åªæœ‰å½“æœ‰è¯•éªŒæ•°æ®æ—¶æ‰ä¿å­˜ç›‘æ§æŠ¥å‘Š
        if len(self.trial_history) > 0:
            report_path = self.result_manager.save_experiment_results(
                results=report_data,
                experiment_type=f"{self.experiment_type}_monitor_report",  # æ”¹åé¿å…è¦†ç›–
                metadata={'monitor_type': 'OptimizationMonitor'}
            )
        else:
            print("âš ï¸ ç›‘æ§å™¨æ²¡æœ‰è®°å½•åˆ°ä»»ä½•è¯•éªŒï¼Œè·³è¿‡ç›‘æ§æŠ¥å‘Šç”Ÿæˆ")
            report_path = None

        self.log_file.write(f"\nç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        self.log_file.write(f"æ€»è¯•éªŒæ¬¡æ•°: {len(self.trial_history)}\n")
        if self.best_score_history:
            self.log_file.write(f"æœ€ç»ˆæœ€ä½³åˆ†æ•°: {min(self.best_score_history):.4f}\n")
        self.log_file.close()

        print(f"\nğŸ“Š ç›‘æ§æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {self.log_path}")
        print(f"æ€»è¯•éªŒæ¬¡æ•°: {len(self.trial_history)}")
        if self.best_score_history:
            print(f"æœ€ç»ˆæœ€ä½³åˆ†æ•°: {min(self.best_score_history):.4f}")

    def _analyze_convergence(self):
        """åˆ†ææ”¶æ•›æƒ…å†µ"""
        if len(self.best_score_history) < 10:
            return {'status': 'insufficient_data'}

        recent_scores = self.best_score_history[-10:]
        improvement = recent_scores[0] - recent_scores[-1]

        return {
            'recent_improvement': improvement,
            'converged': improvement < 0.001,
            'best_trial_index': self.best_score_history.index(min(self.best_score_history)),
            'plateau_length': len(self.best_score_history) - self.best_score_history.index(min(self.best_score_history))
        }


class HPLObjectiveFunction:
    """HPLä¸“ç”¨ç›®æ ‡å‡½æ•°"""

    def __init__(self, data_manager):
        self.data_manager = data_manager
        self.n_users = data_manager.get_statistics()['n_users']
        self.n_items = data_manager.get_statistics()['n_items']
        self.train_data, self.val_data, self.test_data = data_manager.get_splits()

        self.rmse_metric = RMSE()

        print(f"ç›®æ ‡å‡½æ•°åˆå§‹åŒ–:")
        print(f"  è®­ç»ƒé›†: {len(self.train_data)} æ¡")
        print(f"  éªŒè¯é›†: {len(self.val_data)} æ¡")
        print(f"  æµ‹è¯•é›†: {len(self.test_data)} æ¡")

    def _create_model(self, config):
        """åˆ›å»ºæ¨¡å‹"""
        loss_function = HybridPiecewiseLoss(
            delta1=config['delta1'],
            delta2=config['delta2'],
            l_max=config.get('l_max', 4.0),
            c_sigmoid=config.get('c_sigmoid', 1.0)
        )

        regularizer = L2Regularizer(lambda_reg=config['lambda_reg'])

        model = MatrixFactorizationSGD(
            n_users=self.n_users,
            n_items=self.n_items,
            n_factors=config['latent_factors'],
            learning_rate=config['learning_rate'],
            regularizer=regularizer,
            loss_function=loss_function,
            use_bias=True,
            global_mean=self.data_manager.global_mean or 0.0
        )

        initializer = NormalInitializer(mean=0.0, std=0.01)
        model.initialize_parameters(initializer)
        return model

    def _train_and_evaluate(self, model, n_epochs=100):
    # def _train_and_evaluate(self, model, n_epochs=10): # å¿«é€Ÿ
        """è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹"""
        # ä»å…¨å±€è·å–æ¨¡å‹æ—©åœå‚æ•°ï¼Œé»˜è®¤ä¸º12
        model_patience = getattr(self, 'model_early_stopping', 12)

        model.fit(
            train_data=self.train_data,
            val_data=self.val_data,
            n_epochs=n_epochs,
            verbose=0,
            early_stopping_patience=model_patience
        )

        val_predictions = model.predict(
            self.val_data[:, 0].astype(int),
            self.val_data[:, 1].astype(int)
        )

        if self.data_manager.global_mean is not None:
            val_predictions += self.data_manager.global_mean
            val_targets = self.val_data[:, 2] + self.data_manager.global_mean
        else:
            val_targets = self.val_data[:, 2]

        rmse = self.rmse_metric.calculate(val_targets, val_predictions)
        return rmse

    def __call__(self, config):
        try:
            model = self._create_model(config)
            rmse = self._train_and_evaluate(model)

            print(f"HPLé…ç½®: å­¦ä¹ ç‡={config['learning_rate']:.4f}, "
                  f"å› å­æ•°={config['latent_factors']}, "
                  f"æ­£åˆ™åŒ–={config['lambda_reg']:.6f}, "
                  f"Î´1={config['delta1']:.3f}, Î´2={config['delta2']:.3f}, "
                  f"l_max={config.get('l_max', 4.0):.2f}, "
                  f"c_sig={config.get('c_sigmoid', 1.0):.2f}, "
                  f"RMSE={rmse:.4f}")

            return rmse

        except Exception as e:
            print(f"HPLé…ç½®è¯„ä¼°å¤±è´¥ {config}: {e}")
            return 10.0


class BestParameterEvaluator:
    """æœ€ä½³å‚æ•°æ€§èƒ½è¯„ä¼°å™¨"""

    def __init__(self, data_manager, best_config):
        self.data_manager = data_manager
        self.best_config = best_config
        self.n_users = data_manager.get_statistics()['n_users']
        self.n_items = data_manager.get_statistics()['n_items']
        self.train_data, self.val_data, self.test_data = data_manager.get_splits()

        print(f"ğŸ¯ æœ€ä½³å‚æ•°è¯„ä¼°å™¨åˆå§‹åŒ–:")
        print(f"   æœ€ä½³é…ç½®: {best_config}")
        print(f"   ç”¨æˆ·æ•°: {self.n_users}, ç‰©å“æ•°: {self.n_items}")
        print(f"   æµ‹è¯•é›†å¤§å°: {len(self.test_data)}")

    def create_best_model(self):
        """ä½¿ç”¨æœ€ä½³é…ç½®åˆ›å»ºæ¨¡å‹"""
        loss_function = HybridPiecewiseLoss(
            delta1=self.best_config['delta1'],
            delta2=self.best_config['delta2'],
            l_max=self.best_config.get('l_max', 4.0),
            c_sigmoid=self.best_config.get('c_sigmoid', 1.0)
        )

        regularizer = L2Regularizer(lambda_reg=self.best_config['lambda_reg'])

        model = MatrixFactorizationSGD(
            n_users=self.n_users,
            n_items=self.n_items,
            n_factors=self.best_config['latent_factors'],
            learning_rate=self.best_config['learning_rate'],
            regularizer=regularizer,
            loss_function=loss_function,
            use_bias=True,
            global_mean=self.data_manager.global_mean or 0.0
        )

        # æ‰“å°æ‰€æœ‰æ¨¡å‹é…ç½®å‚æ•°
        print(f"\nğŸ“‹ æ¨¡å‹é…ç½®å‚æ•°:")
        print(f"   å­¦ä¹ ç‡: {self.best_config['learning_rate']:.6f}")
        print(f"   æ½œåœ¨å› å­æ•°: {self.best_config['latent_factors']}")
        print(f"   æ­£åˆ™åŒ–ç³»æ•°: {self.best_config['lambda_reg']:.6f}")
        print(f"   HPLå‚æ•° Î´1: {self.best_config['delta1']:.4f}")
        print(f"   HPLå‚æ•° Î´2: {self.best_config['delta2']:.4f}")
        print(f"   HPLå‚æ•° l_max: {self.best_config.get('l_max', 4.0):.2f}")
        print(f"   HPLå‚æ•° c_sigmoid: {self.best_config.get('c_sigmoid', 1.0):.2f}")
        print(f"   ä½¿ç”¨åç½®: True")
        print(f"   å…¨å±€å‡å€¼: {self.data_manager.global_mean or 0.0:.4f}")

        initializer = NormalInitializer(mean=0.0, std=0.01)
        model.initialize_parameters(initializer)
        return model

    def train_best_model(self, n_epochs=100):
        """ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹"""
        print(f"\nğŸš€ å¼€å§‹ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹ (è®­ç»ƒè½®æ•°: {n_epochs})")

        model = self.create_best_model()

        start_time = time.time()
        model.fit(
            train_data=self.train_data,
            val_data=self.val_data,
            n_epochs=n_epochs,
            verbose=1,
            early_stopping_patience=15  # ä½¿ç”¨æ—©åœé¿å…è¿‡æ‹Ÿåˆ
        )
        train_time = time.time() - start_time

        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {train_time:.2f}ç§’")
        return model, train_time

    def evaluate_all_metrics(self, model):
        """è®¡ç®—æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡"""
        print(f"\nğŸ“Š å¼€å§‹è®¡ç®—æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡...")

        results = {}

        # è·å–æµ‹è¯•æ•°æ®
        test_users = self.test_data[:, 0].astype(int)
        test_items = self.test_data[:, 1].astype(int)
        test_ratings = self.test_data[:, 2]

        # é¢„æµ‹è¯„åˆ†
        print("   æ­£åœ¨é¢„æµ‹æµ‹è¯•é›†è¯„åˆ†...")
        predictions = model.predict(test_users, test_items)

        # è¿˜åŸåˆ°åŸå§‹å°ºåº¦
        if hasattr(self.data_manager, 'global_mean') and self.data_manager.global_mean is not None:
            predictions += self.data_manager.global_mean
            test_ratings_adjusted = test_ratings + self.data_manager.global_mean
        else:
            test_ratings_adjusted = test_ratings

        # 1. è¯„åˆ†é¢„æµ‹æŒ‡æ ‡
        print("   è®¡ç®—è¯„åˆ†é¢„æµ‹æŒ‡æ ‡...")
        rating_metrics = ['RMSE', 'MAE', 'MSE', 'R2']
        for metric_name in rating_metrics:
            try:
                metric = MetricFactory.create(metric_name.lower())
                results[metric_name] = metric.calculate(test_ratings_adjusted, predictions)
                print(f"     {metric_name}: {results[metric_name]:.4f}")
            except Exception as e:
                print(f"     âš ï¸ è®¡ç®—{metric_name}å¤±è´¥: {e}")
                results[metric_name] = None

        # 2. æ’åºè´¨é‡æŒ‡æ ‡
        print("   è®¡ç®—æ’åºè´¨é‡æŒ‡æ ‡...")
        ranking_metrics = ['HitRate', 'Precision', 'Recall', 'MAP', 'NDCG']
        k_values = [5, 10, 20]

        for k in k_values:
            print(f"     è®¡ç®— @{k} æŒ‡æ ‡...")
            for metric_name in ranking_metrics:
                try:
                    metric = MetricFactory.create(f"{metric_name}@{k}")
                    # ä¸ºæ’åºæŒ‡æ ‡å‡†å¤‡æ•°æ®
                    user_items, recommendations = self._prepare_ranking_data(model, k)

                    if user_items and recommendations:
                        score = metric.calculate(
                            None, None,  # ä¸ä½¿ç”¨çŸ©é˜µæ ¼å¼
                            user_items=user_items,
                            recommendations=recommendations
                        )
                        results[f'{metric_name}@{k}'] = score
                        print(f"       {metric_name}@{k}: {score:.4f}")
                    else:
                        results[f'{metric_name}@{k}'] = None
                        print(f"       {metric_name}@{k}: æ•°æ®å‡†å¤‡å¤±è´¥")
                except Exception as e:
                    print(f"       âš ï¸ è®¡ç®—{metric_name}@{k}å¤±è´¥: {e}")
                    results[f'{metric_name}@{k}'] = None

        # 3. MRRæŒ‡æ ‡
        print("   è®¡ç®—MRRæŒ‡æ ‡...")
        try:
            mrr_metric = MetricFactory.create('mrr')
            user_items, recommendations = self._prepare_ranking_data(model, 50)  # ä½¿ç”¨æ›´å¤§çš„Kå€¼è®¡ç®—MRR
            if user_items and recommendations:
                results['MRR'] = mrr_metric.calculate(
                    None, None,
                    user_items=user_items,
                    recommendations=recommendations
                )
                print(f"     MRR: {results['MRR']:.4f}")
            else:
                results['MRR'] = None
        except Exception as e:
            print(f"     âš ï¸ è®¡ç®—MRRå¤±è´¥: {e}")
            results['MRR'] = None

        # 4. è¦†ç›–åº¦æŒ‡æ ‡
        print("   è®¡ç®—è¦†ç›–åº¦æŒ‡æ ‡...")
        try:
            user_items, recommendations = self._prepare_ranking_data(model, 10)
            if recommendations:
                # ç›®å½•è¦†ç›–ç‡
                catalog_coverage = MetricFactory.create('catalog_coverage')
                results['CatalogCoverage'] = catalog_coverage.calculate(
                    None, None,
                    recommendations=recommendations,
                    n_items=self.n_items
                )
                print(f"     ç›®å½•è¦†ç›–ç‡: {results['CatalogCoverage']:.4f}")

                # ç”¨æˆ·è¦†ç›–ç‡
                user_coverage = MetricFactory.create('user_coverage')
                results['UserCoverage'] = user_coverage.calculate(
                    None, None,
                    recommendations=recommendations,
                    n_users=self.n_users
                )
                print(f"     ç”¨æˆ·è¦†ç›–ç‡: {results['UserCoverage']:.4f}")
            else:
                results['CatalogCoverage'] = None
                results['UserCoverage'] = None
        except Exception as e:
            print(f"     âš ï¸ è®¡ç®—è¦†ç›–åº¦æŒ‡æ ‡å¤±è´¥: {e}")
            results['CatalogCoverage'] = None
            results['UserCoverage'] = None

        print(f"âœ… æ€§èƒ½æŒ‡æ ‡è®¡ç®—å®Œæˆ")
        return results

    def _prepare_ranking_data(self, model, k=10):
        """ä¸ºæ’åºæŒ‡æ ‡å‡†å¤‡æ•°æ®"""
        try:
            # è·å–æµ‹è¯•é›†ä¸­çš„ç”¨æˆ·-ç‰©å“äº¤äº’
            user_items = {}
            for row in self.test_data:
                user_id = int(row[0])
                item_id = int(row[1])
                if user_id not in user_items:
                    user_items[user_id] = []
                user_items[user_id].append(item_id)

            # ä¸ºæ¯ä¸ªç”¨æˆ·ç”ŸæˆTop-Kæ¨è
            recommendations = {}
            unique_users = list(user_items.keys())[:100]  # é™åˆ¶ç”¨æˆ·æ•°é‡ä»¥æé«˜è®¡ç®—æ•ˆç‡

            for user_id in unique_users:
                try:
                    # è·å–è¯¥ç”¨æˆ·çš„æ‰€æœ‰ç‰©å“é¢„æµ‹åˆ†æ•°
                    all_items = list(range(self.n_items))
                    user_predictions = model.predict([user_id] * len(all_items), all_items)

                    # æ’åºå¹¶è·å–Top-K
                    item_scores = list(zip(all_items, user_predictions))
                    item_scores.sort(key=lambda x: x[1], reverse=True)
                    recommendations[user_id] = [item for item, score in item_scores[:k]]

                except Exception as e:
                    print(f"       âš ï¸ ä¸ºç”¨æˆ·{user_id}ç”Ÿæˆæ¨èå¤±è´¥: {e}")
                    continue

            # åªä¿ç•™æœ‰æ¨èçš„ç”¨æˆ·
            filtered_user_items = {uid: items for uid, items in user_items.items() if uid in recommendations}

            return filtered_user_items, recommendations

        except Exception as e:
            print(f"     âš ï¸ å‡†å¤‡æ’åºæ•°æ®å¤±è´¥: {e}")
            return {}, {}


class ParameterSpaceFactory:
    """å‚æ•°ç©ºé—´å·¥å‚ç±»"""

    @staticmethod
    def create_enhanced_hpl_space():
        """åˆ›å»ºå¢å¼ºçš„HPLå‚æ•°ç©ºé—´"""
        space = ParameterSpace()

        space.add_continuous('learning_rate', 0.005, 0.15, scale='log')
        space.add_discrete('latent_factors', 5, 120, step=5)  # å®Œæ•´æµ‹è¯•
        # space.add_discrete('latent_factors', 5, 15, step=5)  # å¿«é€Ÿ
        space.add_continuous('lambda_reg', 0.0001, 0.05, scale='log')

        space.add_continuous('delta1', 0.05, 1.0)
        space.add_continuous('delta2', 0.3, 3.5)
        space.add_continuous('l_max', 1.5, 6.0)
        space.add_continuous('c_sigmoid', 0.2, 3.0)

        print(f"å¢å¼ºHPLå‚æ•°ç©ºé—´ç»´åº¦: {space.get_dimension()}")
        return space


class ConstraintFactory:
    """çº¦æŸå·¥å‚ç±»"""

    @staticmethod
    def create_hpl_constraints():
        """åˆ›å»ºHPLä¸“ç”¨çº¦æŸæ¡ä»¶"""
        constraints = ConstraintManager()
        constraints.add_relation('delta1', 'delta2', '<')
        return constraints


class OptimizerFactory:
    """ä¼˜åŒ–å™¨å·¥å‚ç±»"""

    @staticmethod
    def create_optimizer(objective, space, constraints=None, name='optimization'):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        sampler = LatinHypercubeSampler(space, seed=42)
        tracker = ExperimentTracker(name, backend='memory')
        optimizer = HyperOptimizer(
            objective_fn=objective,
            space=space,
            sampler=sampler,
            constraints=constraints,
            tracker=tracker,
            maximize=False,
            seed=42
        )
        print("ä½¿ç”¨å®Œæ•´ç‰ˆhyperoptç»„ä»¶")
        return optimizer


def setup_data_manager(dataset_name='movielens100k',
                      dataset_file='dataset/20201202M100K_data_all_random.txt'):
    """è®¾ç½®æ•°æ®ç®¡ç†å™¨"""
    print(f"å‡†å¤‡æ•°æ®é›†: {dataset_name}")

    from data.loader import DatasetLoader
    from data.dataset import MovieLens100K, MovieLens1M, Netflix, AmazonMI, CiaoDVD, Epinions, FilmTrust, MovieTweetings

    dataset_classes = {
        'movielens100k': MovieLens100K,
        'movielens1m': MovieLens1M,
        'netflix': Netflix,
        'amazonmi': AmazonMI,
        'ciaodvd': CiaoDVD,
        'epinions': Epinions,
        'filmtrust': FilmTrust,
        'movietweetings': MovieTweetings
    }

    dataset_class = dataset_classes.get(dataset_name.lower(), MovieLens100K)
    DatasetLoader.register_dataset(dataset_name, dataset_class)
    print(f"âœ“ å·²æ³¨å†Œæ•°æ®é›†: {dataset_name}")

    data_config = {
        'random_seed': 42,
        'train_ratio': 0.8,
        'val_ratio': 0.1,
        'test_ratio': 0.1,
        'batch_size': 128,
        'shuffle': True,
        'center_data': True,
        'ensure_user_in_train': True
    }

    data_manager = DataManager(data_config)

    print(f"DEBUG: å°è¯•åŠ è½½æ•°æ®é›† '{dataset_name}'")
    print(f"DEBUG: æ•°æ®æ–‡ä»¶è·¯å¾„: '{dataset_file}'")

    try:
        data_manager.load_dataset(dataset_name, dataset_file)
        data_manager.preprocess()
        print(f"âœ“ æ•°æ®é›† {dataset_name} åŠ è½½å’Œé¢„å¤„ç†æˆåŠŸ")
        data_manager.print_summary()
        return data_manager
    except Exception as e:
        print(f"åŠ è½½ {dataset_name} æ•°æ®é›†å¤±è´¥: {e}")
        raise


def safe_get_trials(optimizer):
    """å®‰å…¨åœ°è·å–è¯•éªŒæ•°æ®"""
    try:
        if hasattr(optimizer, 'tracker'):
            tracker = optimizer.tracker

            if hasattr(tracker, 'trials'):
                trials = tracker.trials
                return [SafeTrial(t.config if hasattr(t, 'config') else t.get('config', {}),
                                t.score if hasattr(t, 'score') else t.get('score', float('inf')))
                       for t in trials]

            if hasattr(tracker, 'get_trials'):
                trials = tracker.get_trials()
                return [SafeTrial(t.config if hasattr(t, 'config') else t.get('config', {}),
                                t.score if hasattr(t, 'score') else t.get('score', float('inf')))
                       for t in trials]

            if hasattr(tracker, 'get_all_trials'):
                trials = tracker.get_all_trials()
                return [SafeTrial(t.config if hasattr(t, 'config') else t.get('config', {}),
                                t.score if hasattr(t, 'score') else t.get('score', float('inf')))
                       for t in trials]

            if hasattr(tracker, '_trials'):
                trials = tracker._trials
                return [SafeTrial(t.config if hasattr(t, 'config') else t.get('config', {}),
                                t.score if hasattr(t, 'score') else t.get('score', float('inf')))
                       for t in trials]

            if hasattr(tracker, 'results'):
                results = tracker.results
                if isinstance(results, dict):
                    trials = []
                    for trial_id, trial_data in results.items():
                        if 'config' in trial_data and 'score' in trial_data:
                            trials.append(SafeTrial(trial_data['config'], trial_data['score']))
                    return trials

        print("âš ï¸ æ— æ³•è·å–trialsæ•°æ®ï¼Œå°†ä½¿ç”¨ç©ºåˆ—è¡¨")
        return []

    except Exception as e:
        print(f"âš ï¸ è·å–trialsæ•°æ®æ—¶å‡ºé”™: {e}")
        return []


def safe_extract_trial_info(best_trial):
    """å®‰å…¨åœ°æå–è¯•éªŒä¿¡æ¯"""
    try:
        if best_trial is None:
            return None, None

        # å¦‚æœæ˜¯SafeTrialå¯¹è±¡
        if isinstance(best_trial, SafeTrial):
            return best_trial.config, best_trial.score

        # å¦‚æœæœ‰configå’Œscoreå±æ€§
        if hasattr(best_trial, 'config') and hasattr(best_trial, 'score'):
            return best_trial.config, best_trial.score

        # å¦‚æœæ˜¯å­—å…¸
        if isinstance(best_trial, dict):
            config = best_trial.get('config', {})
            score = best_trial.get('score', float('inf'))
            return config, score

        # å…¶ä»–æƒ…å†µ
        print(f"è­¦å‘Šï¼šæ— æ³•è¯†åˆ«çš„best_trialç±»å‹: {type(best_trial)}")
        print(f"best_trialå†…å®¹: {best_trial}")
        return {}, float('inf')

    except Exception as e:
        print(f"æå–è¯•éªŒä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return {}, float('inf')


# def run_enhanced_hpl_optimization(n_trials=150, dataset_name='movielens100k', dataset_file='dataset/20201202M100K_data_all_random.txt'):
def run_enhanced_hpl_optimization(n_trials=5, dataset_name='movielens100k', dataset_file='dataset/20201202M100K_data_all_random.txt'):
    """è¿è¡ŒHPLå¢å¼ºä¼˜åŒ–"""
    print("å¼€å§‹HPLå¢å¼ºä¼˜åŒ– (æ‰©å±•å‚æ•°ç©ºé—´ï¼Œ150æ¬¡è¯•éªŒ)...")

    # ğŸ¯ æ ¹æ®æ•°æ®é›†å’Œæ¨¡å‹ç‰¹å¾è®¾ç½®æ—©åœå‚æ•°
    def calculate_early_stopping_params(dataset_name, n_trials, data_stats=None):
        """æ ¹æ®æ•°æ®é›†ç‰¹å¾åŠ¨æ€è®¡ç®—æ—©åœå‚æ•°"""

        # ğŸ”§ ä¿®æ”¹ï¼šå°è¯•ä»data_statsè·å–çœŸå®æ•°æ®å¤§å°
        if data_stats is not None:
            try:
                # å¦‚æœdata_statsåŒ…å«è®­ç»ƒé›†å¤§å°ä¿¡æ¯
                if hasattr(data_stats, 'get_splits'):
                    train_data, _, _ = data_stats.get_splits()
                    estimated_size = len(train_data)
                    print(f"ğŸ“Š ä½¿ç”¨çœŸå®è®­ç»ƒé›†å¤§å°: {estimated_size:,}")
                elif hasattr(data_stats, 'train_data'):
                    estimated_size = len(data_stats.train_data)
                    print(f"ğŸ“Š ä½¿ç”¨çœŸå®è®­ç»ƒé›†å¤§å°: {estimated_size:,}")
                elif isinstance(data_stats, dict) and 'train_size' in data_stats:
                    estimated_size = data_stats['train_size']
                    print(f"ğŸ“Š ä½¿ç”¨çœŸå®è®­ç»ƒé›†å¤§å°: {estimated_size:,}")
                else:
                    # å›é€€åˆ°é¢„ä¼°å€¼
                    raise AttributeError("æ— æ³•ä»data_statsè·å–è®­ç»ƒé›†å¤§å°")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è·å–çœŸå®æ•°æ®å¤§å°({e})ï¼Œä½¿ç”¨é¢„ä¼°å€¼")
                # æ•°æ®é›†è§„æ¨¡æ˜ å°„ (é¢„ä¼°) - ä½œä¸ºåå¤‡æ–¹æ¡ˆ
                dataset_size_map = {
                    'movielens100k': 100000,
                    'movielens1m': 1000000,
                    'netflix': 100000000,
                    'amazonmi': 50000,
                    'filmtrust': 35000,
                }
                estimated_size = dataset_size_map.get(dataset_name.lower(), 100000)
        else:
            # åŸæœ‰çš„é¢„ä¼°é€»è¾‘
            dataset_size_map = {
                'movielens100k': 100000,
                'movielens1m': 1000000,
                'netflix': 100000000,
                'amazonmi': 50000,
                'filmtrust': 35000,
            }
            estimated_size = dataset_size_map.get(dataset_name.lower(), 100000)
            print(f"ğŸ“Š ä½¿ç”¨é¢„ä¼°æ•°æ®é›†å¤§å°: {estimated_size:,}")

        # æ¨¡å‹è®­ç»ƒæ—©åœ - åŸºäºæ•°æ®è§„æ¨¡å’ŒHPLç‰¹æ€§
        if estimated_size < 10000:  # å°æ•°æ®é›†
            model_patience = 8
            # model_patience = 3  # å¿«é€Ÿ
        elif estimated_size < 100000:  # ä¸­ç­‰æ•°æ®é›†
            # model_patience = 5  # å¿«é€Ÿ
            model_patience = 12  # æ ‡å‡†è®¾ç½®
        else:  # å¤§æ•°æ®é›†
            model_patience = 15
            # model_patience = 6 # å¿«é€Ÿ

        # HPLæŸå¤±å‡½æ•°é€šå¸¸æ”¶æ•›è¾ƒæ…¢ï¼Œå¢åŠ 20%
        model_patience = int(model_patience * 1.2)
        # model_patience = int(model_patience * 0.8) # å¿«é€Ÿ

        # è¶…å‚æ•°ä¼˜åŒ–æ—©åœ - åŸºäºæœç´¢é¢„ç®—
        if n_trials <= 50:
            hyperopt_patience = max(10, n_trials // 8)    # å°é¢„ç®—ï¼š12.5%
        elif n_trials <= 200:
            hyperopt_patience = max(16, n_trials // 12)   # ä¸­é¢„ç®—ï¼š8.3%
        else:
            hyperopt_patience = max(20, n_trials // 15)  # å¤§é¢„ç®—ï¼š6.7%

        return model_patience, hyperopt_patience

    # è®¡ç®—æ—©åœå‚æ•°
    # é¦–å…ˆåŠ è½½æ•°æ®ç®¡ç†å™¨
    data_manager = setup_data_manager(dataset_name, dataset_file)

    # è®¡ç®—æ—©åœå‚æ•°ï¼ˆä½¿ç”¨çœŸå®æ•°æ®ï¼‰
    MODEL_EARLY_STOPPING, HYPEROPT_EARLY_STOPPING = calculate_early_stopping_params(
        dataset_name, n_trials, data_manager  # ä¼ å…¥data_manager
    )

    print(f"ğŸ“Š æ—©åœå‚æ•°è®¾ç½®:")
    print(f"   æ¨¡å‹è®­ç»ƒæ—©åœ: {MODEL_EARLY_STOPPING} epochs")
    print(f"   ä¼˜åŒ–æœç´¢æ—©åœ: {HYPEROPT_EARLY_STOPPING} trials")

    # data_manager = setup_data_manager(dataset_name, dataset_file)
    result_manager = DatasetAwareResultManager(dataset_name, dataset_file)
    monitor = OptimizationMonitor(result_manager, 'enhanced_hpl_optimization')

    # ä½¿ç”¨å®é™…æ•°æ®ç»Ÿè®¡æ›´æ–°æ—©åœå‚æ•°
    data_stats = data_manager.get_statistics()
    actual_data_size = len(data_manager.get_splits()[0])  # è®­ç»ƒé›†å¤§å°

    if actual_data_size < 1000:
        MODEL_EARLY_STOPPING = max(6, MODEL_EARLY_STOPPING - 3)
        print(f"   ğŸ”§ æ£€æµ‹åˆ°å°æ•°æ®é›†({actual_data_size}æ¡)ï¼Œè°ƒæ•´æ¨¡å‹æ—©åœä¸º: {MODEL_EARLY_STOPPING}")
    elif actual_data_size > 500000:
        MODEL_EARLY_STOPPING = min(25, MODEL_EARLY_STOPPING + 3)
        print(f"   ğŸ”§ æ£€æµ‹åˆ°å¤§æ•°æ®é›†({actual_data_size}æ¡)ï¼Œè°ƒæ•´æ¨¡å‹æ—©åœä¸º: {MODEL_EARLY_STOPPING}")

    objective = HPLObjectiveFunction(data_manager)
    space = ParameterSpaceFactory.create_enhanced_hpl_space()
    constraints = ConstraintFactory.create_hpl_constraints()

    optimizer = OptimizerFactory.create_optimizer(
        objective, space, constraints, 'enhanced_hpl_optimization'
    )

    # æµ‹è¯•ç›®æ ‡å‡½æ•°æ˜¯å¦æ­£å¸¸å·¥ä½œ
    print(f"\nğŸ§ª æµ‹è¯•ç›®æ ‡å‡½æ•°...")
    try:
        # ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨numpyçš„éšæœºæ•°ç”Ÿæˆå™¨
        import numpy as np
        random_state = np.random.RandomState(42)  # ä½¿ç”¨å›ºå®šç§å­ä»¥ä¾¿å¤ç°
        # ç›´æ¥ä½¿ç”¨sampleè¿”å›çš„é…ç½®ï¼Œä¸è¦å°è¯•ç´¢å¼•è®¿é—®
        test_config = space.sample(random_state)
        test_score = objective(test_config)
        print(f"   æµ‹è¯•é…ç½®: {test_config}")
        print(f"   æµ‹è¯•åˆ†æ•°: {test_score}")
        if test_score == 10.0:
            print("   âš ï¸ è­¦å‘Šï¼šç›®æ ‡å‡½æ•°è¿”å›é»˜è®¤é”™è¯¯å€¼ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
        else:
            print("   âœ… ç›®æ ‡å‡½æ•°å·¥ä½œæ­£å¸¸")
    except Exception as e:
        print(f"   âŒ ç›®æ ‡å‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    start_time = time.time()
    print(f"\nğŸš€ å¼€å§‹ä¼˜åŒ–ï¼Œç›®æ ‡è¯•éªŒæ¬¡æ•°: {n_trials}")
    print(f"   æ—©åœè½®æ•°: {HYPEROPT_EARLY_STOPPING}")

    try:
        best_trial = optimizer.optimize(
            n_trials=n_trials,
            no_improvement_rounds=HYPEROPT_EARLY_STOPPING,
            batch_size=1
        )
        print(f"âœ… ä¼˜åŒ–å®Œæˆï¼Œè¿”å›çš„best_trialç±»å‹: {type(best_trial)}")
        print(f"   best_trialå†…å®¹: {best_trial}")
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–è¿‡ç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        best_trial = None

    end_time = time.time()

    # å®‰å…¨åœ°æå–ç»“æœ
    best_config, best_score = safe_extract_trial_info(best_trial)
    print(f"ğŸ“Š æå–çš„æœ€ä½³é…ç½®: {best_config}")
    print(f"ğŸ“Š æå–çš„æœ€ä½³åˆ†æ•°: {best_score}")

    print(f"\nHPLå¢å¼ºä¼˜åŒ–ç»“æœ:")
    if best_config:
        print(f"æœ€ä½³é…ç½®: {best_config}")
        print(f"æœ€ä½³RMSE: {best_score:.4f}")
        print(f"ä¼˜åŒ–è€—æ—¶: {end_time - start_time:.2f}ç§’")

        # è·å–è¯•éªŒç»“æœ
        trials = safe_get_trials(optimizer)
        print(f"ğŸ“Š è·å–åˆ° {len(trials)} ä¸ªè¯•éªŒç»“æœ")

        # è·å–ä¼˜åŒ–å™¨ç»“æœ
        optimizer_results = {}
        if hasattr(optimizer, 'get_results'):
            optimizer_results = optimizer.get_results()
        elif hasattr(optimizer, 'tracker') and hasattr(optimizer.tracker, 'get_trials'):
            tracker_trials = optimizer.tracker.get_trials()
            optimizer_results = {'trials': tracker_trials}

        # è®¡ç®—å®é™…æ‰§è¡Œçš„è¯•éªŒæ¬¡æ•°
        actual_trials = len(trials)
        if actual_trials == 0:
            print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰è·å–åˆ°ä»»ä½•è¯•éªŒç»“æœï¼")
            # å°è¯•ä»ä¼˜åŒ–å™¨ç›´æ¥è·å–
            if hasattr(optimizer, 'tracker') and hasattr(optimizer.tracker, 'trials'):
                direct_trials = optimizer.tracker.trials
                print(f"   ä»trackerç›´æ¥è·å–åˆ° {len(direct_trials)} ä¸ªè¯•éªŒ")
                trials = [SafeTrial(t.config if hasattr(t, 'config') else {},
                                  t.score if hasattr(t, 'score') else float('inf'))
                         for t in direct_trials]
                actual_trials = len(trials)

        optimization_results = {
            'best_config': best_config,
            'best_score': best_score,
            'optimization_time': end_time - start_time,
            'n_trials': n_trials,
            'actual_trials': actual_trials,  # å®é™…æ‰§è¡Œçš„è¯•éªŒæ¬¡æ•°
            'early_stopping_config': {
                'model_early_stopping': MODEL_EARLY_STOPPING,
                'hyperopt_early_stopping': HYPEROPT_EARLY_STOPPING,
                'actual_data_size': actual_data_size,
                'estimated_data_size': 100000  # é»˜è®¤ä¼°è®¡å¤§å°
            },
            'parameter_space': {
                'learning_rate': {'min': 0.005, 'max': 0.15, 'scale': 'log'},
                'latent_factors': {'min': 8, 'max': 120, 'step': 8},
                'lambda_reg': {'min': 0.0001, 'max': 0.05, 'scale': 'log'},
                'delta1': {'min': 0.05, 'max': 1.0},
                'delta2': {'min': 0.3, 'max': 3.5},
                'l_max': {'min': 1.5, 'max': 6.0},
                'c_sigmoid': {'min': 0.2, 'max': 3.0}
            },
            'constraints': ['delta1 < delta2'],
            'optimizer_results': optimizer_results,
            'all_trials': [trial.to_dict() for trial in trials]
        }

        result_manager.save_experiment_results(
            results=optimization_results,
            experiment_type='enhanced_hpl_optimization',
            metadata={
                'algorithm': 'Latin Hypercube Sampling',
                'n_trials': n_trials,
                'early_stopping_strategy': {
                    'model_patience': MODEL_EARLY_STOPPING,
                    'hyperopt_patience': HYPEROPT_EARLY_STOPPING,
                    'strategy': 'adaptive_based_on_dataset_size',
                    'hpl_adjustment': '+20% for slow convergence'
                },
                'data_characteristics': {
                    'dataset_name': dataset_name,
                    'training_samples': actual_data_size,
                    'n_users': data_stats.get('n_users', 'unknown'),
                    'n_items': data_stats.get('n_items', 'unknown')
                }
            }
        )

        # ğŸ¯ ä½¿ç”¨æœ€ä½³å‚æ•°è¿›è¡Œå®Œæ•´æ€§èƒ½è¯„ä¼°
        print(f"\nğŸ” å¼€å§‹ä½¿ç”¨æœ€ä½³å‚æ•°è¿›è¡Œå®Œæ•´æ€§èƒ½è¯„ä¼°...")
        try:
            evaluator = BestParameterEvaluator(data_manager, best_config)
            best_model, training_time = evaluator.train_best_model(n_epochs=100)

            # è®¡ç®—æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡
            performance_metrics = evaluator.evaluate_all_metrics(best_model)

            # å°†æ€§èƒ½æŒ‡æ ‡æ·»åŠ åˆ°ç»“æœä¸­
            optimization_results['performance_evaluation'] = {
                'training_time': training_time,
                'metrics': performance_metrics,
                'evaluation_timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
            }

            # é‡æ–°ä¿å­˜åŒ…å«æ€§èƒ½è¯„ä¼°çš„ç»“æœ
            result_manager.save_experiment_results(
                results=optimization_results,
                experiment_type='enhanced_hpl_optimization_with_metrics',
                metadata={
                    'algorithm': 'Latin Hypercube Sampling',
                    'n_trials': n_trials,
                    'includes_performance_evaluation': True,
                    'evaluation_metrics': list(performance_metrics.keys()),
                    'early_stopping_strategy': {
                        'model_patience': MODEL_EARLY_STOPPING,
                        'hyperopt_patience': HYPEROPT_EARLY_STOPPING,
                        'strategy': 'adaptive_based_on_dataset_size',
                        'hpl_adjustment': '+20% for slow convergence'
                    },
                    'data_characteristics': {
                        'dataset_name': dataset_name,
                        'training_samples': actual_data_size,
                        'n_users': data_stats.get('n_users', 'unknown'),
                        'n_items': data_stats.get('n_items', 'unknown')
                    }
                }
            )

            print(f"\nğŸ“Š æ€§èƒ½è¯„ä¼°ç»“æœæ‘˜è¦:")
            print(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")

            # æ˜¾ç¤ºä¸»è¦æŒ‡æ ‡
            key_metrics = ['RMSE', 'MAE', 'HitRate@10', 'Precision@10', 'Recall@10', 'NDCG@10']
            for metric in key_metrics:
                if metric in performance_metrics and performance_metrics[metric] is not None:
                    print(f"   {metric}: {performance_metrics[metric]:.4f}")

        except Exception as e:
            print(f"âš ï¸ æ€§èƒ½è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # å³ä½¿è¯„ä¼°å¤±è´¥ï¼Œä¹Ÿä¿å­˜ä¼˜åŒ–ç»“æœ
            result_manager.save_experiment_results(
                results=optimization_results,
                experiment_type='enhanced_hpl_optimization',
                metadata={
                    'algorithm': 'Latin Hypercube Sampling',
                    'n_trials': n_trials,
                    'performance_evaluation_failed': True,
                    'early_stopping_strategy': {
                        'model_patience': MODEL_EARLY_STOPPING,
                        'hyperopt_patience': HYPEROPT_EARLY_STOPPING,
                        'strategy': 'adaptive_based_on_dataset_size',
                        'hpl_adjustment': '+20% for slow convergence'
                    },
                    'data_characteristics': {
                        'dataset_name': dataset_name,
                        'training_samples': actual_data_size,
                        'n_users': data_stats.get('n_users', 'unknown'),
                        'n_items': data_stats.get('n_items', 'unknown')
                    }
                }
            )

        safe_best_trial = SafeTrial(best_config, best_score)
    else:
        print("ä¼˜åŒ–å¤±è´¥ï¼Œæœªè·å¾—æœ‰æ•ˆç»“æœ")
        safe_best_trial = None

    # ç”Ÿæˆç›‘æ§æŠ¥å‘Šï¼ˆä½†ä¸è¦†ç›–ä¸»è¦ç»“æœï¼‰
    try:
        monitor.generate_final_report()
    except Exception as e:
        print(f"âš ï¸ ç›‘æ§æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

    return optimizer, safe_best_trial, result_manager


def guess_dataset_type(filename):
    """æ ¹æ®æ–‡ä»¶åçŒœæµ‹æ•°æ®é›†ç±»å‹"""
    filename = filename.lower()

    if 'm100k' in filename or 'movielens100k' in filename:
        return 'movielens100k'
    elif 'netflix' in filename:
        return 'netflix'
    elif 'm1m' in filename or 'movielens1m' in filename:
        return 'movielens1m'
    elif 'amazon' in filename and ('musical' in filename or 'mi' in filename):
        return 'amazonmi'
    elif 'ciaodvd' in filename:
        return 'ciaodvd'
    elif 'epinions' in filename:
        return 'epinions'
    elif 'filmtrust' in filename:
        return 'filmtrust'
    elif 'movietweetings' in filename:
        return 'movietweetings'
    else:
        return filename


def check_file_format(file_path):
    """æ£€æŸ¥æ–‡ä»¶æ ¼å¼å¹¶æ˜¾ç¤ºå‰å‡ è¡Œ"""
    try:
        print(f"\næ£€æŸ¥æ–‡ä»¶æ ¼å¼: {os.path.basename(file_path)}")
        print("1. æ–‡ä»¶æ ¼å¼ä¸ºæ¯è¡Œ: [user_id, movie_id, rating]")

        with open(file_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f.readlines()[:10]]

        print("æ–‡ä»¶å‰10è¡Œå†…å®¹:")
        for i, line in enumerate(lines, 1):
            print(f"{i}: {line}")

        confirm = input("æ˜¯å¦ç»§ç»­ä½¿ç”¨æ­¤æ–‡ä»¶? (y/n, é»˜è®¤y): ").strip().lower()
        if confirm == 'n':
            return None
        return file_path
    except Exception as e:
        print(f"æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ—¶å‡ºé”™: {e}")
        return None


def scan_available_datasets(dataset_dir):
    """æ‰«æå¹¶åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ•°æ®é›†"""
    print(f"\nğŸ” æ‰«ææ•°æ®é›†ç›®å½•: {dataset_dir}")

    if not os.path.exists(dataset_dir):
        print(f"âš ï¸ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {dataset_dir}")
        try:
            os.makedirs(dataset_dir, exist_ok=True)
            print(f"âœ… å·²åˆ›å»ºæ•°æ®é›†ç›®å½•: {dataset_dir}")
        except Exception as e:
            print(f"âŒ æ— æ³•åˆ›å»ºæ•°æ®é›†ç›®å½•: {e}")
            return []

    pattern = os.path.join(dataset_dir, '*.txt')
    dataset_files = glob.glob(pattern)

    if not dataset_files:
        print(f"âŒ åœ¨ {dataset_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½• .txt æ•°æ®é›†æ–‡ä»¶")
        return []

    print(f"âœ… æ‰¾åˆ° {len(dataset_files)} ä¸ªæ•°æ®é›†æ–‡ä»¶")
    return dataset_files


def create_dataset_mapping(dataset_files):
    """åˆ›å»ºæ–‡ä»¶ååˆ°æ•°æ®é›†ç±»å‹çš„æ˜ å°„"""
    dataset_map = {}

    print("\nğŸ“‹ å¯ç”¨æ•°æ®é›†åˆ—è¡¨:")
    print("-" * 80)
    print(f"{'åºå·':<4} {'æ–‡ä»¶å':<40} {'æ¨æµ‹ç±»å‹':<20} {'æ–‡ä»¶å¤§å°'}")
    print("-" * 80)

    for i, file_path in enumerate(dataset_files, 1):
        filename = os.path.basename(file_path)
        dataset_type = guess_dataset_type(filename)

        try:
            file_size = os.path.getsize(file_path)
            if file_size > 1024 * 1024:
                size_str = f"{file_size / (1024 * 1024):.1f}MB"
            elif file_size > 1024:
                size_str = f"{file_size / 1024:.1f}KB"
            else:
                size_str = f"{file_size}B"
        except:
            size_str = "æœªçŸ¥"

        dataset_map[str(i)] = {
            'dataset_type': dataset_type,
            'file_path': file_path,
            'filename': filename,
            'file_size': size_str
        }

        print(f"{i:<4} {filename:<40} {dataset_type:<20} {size_str}")

    print("-" * 80)
    return dataset_map


def get_user_dataset_selection(dataset_map):
    """è·å–ç”¨æˆ·çš„æ•°æ®é›†é€‰æ‹©"""
    print("\nğŸ“ æ•°æ®é›†é€‰æ‹©:")
    print("- è¾“å…¥åºå·é€‰æ‹©å•ä¸ªæ•°æ®é›† (ä¾‹å¦‚: 1)")
    print("- è¾“å…¥å¤šä¸ªåºå·é€‰æ‹©å¤šä¸ªæ•°æ®é›† (ä¾‹å¦‚: 1,3,5)")
    print("- è¾“å…¥ 'all' é€‰æ‹©æ‰€æœ‰æ•°æ®é›†")

    while True:
        choice = input("\nè¯·é€‰æ‹©æ•°æ®é›†: ").strip()

        if not choice:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆé€‰æ‹©")
            continue

        selected_datasets = []

        if choice.lower() == 'all':
            selected_datasets = list(dataset_map.values())
            print(f"âœ… å·²é€‰æ‹©æ‰€æœ‰ {len(selected_datasets)} ä¸ªæ•°æ®é›†")
            return selected_datasets

        try:
            choices = [c.strip() for c in choice.split(',')]
            for c in choices:
                if c in dataset_map:
                    selected_datasets.append(dataset_map[c])
                else:
                    print(f"âŒ æ— æ•ˆé€‰æ‹©: {c}")
                    raise ValueError("æ— æ•ˆé€‰æ‹©")

            if selected_datasets:
                print(f"âœ… å·²é€‰æ‹© {len(selected_datasets)} ä¸ªæ•°æ®é›†:")
                for ds in selected_datasets:
                    print(f"   - {ds['filename']} ({ds['dataset_type']})")
                return selected_datasets
            else:
                print("âŒ æœªé€‰æ‹©ä»»ä½•æ•°æ®é›†")

        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„åºå·")
            continue


def execute_single_dataset_experiment(dataset_info):
    """æ‰§è¡Œå•ä¸ªæ•°æ®é›†çš„HPLå¢å¼ºä¼˜åŒ–å®éªŒ"""
    dataset_name = dataset_info['dataset_type']
    dataset_file = dataset_info['file_path']
    filename = dataset_info['filename']

    print(f"\nğŸš€ å¼€å§‹å®éªŒ: {filename}")
    print(f"   æ•°æ®é›†ç±»å‹: {dataset_name}")
    print(f"   å®éªŒç±»å‹: HPLå¢å¼ºä¼˜åŒ–")

    start_time = time.time()

    try:
        optimizer, best_trial, result_manager = run_enhanced_hpl_optimization(
            n_trials= 20, # æµ‹è¯•ç”¨ 20 æ¬¡è¯•éªŒ
            dataset_name=dataset_name,
            dataset_file=dataset_file
        )

        end_time = time.time()
        duration = end_time - start_time

        # å®‰å…¨åœ°æå–è¯•éªŒä¿¡æ¯
        if best_trial:
            best_config, best_score = safe_extract_trial_info(best_trial)

            # æå–æ€§èƒ½è¯„ä¼°ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            performance_metrics = {}
            if hasattr(best_trial, 'performance_evaluation'):
                performance_metrics = best_trial.performance_evaluation.get('metrics', {})
            elif isinstance(best_trial, dict) and 'performance_evaluation' in best_trial:
                performance_metrics = best_trial['performance_evaluation'].get('metrics', {})

            completion_message = f"âœ… å•æ•°æ®é›†HPLä¼˜åŒ–å®Œæˆ\n" \
                               f"æ•°æ®é›†: {dataset_info['filename']}\n" \
                               f"æœ€ä½³RMSE: {best_score:.4f}\n" \
                               f"è€—æ—¶: {duration:.1f}ç§’"

            # å¦‚æœæœ‰æ€§èƒ½æŒ‡æ ‡ï¼Œæ·»åŠ åˆ°é€šçŸ¥ä¸­
            if performance_metrics:
                if 'RMSE' in performance_metrics and performance_metrics['RMSE'] is not None:
                    completion_message += f"\næµ‹è¯•RMSE: {performance_metrics['RMSE']:.4f}"
                if 'HitRate@10' in performance_metrics and performance_metrics['HitRate@10'] is not None:
                    completion_message += f"\nHR@10: {performance_metrics['HitRate@10']:.4f}"

            try:
                # send_sms_notification(completion_message)
                print(f"âœ… å·²å‘é€å®Œæˆé€šçŸ¥: {dataset_info['filename']}")
            except Exception as e:
                print(f"âš ï¸ å‘é€é€šçŸ¥å¤±è´¥: {e}")

            result = {
                'status': 'success',
                'best_trial': best_trial,
                'best_score': best_score,
                'best_config': best_config,
                'duration': duration,
                'dataset_info': dataset_info,
                'performance_metrics': performance_metrics  # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
            }
        else:
            result = {
                'status': 'failed',
                'error': 'No valid results obtained',
                'duration': duration,
                'dataset_info': dataset_info
            }

        print(f"âœ… {filename} å®éªŒå®Œæˆ ({duration:.1f}ç§’)")
        return result

    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time

        error_message = f"âŒ HPLä¼˜åŒ–å®éªŒå¤±è´¥\n" \
                       f"æ•°æ®é›†: {dataset_info['filename']}\n" \
                       f"é”™è¯¯: {str(e)[:50]}...\n" \
                       f"è€—æ—¶: {duration:.1f}ç§’"
        try:
            # send_sms_notification(error_message)
            print(f"âœ… å·²å‘é€é”™è¯¯é€šçŸ¥: {dataset_info['filename']}")
        except Exception as sms_e:
            print(f"âš ï¸ å‘é€é”™è¯¯é€šçŸ¥å¤±è´¥: {sms_e}")

        print(f"âŒ {filename} å®éªŒå¤±è´¥: {e}")
        traceback.print_exc()

        return {
            'status': 'failed',
            'error': str(e),
            'duration': duration,
            'dataset_info': dataset_info
        }


def generate_multi_dataset_summary(all_results):
    """ç”Ÿæˆå¤šæ•°æ®é›†å®éªŒæ±‡æ€»æŠ¥å‘Š - ä¿®å¤ç‰ˆæœ¬"""
    print("\n" + "="*80)
    print("å¤šæ•°æ®é›†HPLå¢å¼ºä¼˜åŒ–æ±‡æ€»æŠ¥å‘Š")
    print("="*80)

    total_datasets = len(all_results)
    successful_datasets = len([r for r in all_results.values() if r['status'] == 'success'])
    failed_datasets = total_datasets - successful_datasets
    total_duration = sum(r['duration'] for r in all_results.values())

    print(f"\nğŸ“Š å®éªŒç»Ÿè®¡:")
    print(f"   æ€»æ•°æ®é›†æ•°: {total_datasets}")
    print(f"   æˆåŠŸæ•°é‡: {successful_datasets}")
    print(f"   å¤±è´¥æ•°é‡: {failed_datasets}")
    print(f"   æ€»è€—æ—¶: {total_duration:.1f}ç§’ ({total_duration/60:.1f}åˆ†é’Ÿ)")
    print(f"   å¹³å‡è€—æ—¶: {total_duration/total_datasets:.1f}ç§’/æ•°æ®é›†")

    print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    print("-" * 120)
    print(f"{'æ•°æ®é›†':<30} {'çŠ¶æ€':<8} {'æœ€ä½³RMSE':<12} {'æµ‹è¯•RMSE':<12} {'HR@10':<10} {'è€—æ—¶(ç§’)':<10}")
    print("-" * 120)

    successful_results = []

    # ğŸ”§ ä¿®å¤ï¼šå¤„ç†ç»“æœæ—¶è¿›è¡Œå®‰å…¨è½¬æ¢
    processed_results = {}
    for dataset_name, result in all_results.items():
        status = "âœ…æˆåŠŸ" if result['status'] == 'success' else "âŒå¤±è´¥"

        if result['status'] == 'success':
            rmse = result.get('best_score', 'N/A')
            rmse_str = f"{rmse:.4f}" if rmse and rmse != 'N/A' else 'N/A'

            # æå–æ€§èƒ½æŒ‡æ ‡
            performance_metrics = result.get('performance_metrics', {})
            test_rmse = performance_metrics.get('RMSE', 'N/A')
            test_rmse_str = f"{test_rmse:.4f}" if test_rmse and test_rmse != 'N/A' else 'N/A'

            hr10 = performance_metrics.get('HitRate@10', 'N/A')
            hr10_str = f"{hr10:.4f}" if hr10 and hr10 != 'N/A' else 'N/A'

            # ğŸ”§ å®‰å…¨åœ°æå–é…ç½®ä¿¡æ¯
            best_config = result.get('best_config', {})
            if not isinstance(best_config, dict):
                best_config = {}

            successful_results.append((dataset_name, rmse, best_config, performance_metrics))

            # ğŸ”§ å®‰å…¨åœ°å¤„ç†best_trial
            best_trial = result.get('best_trial')
            if hasattr(best_trial, 'to_dict'):
                processed_best_trial = best_trial.to_dict()
            elif isinstance(best_trial, dict):
                processed_best_trial = best_trial
            else:
                processed_best_trial = {
                    'config': best_config,
                    'score': rmse
                }

            processed_results[dataset_name] = {
                'status': result['status'],
                'best_score': rmse,
                'best_config': best_config,
                'best_trial_dict': processed_best_trial,  # ä½¿ç”¨å¤„ç†åçš„ç‰ˆæœ¬
                'duration': result.get('duration', 0),
                'dataset_info': result.get('dataset_info', {}),
                'performance_metrics': performance_metrics  # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
            }
        else:
            rmse_str = f"é”™è¯¯: {result.get('error', 'Unknown')[:20]}..."
            test_rmse_str = 'N/A'
            hr10_str = 'N/A'
            processed_results[dataset_name] = {
                'status': result['status'],
                'error': result.get('error', 'Unknown'),
                'duration': result.get('duration', 0),
                'dataset_info': result.get('dataset_info', {})
            }

        duration = result.get('duration', 0)
        print(f"{dataset_name:<30} {status:<8} {rmse_str:<12} {test_rmse_str:<12} {hr10_str:<10} {duration:<10.1f}")

    if successful_results:
        print(f"\nğŸ† æœ€ä½³è¡¨ç°æ’åº (æŒ‰éªŒè¯RMSE):")
        successful_results.sort(key=lambda x: x[1])
        for i, (dataset_name, rmse, config, metrics) in enumerate(successful_results, 1):
            print(f"   {i}. {dataset_name}: éªŒè¯RMSE={rmse:.4f}")

            # æ˜¾ç¤ºæµ‹è¯•æ€§èƒ½æŒ‡æ ‡
            if metrics:
                test_rmse = metrics.get('RMSE', 'N/A')
                hr10 = metrics.get('HitRate@10', 'N/A')
                precision10 = metrics.get('Precision@10', 'N/A')
                recall10 = metrics.get('Recall@10', 'N/A')
                ndcg10 = metrics.get('NDCG@10', 'N/A')

                print(f"      æµ‹è¯•æ€§èƒ½: RMSE={test_rmse:.4f if test_rmse != 'N/A' else 'N/A'}, "
                      f"HR@10={hr10:.4f if hr10 != 'N/A' else 'N/A'}, "
                      f"P@10={precision10:.4f if precision10 != 'N/A' else 'N/A'}")
                print(f"                R@10={recall10:.4f if recall10 != 'N/A' else 'N/A'}, "
                      f"NDCG@10={ndcg10:.4f if ndcg10 != 'N/A' else 'N/A'}")

            print(f"      æœ€ä½³é…ç½®: learning_rate={config.get('learning_rate', 'N/A'):.4f}, "
                  f"latent_factors={config.get('latent_factors', 'N/A')}, "
                  f"lambda_reg={config.get('lambda_reg', 'N/A'):.6f}")
            print(f"      HPLå‚æ•°: Î´1={config.get('delta1', 'N/A'):.3f}, Î´2={config.get('delta2', 'N/A'):.3f}, "
                  f"l_max={config.get('l_max', 'N/A'):.2f}, c_sigmoid={config.get('c_sigmoid', 'N/A'):.2f}")
            print()  # ç©ºè¡Œåˆ†éš”

    # ğŸ”§ ä½¿ç”¨å¤„ç†åçš„ç»“æœæ„å»ºæ±‡æ€»æ•°æ®
    summary_data = {
        'experiment_info': {
            'experiment_type': 'enhanced_hpl_optimization_batch',
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'total_datasets': total_datasets,
            'successful_datasets': successful_datasets,
            'failed_datasets': failed_datasets,
            'total_duration': total_duration
        },
        'detailed_results': processed_results,  # ä½¿ç”¨å¤„ç†åçš„ç»“æœ
        'parameter_space': {
            'learning_rate': {'min': 0.005, 'max': 0.15, 'scale': 'log'},
            'latent_factors': {'min': 8, 'max': 120, 'step': 8},
            'lambda_reg': {'min': 0.0001, 'max': 0.05, 'scale': 'log'},
            'delta1': {'min': 0.05, 'max': 1.0},
            'delta2': {'min': 0.3, 'max': 3.5},
            'l_max': {'min': 1.5, 'max': 6.0},
            'c_sigmoid': {'min': 0.2, 'max': 3.0}
        }
    }

    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    results_dir = Path(os.path.join(current_script_dir, 'results'))
    results_dir.mkdir(exist_ok=True)

    summary_filename = f"multi_dataset_hpl_enhanced_summary_{time.strftime('%Y%m%d_%H%M%S')}.json"
    summary_path = results_dir / summary_filename

    # ğŸ”§ ä½¿ç”¨å®‰å…¨çš„JSONä¿å­˜å‡½æ•°
    success = safe_json_dump(summary_data, summary_path)

    if success:
        print(f"\nğŸ’¾ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
        print(f"ğŸ“ å®Œæ•´è·¯å¾„: {summary_path.absolute()}")
    else:
        print(f"âš ï¸ æ±‡æ€»æŠ¥å‘Šä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™å’Œç£ç›˜ç©ºé—´")

    return summary_data


def convert_numpy_types(obj):
    """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œå¢å¼ºç‰ˆæœ¬"""
    if isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    # ğŸ”§ æ–°å¢ï¼šå¤„ç†SafeTrialå¯¹è±¡
    elif hasattr(obj, 'to_dict'):
        return convert_numpy_types(obj.to_dict())
    # ğŸ”§ æ–°å¢ï¼šå¤„ç†å…¶ä»–è‡ªå®šä¹‰å¯¹è±¡
    elif hasattr(obj, '__dict__'):
        return convert_numpy_types(obj.__dict__)
    elif hasattr(obj, 'item'):
        return obj.item()
    elif hasattr(obj, 'tolist'):
        return obj.tolist()
    # ğŸ”§ æ–°å¢ï¼šå¤„ç†numpyæ ‡é‡
    elif str(type(obj)).startswith("<class 'numpy."):
        try:
            return obj.item()
        except (ValueError, AttributeError):
            return float(obj) if hasattr(obj, '__float__') else str(obj)
    # ğŸ”§ æ–°å¢ï¼šå¤„ç†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
    elif not isinstance(obj, (str, int, float, bool, type(None))):
        try:
            return str(obj)
        except:
            return "unserializable_object"
    else:
        return obj


def safe_json_dump(data, file_path):
    """å®‰å…¨çš„JSONä¿å­˜å‡½æ•°"""
    try:
        # æ­¥éª¤1: å…ˆè½¬æ¢æ•°æ®ç±»å‹
        clean_data = convert_numpy_types(data)

        # æ­¥éª¤2: æµ‹è¯•åºåˆ—åŒ–æ˜¯å¦æˆåŠŸ
        json_str = json.dumps(clean_data, indent=2, ensure_ascii=False)

        # æ­¥éª¤3: å†™å…¥æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(json_str)
            f.flush()  # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒº

        print(f"âœ… JSONæ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_path}")
        return True

    except Exception as e:
        print(f"âŒ JSONä¿å­˜å¤±è´¥: {e}")
        print(f"å°è¯•ä¿å­˜å¤‡ç”¨æ ¼å¼...")

        # å¤‡ç”¨æ–¹æ¡ˆï¼šä¿å­˜ä¸ºæ–‡æœ¬æ ¼å¼
        try:
            backup_path = str(file_path).replace('.json', '_backup.txt')
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(f"JSONä¿å­˜å¤±è´¥ï¼ŒåŸå§‹æ•°æ®ç»“æ„ï¼š\n")
                f.write(str(data))
            print(f"âš ï¸ å·²ä¿å­˜å¤‡ç”¨æ–‡ä»¶: {backup_path}")
        except:
            print(f"âŒ å¤‡ç”¨ä¿å­˜ä¹Ÿå¤±è´¥")

        return False


def run_multiple_datasets(processed_datasets):
    """æ‰¹é‡æ‰§è¡ŒHPLå¢å¼ºä¼˜åŒ–å®éªŒ"""
    print("\n" + "="*80)
    print("ğŸš€ å¼€å§‹æ‰¹é‡æ‰§è¡ŒHPLå¢å¼ºä¼˜åŒ–å®éªŒ")
    print("="*80)

    print(f"å°†å¯¹ {len(processed_datasets)} ä¸ªæ•°æ®é›†æ‰§è¡ŒHPLå¢å¼ºä¼˜åŒ– (150æ¬¡è¯•éªŒ)")

    all_results = {}

    for i, dataset_info in enumerate(processed_datasets, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ”„ è¿›åº¦: {i}/{len(processed_datasets)}")
        print(f"{'='*60}")

        result = execute_single_dataset_experiment(dataset_info)

        dataset_key = f"{dataset_info['dataset_type']}_{os.path.basename(dataset_info['file_path'])}"
        all_results[dataset_key] = result

        if result['status'] == 'success':
            print(f"âœ… å®Œæˆ {i}/{len(processed_datasets)}: {dataset_info['filename']}")
        else:
            print(f"âŒ å¤±è´¥ {i}/{len(processed_datasets)}: {dataset_info['filename']}")

    print("\nğŸ“Š ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
    summary_data = generate_multi_dataset_summary(all_results)

    try:
        successful_count = len([r for r in all_results.values() if r['status'] == 'success'])
        success_rate = successful_count / len(all_results) * 100

        successful_rmses = [r.get('best_score', float('inf')) for r in all_results.values()
                           if r['status'] == 'success' and r.get('best_score') is not None]
        avg_rmse = sum(successful_rmses) / len(successful_rmses) if successful_rmses else float('inf')

        best_dataset = None
        best_rmse = float('inf')
        for dataset_name, result in all_results.items():
            if result['status'] == 'success' and result.get('best_score', float('inf')) < best_rmse:
                best_rmse = result.get('best_score')
                best_dataset = dataset_name

        summary_message = f"ğŸ‰ HPLå¢å¼ºä¼˜åŒ–æ‰¹é‡å®éªŒå®Œæˆ!\n" \
                         f"æ€»æ•°æ®é›†: {len(all_results)}\n" \
                         f"æˆåŠŸç‡: {success_rate:.1f}%\n" \
                         f"å¹³å‡RMSE: {avg_rmse:.4f}\n" \
                         f"æœ€ä½³æ•°æ®é›†: {best_dataset}\n" \
                         f"æœ€ä½³RMSE: {best_rmse:.4f}"

        # send_sms_notification(summary_message)
        print("âœ… å·²å‘é€æ‰¹é‡å®éªŒæ€»ç»“é€šçŸ¥")
    except Exception as e:
        print(f"âš ï¸ å‘é€æ€»ç»“é€šçŸ¥å¤±è´¥: {e}")

    print(f"\nğŸ‰ å¤šæ•°æ®é›†HPLå¢å¼ºä¼˜åŒ–å®éªŒå®Œæˆ!")
    print(f"   æˆåŠŸ: {len([r for r in all_results.values() if r['status'] == 'success'])}/{len(all_results)}")

    return True


def main():
    """ä¸»å‡½æ•° - HPLå¢å¼ºä¼˜åŒ–æ‰¹é‡å¤„ç†"""
    try:
        print("HPLæŸå¤±å‡½æ•°å¢å¼ºä¼˜åŒ–å®éªŒ")
        print("="*60)

        current_script_dir = os.path.dirname(os.path.abspath(__file__))
        results_save_dir = os.path.join(current_script_dir, 'results')
        print(f"ğŸ“‚ å½“å‰è„šæœ¬ç›®å½•: {current_script_dir}")
        print(f"ğŸ’¾ ç»“æœä¿å­˜ç›®å½•: {results_save_dir}")

        print("\nğŸ“‚ ç¬¬ä¸€æ­¥: æ•°æ®é›†æ‰«æä¸é€‰æ‹©")
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        dataset_dir = os.path.join(base_dir, 'dataset')
        print(f"ğŸ” æ•°æ®é›†æœç´¢ç›®å½•: {dataset_dir}")

        dataset_files = scan_available_datasets(dataset_dir)
        if not dataset_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°æ®é›†æ–‡ä»¶")
            return False

        dataset_map = create_dataset_mapping(dataset_files)

        selected_datasets = get_user_dataset_selection(dataset_map)
        if not selected_datasets:
            print("âŒ æ²¡æœ‰é€‰æ‹©ä»»ä½•æ•°æ®é›†")
            return False

        print("\nğŸ”§ ç¬¬äºŒæ­¥: æ–‡ä»¶æ ¼å¼æ£€æŸ¥")
        processed_datasets = []
        for dataset_info in selected_datasets:
            print(f"\næ£€æŸ¥: {dataset_info['filename']}")
            checked_file = check_file_format(dataset_info['file_path'])
            if checked_file:
                processed_datasets.append(dataset_info)
            else:
                print(f"è·³è¿‡: {dataset_info['filename']}")

        if not processed_datasets:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†ï¼Œé€€å‡ºå®éªŒ")
            return False

        print(f"\nğŸƒ ç¬¬ä¸‰æ­¥: æ‰¹é‡æ‰§è¡ŒHPLå¢å¼ºä¼˜åŒ–å®éªŒ")
        print(f"ç¡®è®¤æ‰§è¡Œ: å°†å¯¹ {len(processed_datasets)} ä¸ªæ•°æ®é›†è¿›è¡Œ150æ¬¡è¯•éªŒçš„HPLå¢å¼ºä¼˜åŒ–")
        confirm = input("æ˜¯å¦ç»§ç»­? (y/n, é»˜è®¤y): ").strip().lower()
        if confirm == 'n':
            print("å®éªŒå·²å–æ¶ˆ")
            return False

        success = run_multiple_datasets(processed_datasets)

        if success:
            print("\nâœ… æ‰€æœ‰å®éªŒå·²å®Œæˆ!")
            # send_sms_notification(f"HPLå¢å¼ºä¼˜åŒ–å®éªŒå·²å®Œæˆï¼Œå…±å¤„ç†{len(processed_datasets)}ä¸ªæ•°æ®é›†")
        else:
            print("\nâŒ å®éªŒæ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜")
            # send_sms_notification("HPLå¢å¼ºä¼˜åŒ–å®éªŒæ‰§è¡Œå¤±è´¥")

        return success

    except Exception as e:
        print(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        # send_sms_notification(f"HPLå¢å¼ºä¼˜åŒ–å®éªŒå¤±è´¥: {str(e)[:50]}...")
        return False


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)


