## ç¬¬ 2 æ­¥ï¼šæŸå¤±å‡½æ•°æ¨¡å—å®ç°æ–¹æ³•è¯¦è§£

### 1. **æ¨¡å—ç»“æ„è®¾è®¡**

é¦–å…ˆåˆ›å»ºä»¥ä¸‹æ–‡ä»¶ç»“æ„ï¼š

```
src/
â”œâ”€â”€ losses/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py           # åŸºç±»å®šä¹‰
â”‚   â”œâ”€â”€ standard.py       # æ ‡å‡†æŸå¤±å‡½æ•°ï¼ˆL1, L2ï¼‰
â”‚   â”œâ”€â”€ robust.py         # é²æ£’æŸå¤±å‡½æ•°ï¼ˆHuber, Logcoshï¼‰
â”‚   â”œâ”€â”€ hpl.py            # HPLæŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ siamod.py         # sigmoidæŸå¤±å‡½æ•°
â”‚   â””â”€â”€ utils.py         # å·¥å…·å‡½æ•°
```

### 2. **åŸºç±»è®¾è®¡ï¼ˆbase.pyï¼‰**

#### æŠ½è±¡åŸºç±» `BaseLoss` è®¾è®¡è¦ç‚¹ï¼š

**å¿…é¡»å®ç°çš„æ–¹æ³•**ï¼š

- `forward(predictions, targets)`: è®¡ç®—æŸå¤±å€¼
- `gradient(predictions, targets)`: è®¡ç®—æŸå¤±å¯¹é¢„æµ‹å€¼çš„æ¢¯åº¦
- `get_config()`: è¿”å›æŸå¤±å‡½æ•°çš„é…ç½®å‚æ•°
- `__repr__()`: è¿”å›æŸå¤±å‡½æ•°çš„å­—ç¬¦ä¸²è¡¨ç¤º

**å¯é€‰æ–¹æ³•**ï¼š

- `hessian()`: è®¡ç®—äºŒé˜¶å¯¼æ•°ï¼ˆç”¨äºç‰›é¡¿æ³•ç­‰ä¼˜åŒ–å™¨ï¼‰
- `is_differentiable_at()`: æ£€æŸ¥åœ¨æŸç‚¹æ˜¯å¦å¯å¯¼
- `plot()`: å¯è§†åŒ–æŸå¤±å‡½æ•°å½¢çŠ¶

**è®¾è®¡è€ƒè™‘**ï¼š

- æ”¯æŒæ‰¹é‡è®¡ç®—ï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
- ç»Ÿä¸€çš„æ¥å£ä¾¿äºæ¨¡å‹åˆ‡æ¢ä¸åŒæŸå¤±å‡½æ•°
- å‚æ•°éªŒè¯æœºåˆ¶

### 3. **L2 æŸå¤±å‡½æ•°å®ç°**

**æ•°å­¦å®šä¹‰**ï¼š

- æŸå¤±: `L(e) = 0.5 * eÂ²`
- æ¢¯åº¦: `âˆ‚L/âˆ‚e = e`

**å®ç°è¦ç‚¹**ï¼š

- æœ€ç®€å•çš„å®ç°ï¼Œä½œä¸ºåŸºå‡†
- æ³¨æ„ç³»æ•° 0.5 çš„ä½¿ç”¨ï¼ˆä½¿æ¢¯åº¦æ›´ç®€æ´ï¼‰
- æ— éœ€ç‰¹æ®Šçš„æ•°å€¼ç¨³å®šæ€§å¤„ç†

### 4. **L1 æŸå¤±å‡½æ•°å®ç°**

**æ•°å­¦å®šä¹‰**ï¼š

- æŸå¤±: `L(e) = |e|`
- æ¢¯åº¦: `âˆ‚L/âˆ‚e = sign(e)`

**å®ç°è¦ç‚¹**ï¼š

- åœ¨ e=0 å¤„æ¢¯åº¦ä¸è¿ç»­
- æ¢¯åº¦å®ç°æ—¶éœ€è¦å¤„ç† e=0 çš„æƒ…å†µï¼ˆå¯ä»¥è¿”å› 0 æˆ–ä½¿ç”¨æ¬¡æ¢¯åº¦ï¼‰
- ä½¿ç”¨`np.sign()`æ—¶æ³¨æ„ 0 çš„å¤„ç†

### 5. **Huber æŸå¤±å‡½æ•°å®ç°**

**æ•°å­¦å®šä¹‰**ï¼š

- å½“`|e| â‰¤ Î´`æ—¶: `L(e) = 0.5 * eÂ²`
- å½“`|e| > Î´`æ—¶: `L(e) = Î´ * |e| - 0.5 * Î´Â²`

**æ¢¯åº¦**ï¼š

- å½“`|e| â‰¤ Î´`æ—¶: `âˆ‚L/âˆ‚e = e`
- å½“`|e| > Î´`æ—¶: `âˆ‚L/âˆ‚e = Î´ * sign(e)`

**å®ç°è¦ç‚¹**ï¼š

- éœ€è¦é˜ˆå€¼å‚æ•° Î´ï¼ˆé€šå¸¸é»˜è®¤ä¸º 1.0ï¼‰
- ç¡®ä¿åœ¨`|e| = Î´`å¤„è¿ç»­ä¸”å¯å¯¼
- ä½¿ç”¨æ¡ä»¶åˆ¤æ–­æˆ–`np.where()`å®ç°åˆ†æ®µ

### 6. **Logcosh æŸå¤±å‡½æ•°å®ç°**

**æ•°å­¦å®šä¹‰**ï¼š

- æŸå¤±: `L(e) = log(cosh(e))`
- æ¢¯åº¦: `âˆ‚L/âˆ‚e = tanh(e)`

**å®ç°è¦ç‚¹**ï¼š

- å¯¹å¤§å€¼çš„æ•°å€¼ç¨³å®šæ€§å¤„ç†
- ä½¿ç”¨æ’ç­‰å¼ï¼š`log(cosh(x)) = |x| + log(2) - log(1 + exp(-2|x|))`
- æ¢¯åº¦è®¡ç®—ç›´æ¥ä½¿ç”¨`tanh()`ï¼Œä½†è¦é˜²æ­¢æ•°å€¼æº¢å‡º

### 7. **HPL æŸå¤±å‡½æ•°å®ç°**

**ä¸‰æ®µå¼è®¾è®¡**ï¼š

#### ç¬¬ä¸€æ®µï¼ˆå°è¯¯å·®ï¼‰ï¼š`|e| < Î´â‚`

- æŸå¤±: `L(e) = 0.5 * eÂ²`
- æ¢¯åº¦: `âˆ‚L/âˆ‚e = e`

#### ç¬¬äºŒæ®µï¼ˆä¸­ç­‰è¯¯å·®ï¼‰ï¼š`Î´â‚ â‰¤ |e| < Î´â‚‚`

- æŸå¤±: `L(e) = Î´â‚ * |e| - 0.5 * Î´â‚Â²`
- æ¢¯åº¦: `âˆ‚L/âˆ‚e = Î´â‚ * sign(e)`

#### ç¬¬ä¸‰æ®µï¼ˆå¤§è¯¯å·®ï¼‰ï¼š`|e| â‰¥ Î´â‚‚`

- æŸå¤±: `L(e) = L_max - (L_max - L_lin(Î´â‚‚)) * exp(-B'(|e| - Î´â‚‚))`
- æ¢¯åº¦: `âˆ‚L/âˆ‚e = C_sigmoid * Î´â‚ * exp(-B'(|e| - Î´â‚‚)) * sign(e)`

**CÂ¹ è¿ç»­æ€§ä¿è¯**ï¼š

1. **åœ¨ Î´â‚ å¤„**ï¼š

   - å‡½æ•°å€¼è¿ç»­ï¼šä¸¤æ®µåœ¨ Î´â‚ å¤„çš„å€¼ç›¸ç­‰
   - å¯¼æ•°è¿ç»­ï¼šä¸¤æ®µåœ¨ Î´â‚ å¤„çš„å¯¼æ•°éƒ½ç­‰äº Î´â‚

2. **åœ¨ Î´â‚‚ å¤„**ï¼š
   - å‡½æ•°å€¼è¿ç»­ï¼šé€šè¿‡è®¾è®¡ä¿è¯
   - å¯¼æ•°è¿ç»­ï¼šé€šè¿‡é€‰æ‹©åˆé€‚çš„ B'å‚æ•°å®ç°

**å‚æ•°è®¡ç®—**ï¼š

- `L_lin(Î´â‚‚) = Î´â‚ * Î´â‚‚ - 0.5 * Î´â‚Â²`
- `B' = C_sigmoid * Î´â‚ / (L_max - L_lin(Î´â‚‚) + Îµ)`

**å®ç°æ­¥éª¤**ï¼š

1. éªŒè¯å‚æ•°çº¦æŸï¼ˆÎ´â‚ < Î´â‚‚, L_max > L_lin(Î´â‚‚)ï¼‰
2. é¢„è®¡ç®—å¸¸é‡é¿å…é‡å¤è®¡ç®—
3. ä½¿ç”¨å‘é‡åŒ–æ“ä½œå¤„ç†æ‰¹é‡æ•°æ®
4. åˆ†åˆ«è®¡ç®—ä¸‰æ®µçš„æ©ç ï¼ˆmaskï¼‰
5. æ ¹æ®æ©ç åº”ç”¨å¯¹åº”çš„å…¬å¼

### 8. **Sigmoid-like æŸå¤±å‡½æ•°å®ç°**

**æ•°å­¦å®šä¹‰**ï¼š

- æŸå¤±: `L(e) = L_max * (1 - exp(-Î± * eÂ²))`
- æ¢¯åº¦: `âˆ‚L/âˆ‚e = 2 * Î± * L_max * e * exp(-Î± * eÂ²)`

**å®ç°è¦ç‚¹**ï¼š

- å‚æ•° Î± æ§åˆ¶å¢é•¿é€Ÿåº¦
- L_max æ˜¯æŸå¤±çš„ä¸Šç•Œ
- éœ€è¦å¤„ç†å¤§å€¼æ—¶çš„æ•°å€¼ç¨³å®šæ€§

### 9. **æ•°å€¼ç¨³å®šæ€§å¤„ç†**

**å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**ï¼š

1. **é™¤é›¶ä¿æŠ¤**ï¼š

   - æ·»åŠ å°å¸¸æ•° Îµï¼ˆå¦‚ 1e-8ï¼‰
   - åœ¨åˆ†æ¯ä¸ºé›¶çš„åœ°æ–¹ä½¿ç”¨æ¡ä»¶åˆ¤æ–­

2. **æŒ‡æ•°æº¢å‡º**ï¼š

   - å¯¹æŒ‡æ•°å‚æ•°è¿›è¡Œè£å‰ª
   - ä½¿ç”¨`np.clip()`é™åˆ¶è¾“å…¥èŒƒå›´

3. **å¯¹æ•°ä¸‹æº¢**ï¼š

   - ä½¿ç”¨`log1p()`ä»£æ›¿`log(1+x)`
   - æ·»åŠ å°å¸¸æ•°é˜²æ­¢ log(0)

4. **æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±**ï¼š
   - æ¢¯åº¦è£å‰ª
   - ä½¿ç”¨ç¨³å®šçš„æ•°å­¦ç­‰ä»·å½¢å¼

### 10. **æ¢¯åº¦éªŒè¯å®ç°**

**æ•°å€¼æ¢¯åº¦æ£€æŸ¥**ï¼š

- ä½¿ç”¨æœ‰é™å·®åˆ†è¿‘ä¼¼ï¼š`(f(x+h) - f(x-h)) / (2h)`
- é€‰æ‹©åˆé€‚çš„ h å€¼ï¼ˆå¦‚ 1e-5ï¼‰
- æ¯”è¾ƒè§£ææ¢¯åº¦å’Œæ•°å€¼æ¢¯åº¦çš„ç›¸å¯¹è¯¯å·®

**è¾¹ç•Œæƒ…å†µæµ‹è¯•**ï¼š

- åœ¨åˆ†æ®µç‚¹é™„è¿‘å¯†é›†é‡‡æ ·
- æµ‹è¯•æå¤§å’Œæå°å€¼
- éªŒè¯é›¶ç‚¹å¤„çš„è¡Œä¸º

### 11. **æ€§èƒ½ä¼˜åŒ–**

**å‘é‡åŒ–è®¡ç®—**ï¼š

- é¿å… Python å¾ªç¯ï¼Œä½¿ç”¨ NumPy å‘é‡æ“ä½œ
- é¢„è®¡ç®—å¸¸é‡
- ä½¿ç”¨`np.where()`æˆ–å¸ƒå°”ç´¢å¼•

**å†…å­˜æ•ˆç‡**ï¼š

- é¿å…åˆ›å»ºä¸å¿…è¦çš„ä¸­é—´æ•°ç»„
- ä½¿ç”¨ in-place æ“ä½œ where åˆé€‚
- è€ƒè™‘ä½¿ç”¨ç¨€ç–è¡¨ç¤ºï¼ˆå¦‚æœé€‚ç”¨ï¼‰

### 12. **å¯è§†åŒ–å·¥å…·**

**å®ç°æŸå¤±å‡½æ•°å¯è§†åŒ–**ï¼š

- ç»˜åˆ¶æŸå¤±å‡½æ•°æ›²çº¿
- ç»˜åˆ¶æ¢¯åº¦æ›²çº¿
- æ ‡æ³¨å…³é”®ç‚¹ï¼ˆå¦‚é˜ˆå€¼ä½ç½®ï¼‰
- å¯¹æ¯”ä¸åŒæŸå¤±å‡½æ•°

### 13. **é…ç½®ç®¡ç†**

**å‚æ•°ç®¡ç†**ï¼š

- ä¸ºæ¯ä¸ªæŸå¤±å‡½æ•°å®šä¹‰é»˜è®¤å‚æ•°
- æ”¯æŒå‚æ•°çš„åŠ¨æ€è°ƒæ•´
- å‚æ•°éªŒè¯å’ŒèŒƒå›´æ£€æŸ¥
- åºåˆ—åŒ–å’Œååºåˆ—åŒ–æ”¯æŒ

### 14. **æµ‹è¯•ç­–ç•¥**

**å•å…ƒæµ‹è¯•**ï¼š

- æµ‹è¯•åŸºæœ¬åŠŸèƒ½ï¼ˆforwardã€gradientï¼‰
- æµ‹è¯•ç‰¹æ®Šå€¼ï¼ˆ0ã€æ— ç©·å¤§ã€NaNï¼‰
- æµ‹è¯•æ‰¹é‡è®¡ç®—çš„æ­£ç¡®æ€§
- æµ‹è¯•å‚æ•°éªŒè¯

**é›†æˆæµ‹è¯•**ï¼š

- åœ¨ç®€å•ä¼˜åŒ–é—®é¢˜ä¸Šæµ‹è¯•
- éªŒè¯æ”¶æ•›æ€§
- æ¯”è¾ƒä¸åŒæŸå¤±å‡½æ•°çš„è¡¨ç°

### 15. **æ–‡æ¡£å’Œç¤ºä¾‹**

**æ–‡æ¡£å†…å®¹**ï¼š

- æ•°å­¦å…¬å¼å’Œæ¨å¯¼
- å‚æ•°è¯´æ˜å’Œå»ºè®®å€¼
- ä½¿ç”¨ç¤ºä¾‹
- æ€§èƒ½ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯

# æŸå¤±å‡½æ•°æ¨¡å—é¡¹ç›®è®¾ç½®æŒ‡å—

## ğŸ“ é¡¹ç›®ç»“æ„

ç¡®ä¿ä½ çš„é¡¹ç›®ç»“æ„å¦‚ä¸‹ï¼š

```
your_project/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ losses/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base.py
â”‚       â”œâ”€â”€ standard.py
â”‚       â”œâ”€â”€ robust.py
â”‚       â”œâ”€â”€ hpl.py
â”‚       â”œâ”€â”€ sigmoid.py
â”‚       â””â”€â”€ utils.py
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_usage.py
â”‚   â”œâ”€â”€ quick_start.py
â”‚   â””â”€â”€ advanced_examples.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_losses.py
â””â”€â”€ requirements.txt
```

## ğŸ”§ å®‰è£…å’Œè®¾ç½®

### 1. ä¾èµ–å®‰è£…

åˆ›å»º `requirements.txt` æ–‡ä»¶ï¼š

```txt
numpy>=1.19.0
matplotlib>=3.3.0
scipy>=1.5.0
```

å®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

### 2. æ¨¡å—å¯¼å…¥è®¾ç½®

åœ¨ä½ çš„Pythonè„šæœ¬ä¸­ï¼Œæœ‰å‡ ç§æ–¹å¼å¯¼å…¥æŸå¤±å‡½æ•°ï¼š

#### æ–¹æ³•1ï¼šç›´æ¥å¯¼å…¥ï¼ˆæ¨èï¼‰

```python
# å‡è®¾ä½ çš„è„šæœ¬åœ¨é¡¹ç›®æ ¹ç›®å½•
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from losses import L1Loss, L2Loss, HuberLoss, HybridPiecewiseLoss
```

#### æ–¹æ³•2ï¼šä½¿ç”¨ç›¸å¯¹å¯¼å…¥

```python
# å¦‚æœä½ çš„è„šæœ¬åœ¨ examples/ ç›®å½•ä¸‹
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from losses import *
```

#### æ–¹æ³•3ï¼šè®¾ç½®PYTHONPATHç¯å¢ƒå˜é‡

```bash
export PYTHONPATH="${PYTHONPATH}:/path/to/your_project/src"
```

ç„¶åç›´æ¥å¯¼å…¥ï¼š

```python
from losses import L1Loss, L2Loss, HuberLoss, HybridPiecewiseLoss
```

## ğŸš€ åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹

### 1. ç®€å•æŸå¤±è®¡ç®—

```python
import numpy as np
from losses import L2Loss, HybridPiecewiseLoss

# åˆ›å»ºæŸå¤±å‡½æ•°
l2 = L2Loss()
hpl = HybridPiecewiseLoss(delta1=0.5, delta2=2.0, l_max=3.0)

# å‡†å¤‡æ•°æ®
predictions = np.array([1.0, 2.0, 3.0])
targets = np.array([1.1, 1.8, 3.2])

# è®¡ç®—æŸå¤±
l2_loss = l2.forward(predictions, targets)
hpl_loss = hpl.forward(predictions, targets)

print(f"L2 æŸå¤±: {l2_loss:.4f}")
print(f"HPL æŸå¤±: {hpl_loss:.4f}")

# è®¡ç®—æ¢¯åº¦
l2_grad = l2.gradient(predictions, targets)
hpl_grad = hpl.gradient(predictions, targets)

print(f"L2 æ¢¯åº¦: {l2_grad}")
print(f"HPL æ¢¯åº¦: {hpl_grad}")
```

### 2. åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­ä½¿ç”¨

```python
import numpy as np
from losses import HybridPiecewiseLoss

class SimpleLinearRegression:
    def __init__(self, loss_function, learning_rate=0.01):
        self.loss_function = loss_function
        self.learning_rate = learning_rate
        self.weights = None
        self.bias = None
    
    def fit(self, X, y, epochs=1000):
        n_samples, n_features = X.shape
        
        # åˆå§‹åŒ–å‚æ•°
        self.weights = np.random.normal(0, 0.01, n_features)
        self.bias = 0.0
        
        for epoch in range(epochs):
            # å‰å‘ä¼ æ’­
            predictions = X @ self.weights + self.bias
            
            # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
            loss = self.loss_function.forward(predictions, y)
            grad = self.loss_function.gradient(predictions, y)
            
            # æ›´æ–°å‚æ•°
            dw = X.T @ grad / n_samples
            db = np.mean(grad)
            
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            if epoch % 100 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.6f}")
    
    def predict(self, X):
        return X @ self.weights + self.bias

# ä½¿ç”¨ç¤ºä¾‹
hpl_loss = HybridPiecewiseLoss(delta1=0.5, delta2=2.0, l_max=3.0)
model = SimpleLinearRegression(hpl_loss, learning_rate=0.001)

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
X = np.random.randn(100, 3)
y = X @ [1.5, -2.0, 0.5] + 0.1 * np.random.randn(100)

# è®­ç»ƒæ¨¡å‹
model.fit(X, y, epochs=500)

# é¢„æµ‹
predictions = model.predict(X)
```

### 3. æŸå¤±å‡½æ•°å¯¹æ¯”

```python
from losses import (
    L1Loss, L2Loss, HuberLoss, LogcoshLoss,
    HybridPiecewiseLoss, SigmoidLikeLoss,
    plot_loss_comparison
)

# åˆ›å»ºæŸå¤±å‡½æ•°å­—å…¸
loss_functions = {
    'L2 (MSE)': L2Loss(),
    'L1 (MAE)': L1Loss(),
    'Huber': HuberLoss(delta=1.0),
    'Logcosh': LogcoshLoss(),
    'HPL': HybridPiecewiseLoss(delta1=0.5, delta2=2.0, l_max=3.0),
    'Sigmoid-like': SigmoidLikeLoss(alpha=1.0, l_max=3.0)
}

# ç»˜åˆ¶å¯¹æ¯”å›¾
plot_loss_comparison(
    loss_functions,
    error_range=(-4, 4),
    show_gradient=True,
    save_path='loss_comparison.png'
)
```

### 4. æ¢¯åº¦éªŒè¯

```python
from losses import HybridPiecewiseLoss, check_gradient

# åˆ›å»ºæŸå¤±å‡½æ•°
hpl = HybridPiecewiseLoss(delta1=0.5, delta2=2.0, l_max=3.0)

# æ£€æŸ¥æ¢¯åº¦
result = check_gradient(hpl)

if result['passed']:
    print("âœ… æ¢¯åº¦æ£€æŸ¥é€šè¿‡")
else:
    print("âŒ æ¢¯åº¦æ£€æŸ¥å¤±è´¥")
    print(f"æœ€å¤§è¯¯å·®: {result['max_abs_error']:.2e}")
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰æŸå¤±å‡½æ•°

```python
from losses.base import BaseLoss
import numpy as np

class CustomLoss(BaseLoss):
    def __init__(self, alpha=1.0):
        super().__init__("Custom")
        self.alpha = alpha
        self._config = {'alpha': alpha}
    
    def forward(self, predictions, targets):
        errors = predictions - targets
        return np.mean(self.alpha * errors**2 + np.log(1 + np.exp(errors)))
    
    def gradient(self, predictions, targets):
        errors = predictions - targets
        sigmoid = 1 / (1 + np.exp(-errors))
        return 2 * self.alpha * errors + sigmoid

# ä½¿ç”¨è‡ªå®šä¹‰æŸå¤±å‡½æ•°
custom_loss = CustomLoss(alpha=0.5)
```

### 2. æŸå¤±å‡½æ•°é…ç½®ç®¡ç†

```python
from losses import HybridPiecewiseLoss

# åˆ›å»ºæŸå¤±å‡½æ•°
hpl = HybridPiecewiseLoss(delta1=0.5, delta2=2.0, l_max=3.0)

# ä¿å­˜é…ç½®
hpl.save_config('hpl_config.json')

# è·å–é…ç½®
config = hpl.get_config()
print(config)

# åˆ›å»ºå…·æœ‰ç›¸åŒé…ç½®çš„æ–°å®ä¾‹
new_hpl = HybridPiecewiseLoss(**config)
```

### 3. æ‰¹é‡å¤„ç†

```python
from losses import HybridPiecewiseLoss
import numpy as np

hpl = HybridPiecewiseLoss(delta1=0.5, delta2=2.0, l_max=3.0)

# å¤§æ‰¹é‡æ•°æ®
batch_size = 1000
predictions = np.random.randn(batch_size)
targets = np.random.randn(batch_size)

# æ‰¹é‡è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
loss = hpl.forward(predictions, targets)
gradients = hpl.gradient(predictions, targets)

print(f"æ‰¹é‡æŸå¤±: {loss:.6f}")
print(f"æ¢¯åº¦å½¢çŠ¶: {gradients.shape}")
```

## ğŸ§ª æµ‹è¯•

åˆ›å»ºæµ‹è¯•æ–‡ä»¶ `tests/test_losses.py`ï¼š

```python
import unittest
import numpy as np
from losses import L1Loss, L2Loss, HuberLoss, HybridPiecewiseLoss, check_gradient

class TestLossFunctions(unittest.TestCase):
    
    def setUp(self):
        self.predictions = np.array([1.0, 2.0, 3.0])
        self.targets = np.array([1.1, 1.9, 3.1])
    
    def test_l2_loss(self):
        loss_fn = L2Loss()
        loss = loss_fn.forward(self.predictions, self.targets)
        self.assertGreater(loss, 0)
    
    def test_gradient_check(self):
        hpl = HybridPiecewiseLoss(delta1=0.5, delta2=2.0, l_max=3.0)
        result = check_gradient(hpl)
        self.assertTrue(result['passed'])
    
    def test_hpl_continuity(self):
        hpl = HybridPiecewiseLoss(delta1=0.5, delta2=2.0, l_max=3.0)
        continuity = hpl.verify_continuity()
        self.assertTrue(all(continuity.values()))

if __name__ == '__main__':
    unittest.main()
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
python -m pytest tests/
```

## ğŸ’¡ ä½¿ç”¨æŠ€å·§

### 1. é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°

- **L2 (MSE)**: é€‚ç”¨äºå™ªå£°æœä»æ­£æ€åˆ†å¸ƒçš„å›å½’ä»»åŠ¡
- **L1 (MAE)**: å¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼Œé€‚ç”¨äºæœ‰å¼‚å¸¸å€¼çš„æ•°æ®
- **Huber**: ç»“åˆL1å’ŒL2çš„ä¼˜ç‚¹ï¼Œå¹³è¡¡é²æ£’æ€§å’Œæ•ˆç‡
- **HPL**: å¯ä»¥é€šè¿‡å‚æ•°è°ƒèŠ‚é€‚åº”ä¸åŒçš„å™ªå£°åˆ†å¸ƒ
- **Logcosh**: ç±»ä¼¼Huberä½†æ›´å¹³æ»‘

### 2. HPLå‚æ•°è°ƒä¼˜å»ºè®®

- **delta1**: æ§åˆ¶ä»äºŒæ¬¡åˆ°çº¿æ€§çš„è½¬æ¢ç‚¹ï¼Œé€šå¸¸è®¾ä¸º0.3-0.8
- **delta2**: æ§åˆ¶ä»çº¿æ€§åˆ°é¥±å’Œçš„è½¬æ¢ç‚¹ï¼Œé€šå¸¸è®¾ä¸º1.5-3.0
- **l_max**: æŸå¤±ä¸Šç•Œï¼Œåº”è¯¥å¤§äºçº¿æ€§æ®µåœ¨delta2å¤„çš„å€¼
- **c_sigmoid**: æ§åˆ¶é¥±å’Œé€Ÿåº¦ï¼Œé€šå¸¸è®¾ä¸º0.5-2.0

### 3. æ€§èƒ½ä¼˜åŒ–

- ä½¿ç”¨å‘é‡åŒ–æ“ä½œé¿å…Pythonå¾ªç¯
- å¯¹äºå¤§æ‰¹é‡æ•°æ®ï¼Œè€ƒè™‘å†…å­˜ä½¿ç”¨
- åˆ©ç”¨æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

### 4. è°ƒè¯•æŠ€å·§

- ä½¿ç”¨ `check_gradient()` éªŒè¯æ¢¯åº¦è®¡ç®—
- ä½¿ç”¨ `plot_loss_comparison()` å¯è§†åŒ–æŸå¤±å‡½æ•°è¡Œä¸º
- æ£€æŸ¥æŸå¤±å‡½æ•°çš„è¿ç»­æ€§å’Œå¯å¯¼æ€§

## âš ï¸ å¸¸è§é—®é¢˜

### 1. å¯¼å…¥é”™è¯¯

**é—®é¢˜**: `ModuleNotFoundError: No module named 'losses'`

**è§£å†³**:
- æ£€æŸ¥æ–‡ä»¶è·¯å¾„
- ç¡®ä¿ `__init__.py` æ–‡ä»¶å­˜åœ¨
- æ­£ç¡®è®¾ç½® Python è·¯å¾„

### 2. æ¢¯åº¦æ£€æŸ¥å¤±è´¥

**é—®é¢˜**: æ¢¯åº¦æ£€æŸ¥ä¸é€šè¿‡

**è§£å†³**:
- æ£€æŸ¥æ¢¯åº¦è®¡ç®—å…¬å¼
- ç¡®ä¿å¤„ç†äº†è¾¹ç•Œæƒ…å†µ
- è°ƒæ•´æ•°å€¼å·®åˆ†æ­¥é•¿

### 3. æ€§èƒ½é—®é¢˜

**é—®é¢˜**: æŸå¤±è®¡ç®—é€Ÿåº¦æ…¢

**è§£å†³**:
- ä½¿ç”¨NumPyå‘é‡åŒ–æ“ä½œ
- é¿å…ä¸å¿…è¦çš„æ•°ç»„å¤åˆ¶
- è€ƒè™‘ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç±»å‹

## ğŸ“š è¿›ä¸€æ­¥å­¦ä¹ 

- é˜…è¯»æŸå¤±å‡½æ•°çš„æ•°å­¦æ¨å¯¼
- å®éªŒä¸åŒå‚æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
- å°è¯•åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å®ç°è¿™äº›æŸå¤±å‡½æ•°
- ç ”ç©¶æŸå¤±å‡½æ•°åœ¨ä¸åŒåº”ç”¨é¢†åŸŸçš„è¡¨ç°ã€‚
