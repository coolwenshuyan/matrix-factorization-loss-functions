#!/usr/bin/env python3
"""
Hyperoptæ¨¡å—å¿«é€Ÿæ¼”ç¤º

è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ¼”ç¤ºï¼Œå±•ç¤ºhyperoptæ¨¡å—çš„æ ¸å¿ƒåŠŸèƒ½
"""

import numpy as np
import time
import json

def run_hyperopt_demo():
    """è¿è¡Œhyperoptæ¼”ç¤º"""
    
    print("ğŸš€ Hyperoptæ¨¡å—æ¼”ç¤ºå¼€å§‹")
    print("=" * 50)
    
    # è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
    np.random.seed(42)
    
    # 1. å‚æ•°ç©ºé—´å®šä¹‰
    print("\nğŸ“‹ 1. å‚æ•°ç©ºé—´å®šä¹‰")
    param_info = {
        'learning_rate': 'continuous [0.001, 0.1] log-scale',
        'latent_factors': 'discrete [20, 30, 40, 50, 60, 70, 80, 90, 100]',
        'lambda_reg': 'continuous [0.001, 0.1] log-scale', 
        'delta1': 'continuous [0.1, 1.0] linear-scale',
        'delta2': 'continuous [1.0, 3.0] linear-scale',
        'loss_type': 'categorical [hpl, l2]'
    }
    
    for param, info in param_info.items():
        print(f"   {param}: {info}")
    
    print("\nâš–ï¸  çº¦æŸæ¡ä»¶: delta1 < delta2 (HPLæŸå¤±å‡½æ•°è¦æ±‚)")
    
    # 2. é‡‡æ ·å‡½æ•°
    def sample_hyperparameters():
        """é‡‡æ ·ä¸€ç»„è¶…å‚æ•°"""
        config = {}
        
        # å­¦ä¹ ç‡ (å¯¹æ•°å°ºåº¦)
        log_lr = np.random.uniform(np.log(0.001), np.log(0.1))
        config['learning_rate'] = np.exp(log_lr)
        
        # æ½œåœ¨å› å­æ•° (ç¦»æ•£)
        factors_choices = [20, 30, 40, 50, 60, 70, 80, 90, 100]
        config['latent_factors'] = np.random.choice(factors_choices)
        
        # æ­£åˆ™åŒ–å‚æ•° (å¯¹æ•°å°ºåº¦)
        log_reg = np.random.uniform(np.log(0.001), np.log(0.1))
        config['lambda_reg'] = np.exp(log_reg)
        
        # HPLå‚æ•°
        config['delta1'] = np.random.uniform(0.1, 1.0)
        config['delta2'] = np.random.uniform(1.0, 3.0)
        
        # çº¦æŸæ£€æŸ¥å’Œä¿®æ­£
        if config['delta1'] >= config['delta2']:
            config['delta1'] = config['delta2'] * 0.8  # è‡ªåŠ¨ä¿®æ­£çº¦æŸ
        
        # æŸå¤±å‡½æ•°ç±»å‹
        config['loss_type'] = np.random.choice(['hpl', 'l2'])
        
        return config
    
    # 3. ç›®æ ‡å‡½æ•° (æ¨¡æ‹ŸçŸ©é˜µåˆ†è§£è®­ç»ƒ)
    def evaluate_config(config):
        """
        æ¨¡æ‹ŸçŸ©é˜µåˆ†è§£æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
        è¿”å›éªŒè¯é›†RMSE (è¶Šå°è¶Šå¥½)
        """
        
        # æå–å‚æ•°
        lr = config['learning_rate']
        nf = config['latent_factors'] 
        reg = config['lambda_reg']
        d1 = config['delta1']
        d2 = config['delta2']
        loss_type = config['loss_type']
        
        # æ¨¡æ‹ŸåŸºå‡†æ€§èƒ½
        base_rmse = 0.85
        
        # å„å‚æ•°å¯¹æ€§èƒ½çš„å½±å“ (åŸºäºç»éªŒçŸ¥è¯†)
        
        # å­¦ä¹ ç‡å½±å“ (æœ€ä¼˜åœ¨0.01é™„è¿‘)
        lr_penalty = 0.1 * abs(lr - 0.01)
        
        # å› å­æ•°å½±å“ (æœ€ä¼˜åœ¨50é™„è¿‘)  
        nf_penalty = 0.05 * abs(nf - 50) / 50
        
        # æ­£åˆ™åŒ–å½±å“ (æœ€ä¼˜åœ¨0.01é™„è¿‘)
        reg_penalty = 0.1 * abs(reg - 0.01)
        
        # HPLç‰¹å®šå‚æ•°å½±å“
        hpl_penalty = 0
        if loss_type == 'hpl':
            hpl_penalty += 0.03 * abs(d1 - 0.5)    # delta1æœ€ä¼˜åœ¨0.5
            hpl_penalty += 0.03 * abs(d2 - 2.0)    # delta2æœ€ä¼˜åœ¨2.0
            hpl_penalty -= 0.02  # HPLæ•´ä½“æ€§èƒ½ç¨å¥½
        
        # è®¡ç®—æœ€ç»ˆRMSE
        rmse = base_rmse + lr_penalty + nf_penalty + reg_penalty + hpl_penalty
        
        # æ·»åŠ éšæœºå™ªå£°æ¨¡æ‹Ÿå®éªŒä¸ç¡®å®šæ€§
        rmse += np.random.normal(0, 0.02)
        
        # ç¡®ä¿RMSEåœ¨åˆç†èŒƒå›´å†…
        return max(0.5, rmse)
    
    # 4. è¶…å‚æ•°ä¼˜åŒ–è¿‡ç¨‹
    print("\nğŸ” 2. å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–")
    print("-" * 40)
    
    best_config = None
    best_rmse = float('inf')
    optimization_history = []
    
    start_time = time.time()
    
    # è¿è¡Œ20æ¬¡è¯•éªŒ
    n_trials = 20
    
    for trial_id in range(1, n_trials + 1):
        # é‡‡æ ·æ–°é…ç½®
        config = sample_hyperparameters()
        
        # è¯„ä¼°é…ç½®
        rmse = evaluate_config(config)
        
        # è®°å½•è¯•éªŒ
        trial_info = {
            'trial_id': trial_id,
            'config': config,
            'rmse': rmse,
            'is_best': False
        }
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„æœ€ä½³é…ç½®
        if rmse < best_rmse:
            best_rmse = rmse
            best_config = config.copy()
            trial_info['is_best'] = True
            
            print(f"Trial {trial_id:2d}: ğŸ¯ æ–°æœ€ä½³! RMSE = {rmse:.4f} "
                  f"(lr={config['learning_rate']:.4f}, "
                  f"factors={config['latent_factors']}, "
                  f"loss={config['loss_type']})")
        else:
            print(f"Trial {trial_id:2d}: RMSE = {rmse:.4f}")
        
        optimization_history.append(trial_info)
    
    optimization_time = time.time() - start_time
    
    # 5. ç»“æœåˆ†æ
    print("\nğŸ“Š 3. ä¼˜åŒ–ç»“æœåˆ†æ")
    print("=" * 50)
    
    print(f"æ€»è¯•éªŒæ•°: {n_trials}")
    print(f"ä¼˜åŒ–è€—æ—¶: {optimization_time:.2f}ç§’")
    print(f"æœ€ä½³RMSE: {best_rmse:.4f}")
    
    # è®¡ç®—æ€§èƒ½æ”¹è¿›
    first_rmse = optimization_history[0]['rmse']
    improvement_pct = (first_rmse - best_rmse) / first_rmse * 100
    print(f"æ€§èƒ½æ”¹è¿›: {improvement_pct:.1f}% (ä» {first_rmse:.4f} æå‡åˆ° {best_rmse:.4f})")
    
    # æœ€ä½³é…ç½®è¯¦æƒ…
    print(f"\nğŸ† æœ€ä½³é…ç½®:")
    for param, value in best_config.items():
        if isinstance(value, float):
            print(f"   {param}: {value:.6f}")
        else:
            print(f"   {param}: {value}")
    
    # 6. æŸå¤±å‡½æ•°ç±»å‹åˆ†æ
    print(f"\nğŸ”¬ 4. æŸå¤±å‡½æ•°ç±»å‹åˆ†æ")
    
    hpl_trials = [t for t in optimization_history if t['config']['loss_type'] == 'hpl']
    l2_trials = [t for t in optimization_history if t['config']['loss_type'] == 'l2']
    
    if hpl_trials and l2_trials:
        hpl_avg = np.mean([t['rmse'] for t in hpl_trials])
        l2_avg = np.mean([t['rmse'] for t in l2_trials])
        
        print(f"HPLæŸå¤±å‡½æ•°: {len(hpl_trials)} æ¬¡è¯•éªŒ, å¹³å‡RMSE = {hpl_avg:.4f}")
        print(f"L2æŸå¤±å‡½æ•°:  {len(l2_trials)} æ¬¡è¯•éªŒ, å¹³å‡RMSE = {l2_avg:.4f}")
        
        if hpl_avg < l2_avg:
            print("âœ… HPLæŸå¤±å‡½æ•°è¡¨ç°æ›´å¥½!")
        else:
            print("âœ… L2æŸå¤±å‡½æ•°è¡¨ç°æ›´å¥½!")
    
    # 7. è¯¦ç»†è¯•éªŒè¡¨
    print(f"\nğŸ“‹ 5. è¯¦ç»†è¯•éªŒç»“æœ")
    print("-" * 85)
    print(f"{'Trial':<6} {'RMSE':<8} {'å­¦ä¹ ç‡':<10} {'å› å­':<6} {'æ­£åˆ™åŒ–':<10} {'æŸå¤±':<6} {'Î´1':<6} {'Î´2':<6}")
    print("-" * 85)
    
    for trial in optimization_history:
        config = trial['config']
        marker = "ğŸ¯" if trial['is_best'] else "  "
        
        print(f"{marker}{trial['trial_id']:<4} "
              f"{trial['rmse']:<8.4f} "
              f"{config['learning_rate']:<10.4f} "
              f"{config['latent_factors']:<6} "
              f"{config['lambda_reg']:<10.4f} "
              f"{config['loss_type']:<6} "
              f"{config['delta1']:<6.3f} "
              f"{config['delta2']:<6.3f}")
    
    # 8. ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ’¡ 6. ä¼˜åŒ–å»ºè®®")
    print("=" * 50)
    
    if best_config['loss_type'] == 'hpl':
        print("ğŸ¯ å»ºè®®ä½¿ç”¨HPLæŸå¤±å‡½æ•°")
        print(f"   æ¨èHPLå‚æ•°: Î´1={best_config['delta1']:.3f}, Î´2={best_config['delta2']:.3f}")
    else:
        print("ğŸ¯ å»ºè®®ä½¿ç”¨L2æŸå¤±å‡½æ•°")
    
    print(f"\nğŸ”§ å…³é”®è¶…å‚æ•°è®¾ç½®:")
    print(f"   å­¦ä¹ ç‡: {best_config['learning_rate']:.5f}")
    print(f"   æ½œåœ¨å› å­æ•°: {best_config['latent_factors']}")
    print(f"   æ­£åˆ™åŒ–å¼ºåº¦: {best_config['lambda_reg']:.5f}")
    
    print(f"\nğŸ“ˆ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("   1. åœ¨æœ€ä½³é…ç½®å‘¨å›´è¿›è¡Œç²¾ç»†æœç´¢")
    print("   2. ä½¿ç”¨æ›´å¤šè®­ç»ƒè½®æ•°éªŒè¯ç»“æœ")
    print("   3. åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¯„ä¼°æ€§èƒ½")
    print("   4. è€ƒè™‘ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–æå‡æ•ˆç‡")
    
    # 9. ä¿å­˜ç»“æœ
    results = {
        'best_config': best_config,
        'best_rmse': best_rmse,
        'improvement_percent': improvement_pct,
        'optimization_time': optimization_time,
        'all_trials': optimization_history
    }
    
    try:
        with open('hyperopt_demo_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: hyperopt_demo_results.json")
    except Exception as e:
        print(f"\nâŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    print(f"\nâœ… Hyperoptæ¼”ç¤ºå®Œæˆ!")
    
    return results

def demonstrate_sampling_strategies():
    """æ¼”ç¤ºä¸åŒé‡‡æ ·ç­–ç•¥çš„å·®å¼‚"""
    
    print(f"\n" + "=" * 60)
    print("ğŸ² é‡‡æ ·ç­–ç•¥å¯¹æ¯”æ¼”ç¤º")
    print("=" * 60)
    
    np.random.seed(42)
    
    # 1. éšæœºé‡‡æ ·
    print(f"\n1ï¸âƒ£ éšæœºé‡‡æ · (Random Sampling)")
    print("   å®Œå…¨éšæœºé€‰æ‹©å‚æ•°å€¼")
    print("   ä¼˜ç‚¹: ç®€å•æ˜“å®ç°  ç¼ºç‚¹: å¯èƒ½èšé›†åœ¨æŸäº›åŒºåŸŸ")
    
    random_samples = []
    for i in range(5):
        lr = np.exp(np.random.uniform(np.log(0.001), np.log(0.1)))
        factors = np.random.choice([20, 30, 40, 50, 60, 70, 80, 90, 100])
        random_samples.append((lr, factors))
        print(f"   æ ·æœ¬{i+1}: å­¦ä¹ ç‡={lr:.4f}, å› å­æ•°={factors}")
    
    # 2. æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ · (ç®€åŒ–ç‰ˆ)
    print(f"\n2ï¸âƒ£ æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ · (Latin Hypercube Sampling)")
    print("   ç¡®ä¿æ¯ä¸ªç»´åº¦çš„å‡åŒ€è¦†ç›–")
    print("   ä¼˜ç‚¹: æ›´å¥½çš„ç©ºé—´è¦†ç›–  ç¼ºç‚¹: å®ç°å¤æ‚")
    
    n_samples = 5
    
    # ä¸ºå­¦ä¹ ç‡åˆ›å»ºLHSæ ·æœ¬
    lr_intervals = np.arange(n_samples) / n_samples
    lr_perm = np.random.permutation(n_samples)
    
    # ä¸ºå› å­æ•°åˆ›å»ºLHSæ ·æœ¬  
    factor_intervals = np.arange(n_samples) / n_samples
    factor_perm = np.random.permutation(n_samples)
    
    lhs_samples = []
    for i in range(n_samples):
        # åœ¨æ¯ä¸ªåŒºé—´å†…éšæœºé‡‡æ ·
        lr_norm = (lr_perm[i] + np.random.random()) / n_samples
        factor_norm = (factor_perm[i] + np.random.random()) / n_samples
        
        # è½¬æ¢åˆ°å®é™…èŒƒå›´
        lr = np.exp(np.log(0.001) + lr_norm * (np.log(0.1) - np.log(0.001)))
        
        factor_choices = [20, 30, 40, 50, 60, 70, 80, 90, 100]
        factor_idx = int(factor_norm * len(factor_choices))
        factors = factor_choices[min(factor_idx, len(factor_choices)-1)]
        
        lhs_samples.append((lr, factors))
        print(f"   æ ·æœ¬{i+1}: å­¦ä¹ ç‡={lr:.4f}, å› å­æ•°={factors}")
    
    # 3. å¯¹æ¯”åˆ†æ
    print(f"\n3ï¸âƒ£ é‡‡æ ·æ•ˆæœå¯¹æ¯”")
    
    # å­¦ä¹ ç‡åˆ†å¸ƒåˆ†æ
    random_lrs = [s[0] for s in random_samples]
    lhs_lrs = [s[0] for s in lhs_samples]
    
    print(f"   å­¦ä¹ ç‡è¦†ç›–èŒƒå›´:")
    print(f"   éšæœºé‡‡æ ·: [{min(random_lrs):.4f}, {max(random_lrs):.4f}]")
    print(f"   LHSé‡‡æ ·:  [{min(lhs_lrs):.4f}, {max(lhs_lrs):.4f}]")
    
    # å› å­æ•°åˆ†å¸ƒåˆ†æ
    random_factors = [s[1] for s in random_samples]
    lhs_factors = [s[1] for s in lhs_samples]
    
    print(f"   å› å­æ•°è¦†ç›–èŒƒå›´:")
    print(f"   éšæœºé‡‡æ ·: [{min(random_factors)}, {max(random_factors)}]")
    print(f"   LHSé‡‡æ ·:  [{min(lhs_factors)}, {max(lhs_factors)}]")
    
    print(f"\nğŸ¯ æ€»ç»“:")
    print(f"   â€¢ éšæœºé‡‡æ ·é€‚åˆå¿«é€Ÿæ¢ç´¢å’Œç®€å•åœºæ™¯")
    print(f"   â€¢ LHSé‡‡æ ·é€‚åˆéœ€è¦å‡åŒ€è¦†ç›–çš„ç²¾ç»†ä¼˜åŒ–")
    print(f"   â€¢ å®é™…ä½¿ç”¨ä¸­å¯æ ¹æ®è¯•éªŒé¢„ç®—é€‰æ‹©ç­–ç•¥")

if __name__ == '__main__':
    print("ğŸ”¬ å¼€å§‹Hyperoptæ¨¡å—å®Œæ•´æ¼”ç¤º")
    
    # è¿è¡Œä¸»è¦æ¼”ç¤º
    results = run_hyperopt_demo()
    
    # æ¼”ç¤ºé‡‡æ ·ç­–ç•¥å¯¹æ¯”
    demonstrate_sampling_strategies()
    
    print(f"\n" + "=" * 60)
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)
    print("è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†hyperoptæ¨¡å—çš„æ ¸å¿ƒåŠŸèƒ½:")
    print("âœ“ å¤šç±»å‹å‚æ•°ç©ºé—´å®šä¹‰ (è¿ç»­ã€ç¦»æ•£ã€åˆ†ç±»)")
    print("âœ“ çº¦æŸæ¡ä»¶è‡ªåŠ¨æ£€æŸ¥å’Œä¿®æ­£")
    print("âœ“ æ™ºèƒ½è¶…å‚æ•°é‡‡æ ·")
    print("âœ“ æ¨¡æ‹Ÿç›®æ ‡å‡½æ•°ä¼˜åŒ–")
    print("âœ“ å®Œæ•´çš„å®éªŒè¿½è¸ªå’Œåˆ†æ")
    print("âœ“ å¤šç§é‡‡æ ·ç­–ç•¥å¯¹æ¯”")
    print(f"\nğŸ’¡ åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œç›®æ ‡å‡½æ•°å°†æ˜¯çœŸå®çš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹")
    print(f"   å¯ä»¥ç›´æ¥æ›¿æ¢ evaluate_config å‡½æ•°ä¸ºçœŸå®çš„è®­ç»ƒæµç¨‹")
