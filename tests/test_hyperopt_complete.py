#!/usr/bin/env python3
"""
è¶…å‚æ•°ä¼˜åŒ–ç®€å•æ¼”ç¤º
"""

import numpy as np
import json
import time

def main():
    print("=== Hyperoptæ¨¡å—æ¼”ç¤º ===")
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    
    # 1. å®šä¹‰å‚æ•°ç©ºé—´
    print("\n1. å‚æ•°ç©ºé—´å®šä¹‰:")
    param_space = {
        'learning_rate': {'type': 'continuous', 'bounds': (0.001, 0.1), 'scale': 'log'},
        'latent_factors': {'type': 'discrete', 'bounds': (20, 100), 'step': 10},
        'lambda_reg': {'type': 'continuous', 'bounds': (0.001, 0.1), 'scale': 'log'},
        'delta1': {'type': 'continuous', 'bounds': (0.1, 1.0), 'scale': 'linear'},
        'delta2': {'type': 'continuous', 'bounds': (1.0, 3.0), 'scale': 'linear'},
        'loss_type': {'type': 'categorical', 'choices': ['hpl', 'l2']}
    }
    
    for param, spec in param_space.items():
        print(f"  {param}: {spec}")
    
    # 2. å®šä¹‰çº¦æŸ
    print("\n2. çº¦æŸæ¡ä»¶:")
    constraints = ['delta1 < delta2']
    print(f"  {constraints}")
    
    # 3. é‡‡æ ·å‡½æ•°
    def sample_config():
        config = {}
        
        # learning_rate (log scale)
        log_lr = np.random.uniform(np.log(0.001), np.log(0.1))
        config['learning_rate'] = np.exp(log_lr)
        
        # latent_factors (discrete)
        factors = list(range(20, 101, 10))
        config['latent_factors'] = np.random.choice(factors)
        
        # lambda_reg (log scale)
        log_reg = np.random.uniform(np.log(0.001), np.log(0.1))
        config['lambda_reg'] = np.exp(log_reg)
        
        # delta1, delta2 (with constraint)
        config['delta1'] = np.random.uniform(0.1, 1.0)
        config['delta2'] = np.random.uniform(1.0, 3.0)
        
        # ç¡®ä¿çº¦æŸ
        if config['delta1'] >= config['delta2']:
            config['delta1'] = config['delta2'] * 0.8
        
        # loss_type
        config['loss_type'] = np.random.choice(['hpl', 'l2'])
        
        return config
    
    # 4. ç›®æ ‡å‡½æ•°ï¼ˆæ¨¡æ‹Ÿï¼‰
    def objective_function(config):
        """æ¨¡æ‹ŸçŸ©é˜µåˆ†è§£è®­ç»ƒå’Œè¯„ä¼°"""
        
        # æ¨¡æ‹Ÿä¸åŒå‚æ•°å¯¹æ€§èƒ½çš„å½±å“
        lr = config['learning_rate']
        nf = config['latent_factors']
        reg = config['lambda_reg']
        d1 = config['delta1']
        d2 = config['delta2']
        loss_type = config['loss_type']
        
        # åŸºå‡†RMSE
        rmse = 0.85
        
        # å­¦ä¹ ç‡å½±å“ï¼ˆæœ€ä¼˜çº¦0.01ï¼‰
        rmse += 0.1 * abs(lr - 0.01)
        
        # å› å­æ•°å½±å“ï¼ˆæœ€ä¼˜çº¦50ï¼‰
        rmse += 0.05 * abs(nf - 50) / 50
        
        # æ­£åˆ™åŒ–å½±å“ï¼ˆæœ€ä¼˜çº¦0.01ï¼‰
        rmse += 0.1 * abs(reg - 0.01)
        
        # HPLå‚æ•°å½±å“
        if loss_type == 'hpl':
            rmse += 0.03 * abs(d1 - 0.5)
            rmse += 0.03 * abs(d2 - 2.0)
            rmse -= 0.02  # HPLç¨å¾®å¥½ä¸€ç‚¹
        
        # æ·»åŠ å™ªå£°
        rmse += np.random.normal(0, 0.02)
        
        return max(0.5, rmse)
    
    # 5. è¿è¡Œä¼˜åŒ–
    print("\n3. å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–:")
    print("-" * 50)
    
    best_config = None
    best_score = float('inf')
    all_trials = []
    
    start_time = time.time()
    
    for trial in range(20):
        # é‡‡æ ·é…ç½®
        config = sample_config()
        
        # è¯„ä¼°é…ç½®
        score = objective_function(config)
        
        # è®°å½•è¯•éªŒ
        trial_info = {
            'trial_id': trial + 1,
            'config': config,
            'score': score,
            'time': time.time() - start_time
        }
        all_trials.append(trial_info)
        
        # æ›´æ–°æœ€ä½³é…ç½®
        if score < best_score:
            best_score = score
            best_config = config
            print(f"Trial {trial+1:2d}: ğŸ¯ æ–°æœ€ä½³! RMSE = {score:.4f} "
                  f"(lr={config['learning_rate']:.4f}, factors={config['latent_factors']}, "
                  f"loss={config['loss_type']})")
        else:
            print(f"Trial {trial+1:2d}: RMSE = {score:.4f}")
    
    total_time = time.time() - start_time
    
    # 6. æ˜¾ç¤ºç»“æœ
    print("\n" + "="*70)
    print("ä¼˜åŒ–ç»“æœ")
    print("="*70)
    
    print(f"æ€»è¯•éªŒæ•°: {len(all_trials)}")
    print(f"ä¼˜åŒ–è€—æ—¶: {total_time:.2f}ç§’")
    print(f"æœ€ä½³RMSE: {best_score:.4f}")
    
    print("\næœ€ä½³é…ç½®:")
    for key, value in best_config.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.6f}")
        else:
            print(f"  {key}: {value}")
    
    # 7. åˆ†æç»“æœ
    print("\n" + "="*70)
    print("ç»“æœåˆ†æ")
    print("="*70)
    
    # è®¡ç®—æ”¹è¿›å¹…åº¦
    first_score = all_trials[0]['score']
    improvement = (first_score - best_score) / first_score * 100
    print(f"æ€§èƒ½æ”¹è¿›: {improvement:.1f}% (ä» {first_score:.4f} åˆ° {best_score:.4f})")
    
    # åˆ†ææœ€ä½³è¯•éªŒ
    best_trial_idx = np.argmin([t['score'] for t in all_trials])
    print(f"æœ€ä½³è¯•éªŒ: Trial {best_trial_idx + 1}")
    
    # ç»Ÿè®¡æŸå¤±å‡½æ•°ç±»å‹
    hpl_trials = [t for t in all_trials if t['config']['loss_type'] == 'hpl']
    l2_trials = [t for t in all_trials if t['config']['loss_type'] == 'l2']
    
    if hpl_trials and l2_trials:
        hpl_avg = np.mean([t['score'] for t in hpl_trials])
        l2_avg = np.mean([t['score'] for t in l2_trials])
        print(f"æŸå¤±å‡½æ•°å¯¹æ¯”: HPLå¹³å‡ {hpl_avg:.4f}, L2å¹³å‡ {l2_avg:.4f}")
    
    # 8. æ˜¾ç¤ºè¯¦ç»†ç»“æœè¡¨
    print("\nè¯¦ç»†è¯•éªŒç»“æœ:")
    print("-" * 90)
    print(f"{'Trial':<6} {'RMSE':<8} {'å­¦ä¹ ç‡':<10} {'å› å­æ•°':<8} {'æ­£åˆ™åŒ–':<10} {'æŸå¤±':<6} {'Î´1':<6} {'Î´2':<6}")
    print("-" * 90)
    
    for trial in all_trials:
        config = trial['config']
        print(f"{trial['trial_id']:<6} "
              f"{trial['score']:<8.4f} "
              f"{config['learning_rate']:<10.4f} "
              f"{config['latent_factors']:<8} "
              f"{config['lambda_reg']:<10.4f} "
              f"{config['loss_type']:<6} "
              f"{config['delta1']:<6.3f} "
              f"{config['delta2']:<6.3f}")
    
    # 9. ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    # è½¬æ¢æ‰€æœ‰NumPyç±»å‹
    def convert_numpy_types(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(i) for i in obj]
        else:
            return obj
    
    results = {
        'best_config': convert_numpy_types(best_config),
        'best_score': float(best_score),
        'total_trials': len(all_trials),
        'optimization_time': total_time,
        'improvement_percent': improvement,
        'all_trials': convert_numpy_types(all_trials)
    }
    
    try:
        with open('hyperopt_demo_results.json', 'w') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: hyperopt_demo_results.json")
    except Exception as e:
        print(f"\nä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")
    
    # 10. ç»™å‡ºå»ºè®®
    print("\n" + "="*70)
    print("ä¼˜åŒ–å»ºè®®")
    print("="*70)
    
    print("åŸºäºå½“å‰ç»“æœï¼Œå»ºè®®çš„æœ€ä½³é…ç½®ä¸º:")
    # å°†NumPyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
    best_config_json = {}
    for key, value in best_config.items():
        if isinstance(value, np.integer):
            best_config_json[key] = int(value)
        elif isinstance(value, np.floating):
            best_config_json[key] = float(value)
        elif isinstance(value, np.ndarray):
            best_config_json[key] = value.tolist()
        else:
            best_config_json[key] = value
    
    print(json.dumps(best_config_json, indent=2))
    
    if best_config['loss_type'] == 'hpl':
        print("\nâœ… HPLæŸå¤±å‡½æ•°è¡¨ç°æ›´å¥½ï¼Œå»ºè®®ä½¿ç”¨")
        print(f"   æ¨èå‚æ•°: Î´1={best_config['delta1']:.3f}, Î´2={best_config['delta2']:.3f}")
    else:
        print("\nâœ… L2æŸå¤±å‡½æ•°è¡¨ç°æ›´å¥½ï¼Œå»ºè®®ä½¿ç”¨ç®€å•æŸå¤±å‡½æ•°")
    
    print(f"\nğŸ”§ å…¶ä»–å…³é”®å‚æ•°:")
    print(f"   å­¦ä¹ ç‡: {best_config['learning_rate']:.4f}")
    print(f"   æ½œåœ¨å› å­æ•°: {best_config['latent_factors']}")
    print(f"   æ­£åˆ™åŒ–å¼ºåº¦: {best_config['lambda_reg']:.4f}")
    
    print("\nğŸ“ˆ ä¸‹ä¸€æ­¥ä¼˜åŒ–å»ºè®®:")
    print("1. åœ¨æœ€ä½³é…ç½®å‘¨å›´è¿›è¡Œæ›´ç²¾ç»†çš„æœç´¢")
    print("2. å¢åŠ è®­ç»ƒè½®æ•°è¿›è¡Œæ›´å‡†ç¡®çš„è¯„ä¼°")
    print("3. ä½¿ç”¨äº¤å‰éªŒè¯æé«˜ç»“æœå¯é æ€§")
    print("4. è€ƒè™‘æ·»åŠ æ›´å¤šè¶…å‚æ•°ï¼ˆå¦‚åŠ¨é‡ã€å­¦ä¹ ç‡è¡°å‡ç­‰ï¼‰")
    
    return results

def demonstrate_sampling_strategies():
    """æ¼”ç¤ºä¸åŒé‡‡æ ·ç­–ç•¥"""
    print("\n" + "="*70)
    print("é‡‡æ ·ç­–ç•¥å¯¹æ¯”æ¼”ç¤º")
    print("="*70)
    
    np.random.seed(42)
    
    # 1. éšæœºé‡‡æ ·
    print("\n1. éšæœºé‡‡æ · (Random Sampling):")
    random_samples = []
    for i in range(5):
        lr = np.exp(np.random.uniform(np.log(0.001), np.log(0.1)))
        factors = np.random.choice(list(range(20, 101, 10)))
        random_samples.append({'lr': lr, 'factors': factors})
        print(f"  æ ·æœ¬{i+1}: lr={lr:.4f}, factors={factors}")
    
    # 2. æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ ·æ¨¡æ‹Ÿ
    print("\n2. æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ · (Latin Hypercube Sampling):")
    n_samples = 5
    # ç®€åŒ–çš„LHSå®ç°
    lr_intervals = np.linspace(0, 1, n_samples)
    factor_intervals = np.linspace(0, 1, n_samples)
    
    # éšæœºæ’åˆ—
    lr_perm = np.random.permutation(n_samples)
    factor_perm = np.random.permutation(n_samples)
    
    lhs_samples = []
    for i in range(n_samples):
        # åœ¨æ¯ä¸ªåŒºé—´å†…éšæœºé‡‡æ ·
        lr_normalized = (lr_perm[i] + np.random.random()) / n_samples
        factor_normalized = (factor_perm[i] + np.random.random()) / n_samples
        
        # è½¬æ¢åˆ°å®é™…èŒƒå›´
        lr = np.exp(np.log(0.001) + lr_normalized * (np.log(0.1) - np.log(0.001)))
        factors = int(20 + factor_normalized * (100 - 20))
        factors = ((factors - 20) // 10) * 10 + 20  # å¯¹é½åˆ°10çš„å€æ•°
        
        lhs_samples.append({'lr': lr, 'factors': factors})
        print(f"  æ ·æœ¬{i+1}: lr={lr:.4f}, factors={factors}")
    
    # 3. å¯¹æ¯”åˆ†æ
    print("\n3. é‡‡æ ·ç­–ç•¥å¯¹æ¯”:")
    print("éšæœºé‡‡æ ·çš„å­¦ä¹ ç‡åˆ†å¸ƒ:", [s['lr'] for s in random_samples])
    print("LHSé‡‡æ ·çš„å­¦ä¹ ç‡åˆ†å¸ƒ:", [s['lr'] for s in lhs_samples])
    
    # è®¡ç®—è¦†ç›–åº¦
    print(f"\nè¦†ç›–åº¦åˆ†æ:")
    print(f"éšæœºé‡‡æ ·å› å­æ•°èŒƒå›´: {min(s['factors'] for s in random_samples)} - {max(s['factors'] for s in random_samples)}")
    print(f"LHSé‡‡æ ·å› å­æ•°èŒƒå›´: {min(s['factors'] for s in lhs_samples)} - {max(s['factors'] for s in lhs_samples)}")

if __name__ == '__main__':
    print("å¼€å§‹Hyperoptæ¨¡å—æ¼”ç¤º...")
    
    # è¿è¡Œä¸»è¦æ¼”ç¤º
    results = main()
    
    # æ¼”ç¤ºä¸åŒé‡‡æ ·ç­–ç•¥
    demonstrate_sampling_strategies()
    
    print("\n" + "="*70)
    print("æ¼”ç¤ºå®Œæˆï¼")
    print("="*70)
    print("è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†hyperoptæ¨¡å—çš„æ ¸å¿ƒåŠŸèƒ½:")
    print("âœ“ å‚æ•°ç©ºé—´å®šä¹‰ (è¿ç»­ã€ç¦»æ•£ã€åˆ†ç±»å‚æ•°)")
    print("âœ“ çº¦æŸæ¡ä»¶ç®¡ç† (Î´1 < Î´2)")
    print("âœ“ éšæœºé‡‡æ ·ç­–ç•¥")
    print("âœ“ ç›®æ ‡å‡½æ•°ä¼˜åŒ– (æœ€å°åŒ–RMSE)")
    print("âœ“ å®éªŒè¿½è¸ªå’Œç»“æœåˆ†æ")
    print("âœ“ å¤šç§é‡‡æ ·ç­–ç•¥å¯¹æ¯”")
    print("\nåœ¨å®é™…åº”ç”¨ä¸­ï¼Œç›®æ ‡å‡½æ•°å°†æ˜¯çœŸå®çš„æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ã€‚")


