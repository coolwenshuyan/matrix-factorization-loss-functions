#!/usr/bin/env python3
"""
å™ªå£°æ³¨å…¥æµ‹è¯•æ–¹æ¡ˆ
æ ¸å¿ƒæ€è·¯ï¼šåœ¨æ•°æ®åŠ è½½åã€æ¨¡å‹è¯„ä¼°å‰ï¼Œå¯¹æµ‹è¯•é›†ä¸­çš„è¯„åˆ†æ•°æ®æ³¨å…¥å™ªå£°
ä½¿ç”¨åœ¨åŸå§‹å¹²å‡€è®­ç»ƒé›†ä¸Šè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œåœ¨å¸¦å™ªå£°çš„æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°
é€šè¿‡ä¿®æ”¹å®éªŒé…ç½®æ¥æ§åˆ¶å™ªå£°çš„ç±»å‹å’Œå¼ºåº¦
"""

import sys
import os
import numpy as np
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
import seaborn as sns
# é…ç½®matplotlibä»¥é¿å…ä¸­æ–‡å­—ä½“è­¦å‘Šå’Œç¾åŒ–æ ·å¼
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.facecolor'] = 'white'
plt.rcParams['axes.facecolor'] = 'white'
plt.rcParams['axes.edgecolor'] = '#CCCCCC'
plt.rcParams['axes.linewidth'] = 0.8
plt.rcParams['grid.color'] = '#E5E5E5'
plt.rcParams['grid.linewidth'] = 0.5
# è®¾ç½®seabornæ ·å¼
sns.set_style("whitegrid")
sns.set_palette("husl")
import logging
import json
from pathlib import Path
from typing import Dict, Any, Tuple, List, Optional, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod
import copy

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from data.data_manager import DataManager
from src.models.mf_sgd import MatrixFactorizationSGD
from src.models.initializers import NormalInitializer, XavierInitializer, UniformInitializer
from src.models.regularizers import L2Regularizer, L1Regularizer
from src.losses.standard import L1Loss, L2Loss
from src.losses.hpl import HybridPiecewiseLoss
from src.losses.robust import HuberLoss

# å¯¼å…¥çŸ­ä¿¡é€šçŸ¥æ¨¡å—
from utils.sms_notification import send_sms_notification

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class NoiseConfig:
    """å™ªå£°é…ç½®ç±»"""
    noise_type: str  # 'gaussian', 'uniform', 'salt_pepper', 'outlier', 'systematic'
    noise_strength: float  # å™ªå£°å¼ºåº¦
    noise_ratio: float = 1.0  # å—å™ªå£°å½±å“çš„æ ·æœ¬æ¯”ä¾‹ (0-1)
    random_seed: int = 42
    # ç‰¹å®šå™ªå£°ç±»å‹çš„å‚æ•°
    outlier_scale: float = 3.0  # å¼‚å¸¸å€¼çš„ç¼©æ”¾å€æ•°
    systematic_bias: float = 0.5  # ç³»ç»Ÿæ€§åå·®


class NoiseInjector(ABC):
    """å™ªå£°æ³¨å…¥å™¨åŸºç±»"""

    def __init__(self, config: NoiseConfig):
        self.config = config
        self.rng = np.random.RandomState(config.random_seed)

    @abstractmethod
    def inject_noise(self, ratings: np.ndarray, rating_scale: Tuple[float, float]) -> np.ndarray:
        """æ³¨å…¥å™ªå£°åˆ°è¯„åˆ†æ•°æ®"""
        pass

    def _select_affected_samples(self, n_samples: int) -> np.ndarray:
        """é€‰æ‹©å—å™ªå£°å½±å“çš„æ ·æœ¬ç´¢å¼•"""
        n_affected = int(n_samples * self.config.noise_ratio)
        return self.rng.choice(n_samples, n_affected, replace=False)


class GaussianNoiseInjector(NoiseInjector):
    """é«˜æ–¯å™ªå£°æ³¨å…¥å™¨"""

    def inject_noise(self, ratings: np.ndarray, rating_scale: Tuple[float, float]) -> np.ndarray:
        noisy_ratings = ratings.copy()
        affected_indices = self._select_affected_samples(len(ratings))

        # ç”Ÿæˆé«˜æ–¯å™ªå£°
        noise = self.rng.normal(0, self.config.noise_strength, len(affected_indices))
        noisy_ratings[affected_indices] += noise

        # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
        min_rating, max_rating = rating_scale
        noisy_ratings = np.clip(noisy_ratings, min_rating, max_rating)

        return noisy_ratings


class UniformNoiseInjector(NoiseInjector):
    """å‡åŒ€å™ªå£°æ³¨å…¥å™¨"""

    def inject_noise(self, ratings: np.ndarray, rating_scale: Tuple[float, float]) -> np.ndarray:
        noisy_ratings = ratings.copy()
        affected_indices = self._select_affected_samples(len(ratings))

        # ç”Ÿæˆå‡åŒ€å™ªå£° [-strength, +strength]
        noise = self.rng.uniform(-self.config.noise_strength,
                               self.config.noise_strength,
                               len(affected_indices))
        noisy_ratings[affected_indices] += noise

        # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
        min_rating, max_rating = rating_scale
        noisy_ratings = np.clip(noisy_ratings, min_rating, max_rating)

        return noisy_ratings


class SaltPepperNoiseInjector(NoiseInjector):
    """æ¤’ç›å™ªå£°æ³¨å…¥å™¨"""

    def inject_noise(self, ratings: np.ndarray, rating_scale: Tuple[float, float]) -> np.ndarray:
        noisy_ratings = ratings.copy()
        affected_indices = self._select_affected_samples(len(ratings))

        min_rating, max_rating = rating_scale

        # éšæœºé€‰æ‹©ä¸€åŠè®¾ä¸ºæœ€å°å€¼ï¼Œä¸€åŠè®¾ä¸ºæœ€å¤§å€¼
        n_salt = len(affected_indices) // 2
        salt_indices = affected_indices[:n_salt]
        pepper_indices = affected_indices[n_salt:]

        noisy_ratings[salt_indices] = max_rating  # "ç›"
        noisy_ratings[pepper_indices] = min_rating  # "èƒ¡æ¤’"

        return noisy_ratings


class OutlierNoiseInjector(NoiseInjector):
    """å¼‚å¸¸å€¼å™ªå£°æ³¨å…¥å™¨"""

    def inject_noise(self, ratings: np.ndarray, rating_scale: Tuple[float, float]) -> np.ndarray:
        noisy_ratings = ratings.copy()
        affected_indices = self._select_affected_samples(len(ratings))

        # è®¡ç®—è¯„åˆ†çš„æ ‡å‡†å·®
        rating_std = np.std(ratings)

        # ç”Ÿæˆå¼‚å¸¸å€¼ï¼šåŸå€¼ Â± outlier_scale * std
        outlier_noise = self.rng.choice([-1, 1], len(affected_indices)) * \
                       self.config.outlier_scale * rating_std

        noisy_ratings[affected_indices] += outlier_noise

        # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
        min_rating, max_rating = rating_scale
        noisy_ratings = np.clip(noisy_ratings, min_rating, max_rating)

        return noisy_ratings


class SystematicNoiseInjector(NoiseInjector):
    """ç³»ç»Ÿæ€§åå·®å™ªå£°æ³¨å…¥å™¨"""

    def inject_noise(self, ratings: np.ndarray, rating_scale: Tuple[float, float]) -> np.ndarray:
        noisy_ratings = ratings.copy()
        affected_indices = self._select_affected_samples(len(ratings))

        # æ·»åŠ ç³»ç»Ÿæ€§åå·®ï¼ˆæ‰€æœ‰å—å½±å“çš„æ ·æœ¬éƒ½åŠ ä¸Šç›¸åŒçš„åå·®ï¼‰
        noisy_ratings[affected_indices] += self.config.systematic_bias

        # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
        min_rating, max_rating = rating_scale
        noisy_ratings = np.clip(noisy_ratings, min_rating, max_rating)

        return noisy_ratings


class NoiseInjectionManager:
    """å™ªå£°æ³¨å…¥ç®¡ç†å™¨"""

    def __init__(self):
        self.injectors = {
            'gaussian': GaussianNoiseInjector,
            'uniform': UniformNoiseInjector,
            'salt_pepper': SaltPepperNoiseInjector,
            'outlier': OutlierNoiseInjector,
            'systematic': SystematicNoiseInjector
        }

    def create_injector(self, config: NoiseConfig) -> NoiseInjector:
        """åˆ›å»ºå™ªå£°æ³¨å…¥å™¨"""
        if config.noise_type not in self.injectors:
            raise ValueError(f"ä¸æ”¯æŒçš„å™ªå£°ç±»å‹: {config.noise_type}")

        return self.injectors[config.noise_type](config)

    def inject_noise_to_test_data(self, test_data: np.ndarray,
                                 noise_config: NoiseConfig,
                                 rating_scale: Tuple[float, float] = (1, 5)) -> np.ndarray:
        """
        å¯¹æµ‹è¯•æ•°æ®æ³¨å…¥å™ªå£°

        Args:
            test_data: æµ‹è¯•æ•°æ® [user_id, item_id, rating]
            noise_config: å™ªå£°é…ç½®
            rating_scale: è¯„åˆ†èŒƒå›´

        Returns:
            å¸¦å™ªå£°çš„æµ‹è¯•æ•°æ®
        """
        # åˆ›å»ºå™ªå£°æ³¨å…¥å™¨
        injector = self.create_injector(noise_config)

        # å¤åˆ¶æµ‹è¯•æ•°æ®
        noisy_test_data = test_data.copy()

        # æ³¨å…¥å™ªå£°åˆ°è¯„åˆ†åˆ—
        original_ratings = test_data[:, 2]
        noisy_ratings = injector.inject_noise(original_ratings, rating_scale)
        noisy_test_data[:, 2] = noisy_ratings

        # è®°å½•å™ªå£°ç»Ÿè®¡ä¿¡æ¯
        rating_diff = noisy_ratings - original_ratings
        logger.info(f"å™ªå£°æ³¨å…¥å®Œæˆ:")
        logger.info(f"  ç±»å‹: {noise_config.noise_type}")
        logger.info(f"  å¼ºåº¦: {noise_config.noise_strength}")
        logger.info(f"  å½±å“æ¯”ä¾‹: {noise_config.noise_ratio}")
        logger.info(f"  å¹³å‡å˜åŒ–: {np.mean(rating_diff):.4f}")
        logger.info(f"  å˜åŒ–æ ‡å‡†å·®: {np.std(rating_diff):.4f}")
        logger.info(f"  æœ€å¤§å˜åŒ–: {np.max(np.abs(rating_diff)):.4f}")

        return noisy_test_data


class RobustnessEvaluator:
    """é²æ£’æ€§è¯„ä¼°å™¨"""

    def __init__(self, data_manager: DataManager):
        self.data_manager = data_manager
        self.noise_manager = NoiseInjectionManager()
        self.results = {}

    def evaluate_model_robustness(self, model, noise_configs: List[NoiseConfig],
                                dataset_name: str = "test_dataset") -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒå™ªå£°æ¡ä»¶ä¸‹çš„é²æ£’æ€§

        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹
            noise_configs: å™ªå£°é…ç½®åˆ—è¡¨
            dataset_name: æ•°æ®é›†åç§°

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        # è·å–åŸå§‹æµ‹è¯•æ•°æ®
        _, _, original_test_data = self.data_manager.get_splits()

        # è·å–è¯„åˆ†èŒƒå›´
        stats = self.data_manager.get_statistics()
        rating_scale = (stats['rating_min'], stats['rating_max'])

        # å­˜å‚¨ç»“æœ
        results = {
            'dataset_name': dataset_name,
            'original_performance': {},
            'noisy_performance': {},
            'robustness_metrics': {}
        }

        # è¯„ä¼°åŸå§‹æµ‹è¯•é›†æ€§èƒ½
        logger.info("è¯„ä¼°åŸå§‹æµ‹è¯•é›†æ€§èƒ½...")
        original_performance = self._evaluate_model_on_data(model, original_test_data)
        results['original_performance'] = original_performance

        # è¯„ä¼°æ¯ç§å™ªå£°æ¡ä»¶ä¸‹çš„æ€§èƒ½
        for i, noise_config in enumerate(noise_configs):
            logger.info(f"è¯„ä¼°å™ªå£°æ¡ä»¶ {i+1}/{len(noise_configs)}: {noise_config.noise_type}")

            # æ³¨å…¥å™ªå£°
            noisy_test_data = self.noise_manager.inject_noise_to_test_data(
                original_test_data, noise_config, rating_scale
            )

            # è¯„ä¼°æ¨¡å‹æ€§èƒ½
            noisy_performance = self._evaluate_model_on_data(model, noisy_test_data)

            # å­˜å‚¨ç»“æœ
            config_key = f"{noise_config.noise_type}_strength_{noise_config.noise_strength}_ratio_{noise_config.noise_ratio}"
            results['noisy_performance'][config_key] = {
                'noise_config': noise_config.__dict__,
                'performance': noisy_performance
            }

            # è®¡ç®—é²æ£’æ€§æŒ‡æ ‡
            robustness_metrics = self._calculate_robustness_metrics(
                original_performance, noisy_performance
            )
            results['robustness_metrics'][config_key] = robustness_metrics

        self.results[dataset_name] = results
        return results

    def _evaluate_model_on_data(self, model, test_data: np.ndarray) -> Dict[str, float]:
        """åœ¨ç»™å®šæ•°æ®ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        # é¢„æµ‹
        user_ids = test_data[:, 0].astype(int)
        item_ids = test_data[:, 1].astype(int)
        true_ratings = test_data[:, 2]

        predicted_ratings = model.predict(user_ids, item_ids)

        # å¦‚æœæ•°æ®è¢«ä¸­å¿ƒåŒ–ï¼Œéœ€è¦è¿˜åŸåˆ°åŸå§‹å°ºåº¦
        if self.data_manager.global_mean is not None:
            predicted_ratings += self.data_manager.global_mean
            true_ratings = true_ratings + self.data_manager.global_mean

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        mae = np.mean(np.abs(predicted_ratings - true_ratings))
        mse = np.mean((predicted_ratings - true_ratings) ** 2)
        rmse = np.sqrt(mse)

        # è®¡ç®—MAPEï¼ˆé¿å…é™¤é›¶é”™è¯¯ï¼‰
        non_zero_mask = true_ratings != 0
        if np.any(non_zero_mask):
            mape = np.mean(np.abs((predicted_ratings[non_zero_mask] - true_ratings[non_zero_mask])
                                / true_ratings[non_zero_mask])) * 100
        else:
            mape = float('inf')

        return {
            'MAE': mae,
            'MSE': mse,
            'RMSE': rmse,
            'MAPE': mape
        }

    def _calculate_robustness_metrics(self, original_perf: Dict[str, float],
                                    noisy_perf: Dict[str, float]) -> Dict[str, float]:
        """è®¡ç®—é²æ£’æ€§æŒ‡æ ‡"""
        robustness_metrics = {}

        for metric in ['MAE', 'MSE', 'RMSE', 'MAPE']:
            original_value = original_perf[metric]
            noisy_value = noisy_perf[metric]

            # æ€§èƒ½é™è§£ï¼ˆç›¸å¯¹å˜åŒ–ï¼‰
            if original_value != 0:
                relative_change = (noisy_value - original_value) / original_value
            else:
                relative_change = float('inf') if noisy_value != 0 else 0

            # ç»å¯¹å˜åŒ–
            absolute_change = noisy_value - original_value

            robustness_metrics[f'{metric}_relative_change'] = relative_change
            robustness_metrics[f'{metric}_absolute_change'] = absolute_change
            robustness_metrics[f'{metric}_noisy'] = noisy_value

        return robustness_metrics

    def save_results(self, save_path: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)

        with open(save_path, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)

        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {save_path}")

    def generate_robustness_report(self) -> str:
        """ç”Ÿæˆé²æ£’æ€§è¯„ä¼°æŠ¥å‘Š"""
        if not self.results:
            return "æ— è¯„ä¼°ç»“æœ"

        report = []
        report.append("=" * 80)
        report.append("æ¨¡å‹é²æ£’æ€§è¯„ä¼°æŠ¥å‘Š")
        report.append("=" * 80)

        for dataset_name, dataset_results in self.results.items():
            report.append(f"\næ•°æ®é›†: {dataset_name}")
            report.append("-" * 60)

            # åŸå§‹æ€§èƒ½
            original_perf = dataset_results['original_performance']
            report.append(f"åŸå§‹æµ‹è¯•é›†æ€§èƒ½:")
            for metric, value in original_perf.items():
                report.append(f"  {metric}: {value:.4f}")

            # å™ªå£°æ¡ä»¶ä¸‹çš„æ€§èƒ½
            report.append(f"\nå™ªå£°æ¡ä»¶ä¸‹çš„æ€§èƒ½å˜åŒ–:")
            report.append(f"{'å™ªå£°ç±»å‹':<12} {'å¼ºåº¦':<8} {'æ¯”ä¾‹':<8} {'RMSEå˜åŒ–':<12} {'MAEå˜åŒ–':<12}")
            report.append("-" * 60)

            for config_key, result in dataset_results['robustness_metrics'].items():
                parts = config_key.split('_')
                noise_type = parts[0]
                strength = parts[2]
                ratio = parts[4]

                rmse_change = result.get('RMSE_relative_change', 0) * 100
                mae_change = result.get('MAE_relative_change', 0) * 100

                report.append(f"{noise_type:<12} {strength:<8} {ratio:<8} "
                            f"{rmse_change:+7.2f}%    {mae_change:+7.2f}%")

        report.append("=" * 80)
        return "\n".join(report)


class ExperimentRunner:
    """å®éªŒè¿è¡Œå™¨"""

    def __init__(self, base_config: Dict[str, Any]):
        self.base_config = base_config
        self.results = {}
        self.model_configs = []  # å­˜å‚¨å½“å‰å®éªŒçš„æ¨¡å‹é…ç½®

    def run_robustness_experiment(self,
                                 dataset_name: str,
                                 data_path: str,
                                 noise_experiment_configs: List[Dict[str, Any]],
                                 model_configs: List[Dict[str, Any]],
                                 save_dir: str = "robustness_experiments"):
        """
        è¿è¡Œå®Œæ•´çš„é²æ£’æ€§å®éªŒ

        Args:
            dataset_name: æ•°æ®é›†åç§°
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            noise_experiment_configs: å™ªå£°å®éªŒé…ç½®åˆ—è¡¨
            model_configs: æ¨¡å‹é…ç½®åˆ—è¡¨
            save_dir: ä¿å­˜ç›®å½•
        """
        logger.info(f"å¼€å§‹é²æ£’æ€§å®éªŒ: {dataset_name}")

        # å­˜å‚¨æ¨¡å‹é…ç½®ä»¥ä¾›å¯è§†åŒ–ä½¿ç”¨
        self.model_configs = model_configs

        # åˆ›å»ºä¿å­˜ç›®å½•
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)

        # å‡†å¤‡æ•°æ®
        data_manager = self._prepare_data(dataset_name, data_path)

        # åˆ›å»ºé²æ£’æ€§è¯„ä¼°å™¨ï¼ˆåœ¨æ¨¡å‹å¾ªç¯å¤–åˆ›å»ºï¼Œä»¥ä¾¿æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„ç»“æœï¼‰
        evaluator = RobustnessEvaluator(data_manager)

        # å¯¹æ¯ä¸ªæ¨¡å‹é…ç½®è¿è¡Œå®éªŒ
        for model_idx, model_config in enumerate(model_configs):
            logger.info(f"è®­ç»ƒæ¨¡å‹é…ç½® {model_idx + 1}/{len(model_configs)}")

            # è®­ç»ƒæ¨¡å‹
            model = self._train_model(data_manager, model_config)

            # å¯¹æ¯ä¸ªå™ªå£°å®éªŒé…ç½®è¿è¡Œè¯„ä¼°
            for exp_idx, exp_config in enumerate(noise_experiment_configs):
                logger.info(f"å™ªå£°å®éªŒ {exp_idx + 1}/{len(noise_experiment_configs)}")

                # åˆ›å»ºå™ªå£°é…ç½®åˆ—è¡¨
                noise_configs = self._create_noise_configs(exp_config)

                # è¯„ä¼°é²æ£’æ€§
                _ = evaluator.evaluate_model_robustness(
                    model, noise_configs,
                    f"{dataset_name}_model_{model_idx}_exp_{exp_idx}"
                )

                # ä¿å­˜å•ä¸ªå®éªŒç»“æœ
                exp_save_path = save_path / f"model_{model_idx}_exp_{exp_idx}_results.json"
                evaluator.save_results(exp_save_path)

                # ä¿å­˜æŠ¥å‘Š
                report = evaluator.generate_robustness_report()
                report_path = save_path / f"model_{model_idx}_exp_{exp_idx}_report.txt"
                with open(report_path, 'w', encoding='utf-8') as f:
                    f.write(report)

        # åœ¨æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆåç”Ÿæˆå¯è§†åŒ–
        self._visualize_results(evaluator, save_path)

        logger.info("é²æ£’æ€§å®éªŒå®Œæˆ")

    def _prepare_data(self, dataset_name: str, data_path: str) -> DataManager:
        """å‡†å¤‡æ•°æ®"""
        config = self.base_config.copy()
        data_manager = DataManager(config)
        data_manager.load_dataset(dataset_name, data_path).preprocess()
        return data_manager

    def _train_model(self, data_manager: DataManager, model_config: Dict[str, Any]):
        """è®­ç»ƒæ¨¡å‹"""
        # è·å–æ•°æ®ç»´åº¦
        stats = data_manager.get_statistics()
        n_users = stats['n_users']
        n_items = stats['n_items']

        # åˆ›å»ºæŸå¤±å‡½æ•°
        loss_function = self._create_loss_function(model_config['loss_function'])

        # åˆ›å»ºæ­£åˆ™åŒ–å™¨
        regularizer = self._create_regularizer(model_config['regularizer'])

        # åˆ›å»ºåˆå§‹åŒ–å™¨
        initializer = self._create_initializer(model_config['initializer'])

        # åˆ›å»ºæ¨¡å‹
        model = MatrixFactorizationSGD(
            n_users=n_users,
            n_items=n_items,
            n_factors=model_config['n_factors'],
            learning_rate=model_config['learning_rate'],
            regularizer=regularizer,
            loss_function=loss_function,
            use_bias=model_config['use_bias'],
            global_mean=data_manager.global_mean or 0.0
        )

        # åˆå§‹åŒ–å‚æ•°
        model.initialize_parameters(initializer)

        # è·å–è®­ç»ƒæ•°æ®
        train_data, val_data, _ = data_manager.get_splits()

        # è®­ç»ƒæ¨¡å‹
        model.fit(
            train_data=train_data,
            val_data=val_data,
            n_epochs=model_config['n_epochs'],
            verbose=1
        )

        return model

    def _create_loss_function(self, loss_config: Dict[str, Any]):
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        loss_type = loss_config['type']

        if loss_type == 'L2':
            return L2Loss()
        elif loss_type == 'L1':
            return L1Loss()
        elif loss_type == 'Huber':
            return HuberLoss(delta=loss_config.get('delta', 1.0))
        elif loss_type == 'HPL':
            return HybridPiecewiseLoss(
                delta1=loss_config.get('delta1', 0.5),
                delta2=loss_config.get('delta2', 2.0),
                l_max=loss_config.get('l_max', 3.0)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {loss_type}")

    def _create_regularizer(self, reg_config: Dict[str, Any]):
        """åˆ›å»ºæ­£åˆ™åŒ–å™¨"""
        reg_type = reg_config['type']

        if reg_type == 'L2':
            return L2Regularizer(lambda_reg=reg_config.get('lambda_reg', 0.01))
        elif reg_type == 'L1':
            return L1Regularizer(lambda_reg=reg_config.get('lambda_reg', 0.01))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ­£åˆ™åŒ–å™¨: {reg_type}")

    def _create_initializer(self, init_config: Dict[str, Any]):
        """åˆ›å»ºåˆå§‹åŒ–å™¨"""
        init_type = init_config['type']

        if init_type == 'Normal':
            return NormalInitializer(
                mean=init_config.get('mean', 0.0),
                std=init_config.get('std', 0.01)
            )
        elif init_type == 'Xavier':
            return XavierInitializer()
        elif init_type == 'Uniform':
            return UniformInitializer(
                low=init_config.get('low', -0.01),
                high=init_config.get('high', 0.01)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆå§‹åŒ–å™¨: {init_type}")

    def _create_noise_configs(self, exp_config: Dict[str, Any]) -> List[NoiseConfig]:
        """åˆ›å»ºå™ªå£°é…ç½®åˆ—è¡¨"""
        noise_configs = []

        for noise_type in exp_config['noise_types']:
            for strength in exp_config['noise_strengths']:
                for ratio in exp_config['noise_ratios']:
                    config = NoiseConfig(
                        noise_type=noise_type,
                        noise_strength=strength,
                        noise_ratio=ratio,
                        random_seed=exp_config.get('random_seed', 42)
                    )
                    noise_configs.append(config)

        return noise_configs

    def _visualize_results(self, evaluator: RobustnessEvaluator, save_dir: Path):
        """å¯è§†åŒ–å®éªŒç»“æœ"""
        save_dir.mkdir(parents=True, exist_ok=True)

        # ä¸ºæ¯ä¸ªæ¨¡å‹å•ç‹¬ç”Ÿæˆå¯è§†åŒ–
        for model_idx, model_config in enumerate(self.model_configs):
            model_name = model_config['name']
            loss_type = model_config['loss_function']['type']

            model_save_dir = save_dir / f"model_{model_idx}_{model_name}"
            model_save_dir.mkdir(parents=True, exist_ok=True)

            # æå–è¯¥æ¨¡å‹çš„æ•°æ®
            model_data = self._extract_model_data(evaluator, model_idx)

            if not model_data:
                continue

            # ç”Ÿæˆå•æ¨¡å‹å¯è§†åŒ–
            self._create_single_model_plots(model_data, model_name, loss_type, model_save_dir)

        # ç”Ÿæˆå¤šæ¨¡å‹å¯¹æ¯”å›¾
        self._create_model_comparison_plots(evaluator, save_dir)

    def _extract_model_data(self, evaluator: RobustnessEvaluator, model_idx: int):
        """æå–ç‰¹å®šæ¨¡å‹çš„æ•°æ®"""
        model_data = {}

        for dataset_name, dataset_results in evaluator.results.items():
            noise_types = []
            strengths = []
            rmse_changes = []
            mae_changes = []

            for config_key, result in dataset_results['robustness_metrics'].items():
                # æ£€æŸ¥æ˜¯å¦å±äºå½“å‰æ¨¡å‹
                if f'model_{model_idx}' not in config_key:
                    continue

                parts = config_key.split('_')
                noise_type = parts[0]
                strength = float(parts[2])

                noise_types.append(noise_type)
                strengths.append(strength)
                rmse_changes.append(result.get('RMSE_relative_change', 0) * 100)
                mae_changes.append(result.get('MAE_relative_change', 0) * 100)

            if noise_types:  # åªæœ‰å½“æœ‰æ•°æ®æ—¶æ‰æ·»åŠ 
                model_data[dataset_name] = {
                    'noise_types': noise_types,
                    'strengths': strengths,
                    'rmse_changes': rmse_changes,
                    'mae_changes': mae_changes
                }

        return model_data

    def _create_single_model_plots(self, model_data: dict, model_name: str, loss_type: str, save_dir: Path):
        """ä¸ºå•ä¸ªæ¨¡å‹åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        for dataset_name, data in model_data.items():
            noise_types = data['noise_types']
            strengths = data['strengths']
            rmse_changes = data['rmse_changes']
            mae_changes = data['mae_changes']

            # åˆ›å»ºå¯è§†åŒ–
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'{model_name} ({loss_type} Loss) - {dataset_name}', fontsize=16)

            # 1. RMSEå˜åŒ– vs å™ªå£°å¼ºåº¦
            ax1 = axes[0, 0]
            unique_types = list(set(noise_types))
            for noise_type in unique_types:
                mask = [nt == noise_type for nt in noise_types]
                type_strengths = [s for s, m in zip(strengths, mask) if m]
                type_rmse = [r for r, m in zip(rmse_changes, mask) if m]
                ax1.plot(type_strengths, type_rmse, 'o-', label=noise_type)

            ax1.set_xlabel('Noise Strength')
            ax1.set_ylabel('RMSE Relative Change (%)')
            ax1.set_title('RMSE Change vs Noise Strength')
            ax1.legend()
            ax1.grid(True, alpha=0.3)

            # 2. MAEå˜åŒ– vs å™ªå£°å¼ºåº¦
            ax2 = axes[0, 1]
            for noise_type in unique_types:
                mask = [nt == noise_type for nt in noise_types]
                type_strengths = [s for s, m in zip(strengths, mask) if m]
                type_mae = [r for r, m in zip(mae_changes, mask) if m]
                ax2.plot(type_strengths, type_mae, 's-', label=noise_type)

            ax2.set_xlabel('Noise Strength')
            ax2.set_ylabel('MAE Relative Change (%)')
            ax2.set_title('MAE Change vs Noise Strength')
            ax2.legend()
            ax2.grid(True, alpha=0.3)

            # 3. ä¸åŒå™ªå£°ç±»å‹çš„é²æ£’æ€§å¯¹æ¯”
            ax3 = axes[1, 0]
            avg_rmse_by_type = {}
            avg_mae_by_type = {}

            for noise_type in unique_types:
                mask = [nt == noise_type for nt in noise_types]
                type_rmse = [r for r, m in zip(rmse_changes, mask) if m]
                type_mae = [r for r, m in zip(mae_changes, mask) if m]
                avg_rmse_by_type[noise_type] = np.mean(type_rmse)
                avg_mae_by_type[noise_type] = np.mean(type_mae)

            x_pos = np.arange(len(unique_types))
            width = 0.35

            ax3.bar(x_pos - width/2, list(avg_rmse_by_type.values()),
                   width, label='RMSE Change (%)', alpha=0.8)
            ax3.bar(x_pos + width/2, list(avg_mae_by_type.values()),
                   width, label='MAE Change (%)', alpha=0.8)

            ax3.set_xlabel('Noise Type')
            ax3.set_ylabel('Average Relative Change (%)')
            ax3.set_title('Average Impact of Different Noise Types')
            ax3.set_xticks(x_pos)
            ax3.set_xticklabels(unique_types)
            ax3.legend()
            ax3.grid(True, alpha=0.3)

            # 4. é²æ£’æ€§çƒ­åŠ›å›¾
            ax4 = axes[1, 1]

            # åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®
            if len(unique_types) > 1 and len(set(strengths)) > 1:
                heatmap_data = np.zeros((len(unique_types), len(set(strengths))))
                strength_values = sorted(set(strengths))

                for i, noise_type in enumerate(unique_types):
                    for j, strength in enumerate(strength_values):
                        # æ‰¾åˆ°å¯¹åº”çš„RMSEå˜åŒ–å€¼
                        for nt, s, rmse_change in zip(noise_types, strengths, rmse_changes):
                            if nt == noise_type and s == strength:
                                heatmap_data[i, j] = rmse_change
                                break

                im = ax4.imshow(heatmap_data, cmap='RdYlBu_r', aspect='auto')
                ax4.set_xticks(range(len(strength_values)))
                ax4.set_xticklabels([f'{s:.2f}' for s in strength_values])
                ax4.set_yticks(range(len(unique_types)))
                ax4.set_yticklabels(unique_types)
                ax4.set_xlabel('Noise Strength')
                ax4.set_ylabel('Noise Type')
                ax4.set_title('RMSE Change Heatmap (%)')

                # æ·»åŠ é¢œè‰²æ¡
                plt.colorbar(im, ax=ax4)
            else:
                ax4.text(0.5, 0.5, 'Insufficient data for heatmap',
                        transform=ax4.transAxes, ha='center', va='center')
                ax4.set_title('RMSE Change Heatmap')

            plt.tight_layout()

            # ä¿å­˜å›¾åƒ
            plot_path = save_dir / f'{dataset_name}_robustness_analysis.png'
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()

            logger.info(f"å•æ¨¡å‹å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {plot_path}")

    def _create_model_comparison_plots(self, evaluator: RobustnessEvaluator, save_dir: Path):
        """åˆ›å»ºå¤šæ¨¡å‹å¯¹æ¯”å›¾è¡¨"""
        if len(self.model_configs) < 2:
            logger.info("æ¨¡å‹æ•°é‡å°‘äº2ä¸ªï¼Œè·³è¿‡æ¨¡å‹å¯¹æ¯”å›¾ç”Ÿæˆ")
            return

        comparison_dir = save_dir / "model_comparison"
        comparison_dir.mkdir(parents=True, exist_ok=True)

        # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºæ¨¡å‹å¯¹æ¯”å›¾
        for dataset_name in evaluator.results.keys():
            self._create_dataset_comparison_plot(evaluator, dataset_name, comparison_dir)

    def _create_dataset_comparison_plot(self, evaluator: RobustnessEvaluator, dataset_name: str, save_dir: Path):
        """ä¸ºç‰¹å®šæ•°æ®é›†åˆ›å»ºç¾è§‚çš„æ¨¡å‹å¯¹æ¯”å›¾"""
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„æ•°æ®
        model_data = {}
        loss_types = {}

        for model_idx, model_config in enumerate(self.model_configs):
            model_name = model_config['name']
            loss_type = model_config['loss_function']['type']
            loss_types[model_name] = loss_type

            # æå–è¯¥æ¨¡å‹çš„æ•°æ®
            data = self._extract_model_data(evaluator, model_idx)
            if dataset_name in data:
                model_data[model_name] = data[dataset_name]

        if not model_data:
            logger.warning(f"æ²¡æœ‰æ‰¾åˆ°æ•°æ®é›† {dataset_name} çš„æ¨¡å‹æ•°æ®")
            return

        # åˆ›å»ºä¸“ä¸šçš„é…è‰²æ–¹æ¡ˆ
        loss_color_map = {
            'L2': '#E74C3C',      # çº¢è‰² - ä¼ ç»Ÿæ–¹æ³•
            'L1': '#F39C12',      # æ©™è‰² - ä¼ ç»Ÿæ–¹æ³•
            'Huber': '#3498DB',   # è“è‰² - é²æ£’æ–¹æ³•
            'HPL': '#27AE60',     # ç»¿è‰² - æˆ‘ä»¬çš„æ–¹æ³•ï¼ˆçªå‡ºæ˜¾ç¤ºï¼‰
            'MAE': '#9B59B6',     # ç´«è‰²
            'MSE': '#E67E22'      # æ·±æ©™è‰²
        }

        loss_marker_map = {
            'L2': 'o',      # åœ†å½¢
            'L1': 's',      # æ–¹å½¢
            'Huber': '^',   # ä¸‰è§’å½¢
            'HPL': 'D',     # é’»çŸ³å½¢ï¼ˆçªå‡ºæ˜¾ç¤ºï¼‰
            'MAE': 'v',     # å€’ä¸‰è§’
            'MSE': '<'      # å·¦ä¸‰è§’
        }

        # åˆ›å»ºç¾è§‚çš„å¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 2, figsize=(18, 14))
        fig.patch.set_facecolor('white')

        # ä¸»æ ‡é¢˜ - ä½¿ç”¨æ›´ä¸“ä¸šçš„æ ·å¼
        fig.suptitle(f'Loss Function Robustness Comparison\nDataset: {dataset_name}',
                    fontsize=18, fontweight='bold', y=0.95,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor='lightblue', alpha=0.3))

        # 1. RMSEé²æ£’æ€§å¯¹æ¯” - é«˜æ–¯å™ªå£°
        ax1 = axes[0, 0]
        for model_name, data in model_data.items():
            loss_type = loss_types[model_name]

            # æŒ‰é«˜æ–¯å™ªå£°ç»˜åˆ¶
            gaussian_mask = [nt == 'gaussian' for nt in data['noise_types']]
            if any(gaussian_mask):
                gaussian_strengths = [s for s, m in zip(data['strengths'], gaussian_mask) if m]
                gaussian_rmse = [r for r, m in zip(data['rmse_changes'], gaussian_mask) if m]

                # æ’åºä»¥ä¾¿ç»˜åˆ¶è¿ç»­çº¿æ¡
                sorted_data = sorted(zip(gaussian_strengths, gaussian_rmse))
                strengths_sorted, rmse_sorted = zip(*sorted_data) if sorted_data else ([], [])

                # ä½¿ç”¨ä¸“ä¸šé…è‰²
                color = loss_color_map.get(loss_type, '#7F8C8D')
                marker = loss_marker_map.get(loss_type, 'o')

                # HPLä½¿ç”¨æ›´ç²—çš„çº¿æ¡çªå‡ºæ˜¾ç¤º
                linewidth = 3.5 if loss_type == 'HPL' else 2.5
                markersize = 10 if loss_type == 'HPL' else 8
                alpha = 1.0 if loss_type == 'HPL' else 0.8

                ax1.plot(strengths_sorted, rmse_sorted,
                        color=color, marker=marker,
                        linewidth=linewidth, markersize=markersize,
                        alpha=alpha, label=f'{loss_type} Loss',
                        markeredgecolor='white', markeredgewidth=1)

        # ç¾åŒ–åæ ‡è½´
        ax1.set_xlabel('Gaussian Noise Strength', fontsize=12, fontweight='bold')
        ax1.set_ylabel('RMSE Relative Change (%)', fontsize=12, fontweight='bold')
        ax1.set_title('RMSE Robustness vs Noise Strength', fontsize=14, fontweight='bold', pad=20)
        ax1.legend(frameon=True, fancybox=True, shadow=True, fontsize=10)
        ax1.grid(True, alpha=0.3, linestyle='--')
        ax1.spines['top'].set_visible(False)
        ax1.spines['right'].set_visible(False)

        # 2. MAEé²æ£’æ€§å¯¹æ¯” - é«˜æ–¯å™ªå£°
        ax2 = axes[0, 1]
        for model_name, data in model_data.items():
            loss_type = loss_types[model_name]

            gaussian_mask = [nt == 'gaussian' for nt in data['noise_types']]
            if any(gaussian_mask):
                gaussian_strengths = [s for s, m in zip(data['strengths'], gaussian_mask) if m]
                gaussian_mae = [r for r, m in zip(data['mae_changes'], gaussian_mask) if m]

                sorted_data = sorted(zip(gaussian_strengths, gaussian_mae))
                strengths_sorted, mae_sorted = zip(*sorted_data) if sorted_data else ([], [])

                # ä½¿ç”¨ä¸“ä¸šé…è‰²
                color = loss_color_map.get(loss_type, '#7F8C8D')
                marker = loss_marker_map.get(loss_type, 'o')

                # HPLä½¿ç”¨æ›´ç²—çš„çº¿æ¡çªå‡ºæ˜¾ç¤º
                linewidth = 3.5 if loss_type == 'HPL' else 2.5
                markersize = 10 if loss_type == 'HPL' else 8
                alpha = 1.0 if loss_type == 'HPL' else 0.8

                ax2.plot(strengths_sorted, mae_sorted,
                        color=color, marker=marker,
                        linewidth=linewidth, markersize=markersize,
                        alpha=alpha, label=f'{loss_type} Loss',
                        markeredgecolor='white', markeredgewidth=1)

        # ç¾åŒ–åæ ‡è½´
        ax2.set_xlabel('Gaussian Noise Strength', fontsize=12, fontweight='bold')
        ax2.set_ylabel('MAE Relative Change (%)', fontsize=12, fontweight='bold')
        ax2.set_title('MAE Robustness vs Noise Strength', fontsize=14, fontweight='bold', pad=20)
        ax2.legend(frameon=True, fancybox=True, shadow=True, fontsize=10)
        ax2.grid(True, alpha=0.3, linestyle='--')
        ax2.spines['top'].set_visible(False)
        ax2.spines['right'].set_visible(False)

        # 3. æ•´ä½“é²æ£’æ€§å¯¹æ¯” - ç¾è§‚çš„æŸ±çŠ¶å›¾
        ax3 = axes[1, 0]
        loss_names = []
        avg_rmse_changes = []
        avg_mae_changes = []
        bar_colors = []

        for model_name, data in model_data.items():
            loss_type = loss_types[model_name]
            loss_names.append(loss_type)
            avg_rmse_changes.append(np.mean([abs(r) for r in data['rmse_changes']]))
            avg_mae_changes.append(np.mean([abs(r) for r in data['mae_changes']]))
            bar_colors.append(loss_color_map.get(loss_type, '#7F8C8D'))

        x_pos = np.arange(len(loss_names))
        width = 0.35

        # åˆ›å»ºæ¸å˜æ•ˆæœçš„æŸ±çŠ¶å›¾
        bars1 = ax3.bar(x_pos - width/2, avg_rmse_changes, width,
                       label='RMSE Change (%)', alpha=0.8,
                       color=[c for c in bar_colors], edgecolor='white', linewidth=1.5)
        bars2 = ax3.bar(x_pos + width/2, avg_mae_changes, width,
                       label='MAE Change (%)', alpha=0.6,
                       color=[c for c in bar_colors], edgecolor='white', linewidth=1.5)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):
            height1 = bar1.get_height()
            height2 = bar2.get_height()
            ax3.text(bar1.get_x() + bar1.get_width()/2., height1 + 0.1,
                    f'{height1:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=9)
            ax3.text(bar2.get_x() + bar2.get_width()/2., height2 + 0.1,
                    f'{height2:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=9)

        # ç¾åŒ–åæ ‡è½´
        ax3.set_xlabel('Loss Function', fontsize=12, fontweight='bold')
        ax3.set_ylabel('Average Performance Change (%)', fontsize=12, fontweight='bold')
        ax3.set_title('Overall Robustness Comparison', fontsize=14, fontweight='bold', pad=20)
        ax3.set_xticks(x_pos)
        ax3.set_xticklabels(loss_names, fontsize=11, fontweight='bold')
        ax3.legend(frameon=True, fancybox=True, shadow=True, fontsize=10)
        ax3.grid(True, alpha=0.3, linestyle='--', axis='y')
        ax3.spines['top'].set_visible(False)
        ax3.spines['right'].set_visible(False)

        # 4. é²æ£’æ€§æ’å - ä¸“ä¸šçš„æ’åå›¾
        ax4 = axes[1, 1]

        # è®¡ç®—ç»¼åˆé²æ£’æ€§å¾—åˆ†ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        robustness_scores = []
        for model_name, data in model_data.items():
            loss_type = loss_types[model_name]
            # ä½¿ç”¨RMSEå’ŒMAEå˜åŒ–çš„å¹³å‡ç»å¯¹å€¼ä½œä¸ºé²æ£’æ€§å¾—åˆ†
            rmse_score = np.mean([abs(r) for r in data['rmse_changes']])
            mae_score = np.mean([abs(r) for r in data['mae_changes']])
            combined_score = (rmse_score + mae_score) / 2
            robustness_scores.append((loss_type, combined_score))

        # æŒ‰å¾—åˆ†æ’åºï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        robustness_scores.sort(key=lambda x: x[1])

        loss_names, scores = zip(*robustness_scores)
        y_pos = np.arange(len(loss_names))

        # åˆ›å»ºæ¸å˜è‰²å½©çš„æ°´å¹³æŸ±çŠ¶å›¾
        colors_for_ranking = []
        for i, (_, _) in enumerate(robustness_scores):
            if i == 0:  # æœ€ä½³
                colors_for_ranking.append('#27AE60')  # ç»¿è‰²
            elif i == 1:  # ç¬¬äºŒ
                colors_for_ranking.append('#3498DB')  # è“è‰²
            elif i == 2:  # ç¬¬ä¸‰
                colors_for_ranking.append('#F39C12')  # æ©™è‰²
            else:  # å…¶ä»–
                colors_for_ranking.append('#E74C3C')  # çº¢è‰²

        ax4.barh(y_pos, scores, alpha=0.8, color=colors_for_ranking,
                edgecolor='white', linewidth=2)

        # æ·»åŠ æ’åæ ‡è®°
        rank_labels = ['ğŸ¥‡', 'ğŸ¥ˆ', 'ğŸ¥‰'] + [f'{i+1}' for i in range(3, len(loss_names))]

        ax4.set_yticks(y_pos)
        ax4.set_yticklabels([f'{rank} {name}' for rank, name in zip(rank_labels, loss_names)],
                           fontsize=11, fontweight='bold')
        ax4.set_xlabel('Robustness Score (lower = better)', fontsize=12, fontweight='bold')
        ax4.set_title('Loss Function Robustness Ranking', fontsize=14, fontweight='bold', pad=20)
        ax4.grid(True, alpha=0.3, linestyle='--', axis='x')
        ax4.spines['top'].set_visible(False)
        ax4.spines['right'].set_visible(False)

        # æ·»åŠ ç²¾ç¾çš„æ•°å€¼æ ‡ç­¾
        for i, (_, score) in enumerate(robustness_scores):
            ax4.text(score + max(scores) * 0.02, i, f'{score:.2f}',
                    va='center', ha='left', fontweight='bold', fontsize=10,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8))

        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        plot_path = save_dir / f'{dataset_name}_model_comparison.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"æ¨¡å‹å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {plot_path}")

        # åˆ›å»ºé¢å¤–çš„æ€»ç»“å›¾
        self._create_summary_robustness_plot(evaluator, dataset_name, save_dir)

    def _create_summary_robustness_plot(self, evaluator: RobustnessEvaluator, dataset_name: str, save_dir: Path):
        """åˆ›å»ºç®€æ´ç¾è§‚çš„é²æ£’æ€§æ€»ç»“å›¾"""
        # æ”¶é›†æ•°æ®
        model_data = {}
        loss_types = {}

        for model_idx, model_config in enumerate(self.model_configs):
            model_name = model_config['name']
            loss_type = model_config['loss_function']['type']
            loss_types[model_name] = loss_type

            data = self._extract_model_data(evaluator, model_idx)
            if dataset_name in data:
                model_data[model_name] = data[dataset_name]

        if not model_data:
            return

        # åˆ›å»ºä¸“ä¸šçš„å•å›¾æ€»ç»“
        fig, ax = plt.subplots(1, 1, figsize=(12, 8))
        fig.patch.set_facecolor('white')

        # é…è‰²æ–¹æ¡ˆ
        loss_color_map = {
            'L2': '#E74C3C', 'L1': '#F39C12', 'Huber': '#3498DB',
            'HPL': '#27AE60', 'MAE': '#9B59B6', 'MSE': '#E67E22'
        }
        loss_marker_map = {
            'L2': 'o', 'L1': 's', 'Huber': '^',
            'HPL': 'D', 'MAE': 'v', 'MSE': '<'
        }

        # ç»˜åˆ¶é«˜æ–¯å™ªå£°ä¸‹çš„RMSEå˜åŒ–
        for model_name, data in model_data.items():
            loss_type = loss_types[model_name]

            gaussian_mask = [nt == 'gaussian' for nt in data['noise_types']]
            if any(gaussian_mask):
                gaussian_strengths = [s for s, m in zip(data['strengths'], gaussian_mask) if m]
                gaussian_rmse = [r for r, m in zip(data['rmse_changes'], gaussian_mask) if m]

                sorted_data = sorted(zip(gaussian_strengths, gaussian_rmse))
                strengths_sorted, rmse_sorted = zip(*sorted_data) if sorted_data else ([], [])

                color = loss_color_map.get(loss_type, '#7F8C8D')
                marker = loss_marker_map.get(loss_type, 'o')

                # HPLç‰¹æ®Šå¤„ç†
                if loss_type == 'HPL':
                    ax.plot(strengths_sorted, rmse_sorted, color=color, marker=marker,
                           linewidth=4, markersize=12, alpha=1.0, label=f'{loss_type} (Proposed)',
                           markeredgecolor='white', markeredgewidth=2, zorder=10)
                else:
                    ax.plot(strengths_sorted, rmse_sorted, color=color, marker=marker,
                           linewidth=2.5, markersize=8, alpha=0.8, label=f'{loss_type}',
                           markeredgecolor='white', markeredgewidth=1)

        # ç¾åŒ–å›¾è¡¨
        ax.set_xlabel('Gaussian Noise Strength', fontsize=14, fontweight='bold')
        ax.set_ylabel('RMSE Relative Change (%)', fontsize=14, fontweight='bold')
        ax.set_title(f'Loss Function Robustness Comparison\n{dataset_name} Dataset',
                    fontsize=16, fontweight='bold', pad=25)

        # å›¾ä¾‹ç¾åŒ–
        legend = ax.legend(frameon=True, fancybox=True, shadow=True, fontsize=12,
                          loc='upper left', bbox_to_anchor=(0.02, 0.98))
        legend.get_frame().set_facecolor('white')
        legend.get_frame().set_alpha(0.9)

        # ç½‘æ ¼å’Œè¾¹æ¡†
        ax.grid(True, alpha=0.3, linestyle='--')
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['left'].set_linewidth(1.5)
        ax.spines['bottom'].set_linewidth(1.5)

        # æ·»åŠ æ€§èƒ½è¯´æ˜æ–‡æœ¬
        textstr = 'Lower curves indicate\nbetter robustness'
        props = dict(boxstyle='round', facecolor='lightblue', alpha=0.8)
        ax.text(0.98, 0.02, textstr, transform=ax.transAxes, fontsize=11,
                verticalalignment='bottom', horizontalalignment='right', bbox=props)

        plt.tight_layout()

        # ä¿å­˜æ€»ç»“å›¾
        summary_path = save_dir / f'{dataset_name}_robustness_summary.png'
        plt.savefig(summary_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()

        logger.info(f"é²æ£’æ€§æ€»ç»“å›¾å·²ä¿å­˜åˆ°: {summary_path}")


def run_demo_experiment():
    """è¿è¡Œæ¼”ç¤ºå®éªŒ"""
    logger.info("å¼€å§‹æ¼”ç¤ºå®éªŒ...")

    # åŸºç¡€é…ç½®
    base_config = {
        'random_seed': 42,
        'train_ratio': 0.8,
        'val_ratio': 0.1,
        'test_ratio': 0.1,
        'center_data': True,
        'ensure_user_in_train': True
    }

    # ç®€å•çš„å™ªå£°å®éªŒé…ç½®
    noise_exp_config = {
        'noise_types': ['gaussian', 'uniform'],
        'noise_strengths': [0.1, 0.3, 0.5],
        'noise_ratios': [0.5, 1.0],
        'random_seed': 42
    }

    # ç®€å•çš„æ¨¡å‹é…ç½®
    model_config = {
        'name': 'L2_baseline_demo',
        'n_factors': 20,
        'learning_rate': 0.01,
        'n_epochs': 50,
        'use_bias': True,
        'loss_function': {'type': 'L2'},
        'regularizer': {'type': 'L2', 'lambda_reg': 0.01},
        'initializer': {'type': 'Normal', 'mean': 0.0, 'std': 0.01}
    }

    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = ExperimentRunner(base_config)

    # è¿è¡Œå®éªŒ
    try:
        runner.run_robustness_experiment(
            dataset_name='MovieLens100K',
            data_path='dataset/20201202M100K_data_all_random.txt',
            noise_experiment_configs=[noise_exp_config],
            model_configs=[model_config],
            save_dir='demo_results'
        )
        logger.info("æ¼”ç¤ºå®éªŒå®Œæˆ")
    except Exception as e:
        logger.error(f"æ¼”ç¤ºå®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def run_full_experiment():
    """è¿è¡Œå®Œæ•´å®éªŒ"""
    try:
        # å¯¼å…¥é…ç½®æ–‡ä»¶
        from noise_injection_config import get_experiment_config

        # è·å–å®Œæ•´å®éªŒé…ç½®
        config = get_experiment_config('comprehensive_robustness')

        # åˆ›å»ºå®éªŒè¿è¡Œå™¨
        runner = ExperimentRunner(config['base_config'])

        # å¯¹æ¯ä¸ªæ•°æ®é›†è¿è¡Œå®éªŒ
        for dataset_name in config['datasets']:
            # è·å–æ•°æ®é›†è·¯å¾„
            dataset_configs = {
                'movielens100k': 'dataset/20201202M100K_data_all_random.txt',
                'netflix': 'dataset/20201202NetFlix_data_all_random.txt',
                'filmtrust': 'dataset/flimtrust20220604random.txt'
            }

            if dataset_name not in dataset_configs:
                logger.warning(f"è·³è¿‡æœªçŸ¥æ•°æ®é›†: {dataset_name}")
                continue

            data_path = dataset_configs[dataset_name]

            runner.run_robustness_experiment(
                dataset_name=dataset_name,
                data_path=data_path,
                noise_experiment_configs=config['noise_experiments'],
                model_configs=config['models'],
                save_dir=f'full_results/{dataset_name}'
            )

        logger.info("å®Œæ•´å®éªŒå®Œæˆ")
        send_sms_notification(f"å™ªå£°æ³¨å…¥é²æ£’æ€§å®éªŒå®Œæˆï¼Œæ•°æ®é›†: {', '.join(config['datasets'])}")

    except ImportError:
        logger.error("æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶ï¼Œè¯·ç¡®ä¿ noise_injection_config.py å­˜åœ¨")
        send_sms_notification("å™ªå£°æ³¨å…¥é²æ£’æ€§å®éªŒå¤±è´¥: æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶")
    except Exception as e:
        logger.error(f"å®Œæ•´å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        send_sms_notification(f"å™ªå£°æ³¨å…¥é²æ£’æ€§å®éªŒå¤±è´¥: {str(e)[:50]}...")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='å™ªå£°æ³¨å…¥é²æ£’æ€§æµ‹è¯•ç³»ç»Ÿ')
    parser.add_argument('--mode', type=str, default='demo',
                       choices=['demo', 'full'],
                       help='è¿è¡Œæ¨¡å¼: demo(æ¼”ç¤º) æˆ– full(å®Œæ•´å®éªŒ)')

    args = parser.parse_args()

    if args.mode == 'demo':
        run_demo_experiment()
    elif args.mode == 'full':
        run_full_experiment()
    else:
        logger.error(f"æœªçŸ¥çš„è¿è¡Œæ¨¡å¼: {args.mode}")


if __name__ == "__main__":
    main()

