#!/usr/bin/env python3
"""
å®éªŒæ‰§è¡Œå™¨æ¨¡å—
è´Ÿè´£åè°ƒå’Œæ‰§è¡Œå®Œæ•´çš„é²æ£’æ€§æµ‹è¯•å®éªŒ
"""

import sys
import os
import time
import json
import logging
import numpy as np
from typing import Dict, List, Any, Tuple, Optional
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from dataset_selector import DatasetInfo
from config_matcher import OptimalConfig
from result_manager import ResultManager
from data.data_manager import DataManager
from src.models.mf_sgd import MatrixFactorizationSGD
from src.models.initializers import NormalInitializer, XavierInitializer, UniformInitializer
from src.models.regularizers import L2Regularizer, L1Regularizer
from src.losses.standard import L1Loss, L2Loss
from src.losses.hpl import HybridPiecewiseLoss
from src.losses.robust import HuberLoss, LogcoshLoss
from src.losses.sigmoid import SigmoidLikeLoss

# å‡è®¾å·²æœ‰çš„å™ªå£°æ³¨å…¥ç³»ç»Ÿ
from noise_injection_system import NoiseConfig, RobustnessEvaluator

logger = logging.getLogger(__name__)


class ExperimentStatus:
    """å®éªŒçŠ¶æ€è·Ÿè¸ªå™¨"""

    def __init__(self):
        """åˆå§‹åŒ–çŠ¶æ€è·Ÿè¸ªå™¨"""
        self.start_time = None
        self.total_tasks = 0
        self.completed_tasks = 0
        self.successful_tasks = 0
        self.failed_tasks = 0
        self.current_task = ""
        self.task_start_time = None

    def start(self, total_tasks: int):
        """å¼€å§‹å®éªŒ"""
        self.start_time = time.time()
        self.total_tasks = total_tasks
        self.completed_tasks = 0
        self.successful_tasks = 0
        self.failed_tasks = 0
        self.current_task = "åˆå§‹åŒ–"
        self.task_start_time = time.time()

    def update(self, task_description: str):
        """æ›´æ–°å½“å‰ä»»åŠ¡"""
        self.current_task = task_description
        self.task_start_time = time.time()

    def complete_task(self, success: bool, message: str = ""):
        """å®Œæˆä»»åŠ¡"""
        self.completed_tasks += 1
        if success:
            self.successful_tasks += 1
        else:
            self.failed_tasks += 1
            logger.warning(f"ä»»åŠ¡å¤±è´¥: {message}")

    def get_progress(self) -> Tuple[float, str]:
        """è·å–è¿›åº¦ä¿¡æ¯"""
        if self.total_tasks == 0:
            return 0.0, "æœªå¼€å§‹"

        progress = self.completed_tasks / self.total_tasks
        elapsed = time.time() - self.start_time

        if progress > 0:
            estimated_total = elapsed / progress
            remaining = estimated_total - elapsed
            time_info = f"å·²ç”¨æ—¶: {self._format_time(elapsed)} | å‰©ä½™: {self._format_time(remaining)}"
        else:
            time_info = f"å·²ç”¨æ—¶: {self._format_time(elapsed)}"

        return progress, time_info

    def get_summary(self) -> str:
        """è·å–å®éªŒæ‘˜è¦"""
        elapsed = time.time() - self.start_time
        return (f"å®éªŒå®Œæˆ: {self.completed_tasks}/{self.total_tasks} ä»»åŠ¡ | "
                f"æˆåŠŸ: {self.successful_tasks} | å¤±è´¥: {self.failed_tasks} | "
                f"æ€»ç”¨æ—¶: {self._format_time(elapsed)}")

    def _format_time(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´"""
        hours, remainder = divmod(int(seconds), 3600)
        minutes, seconds = divmod(remainder, 60)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"


class ExperimentExecutor:
    """å®éªŒæ‰§è¡Œå™¨"""

    def __init__(self, base_config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–å®éªŒæ‰§è¡Œå™¨

        Args:
            base_config: åŸºç¡€é…ç½®
        """
        self.base_config = base_config or self._get_default_base_config()
        self.status = ExperimentStatus()

        # åˆå§‹åŒ–æ—¶ä¸åˆ›å»ºResultManagerï¼Œè€Œæ˜¯åœ¨éœ€è¦æ—¶è®¾ç½®
        self.result_manager = None  # ä¸è¦åœ¨è¿™é‡Œåˆ›å»ºResultManagerå®ä¾‹

        # å™ªå£°å®éªŒé…ç½®
        self.noise_experiment_configs = self._get_default_noise_configs()

    def _get_default_base_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤åŸºç¡€é…ç½®"""
        return {
            'random_seed': 42,
            'train_ratio': 0.8,
            'val_ratio': 0.1,
            'test_ratio': 0.1,
            'center_data': True,
            'ensure_user_in_train': True
        }

    def _get_default_noise_configs(self) -> List[Dict[str, Any]]:
        """è·å–é»˜è®¤å™ªå£°å®éªŒé…ç½®"""
        return [
            # é«˜æ–¯å™ªå£°æµ‹è¯•
            {
                'name': 'gaussian_test',
                'description': 'é«˜æ–¯å™ªå£°æµ‹è¯•',
                'noise_types': ['gaussian'],
                'noise_strengths': [0.1, 0.2, 0.3, 0.5, 0.8],
                'noise_ratios': [0.1, 0.5, 1.0],
                'random_seed': 42
            },
            # å¼‚å¸¸å€¼æµ‹è¯•
            {
                'name': 'outlier_test',
                'description': 'å¼‚å¸¸å€¼å™ªå£°æµ‹è¯•',
                'noise_types': ['outlier'],
                'noise_strengths': [2.0, 3.0, 5.0],
                'noise_ratios': [0.01, 0.05, 0.1],
                'random_seed': 42
            },
            # æ¤’ç›å™ªå£°æµ‹è¯•
            {
                'name': 'salt_pepper_test',
                'description': 'æ¤’ç›å™ªå£°æµ‹è¯•',
                'noise_types': ['salt_pepper'],
                'noise_strengths': [0],
                'noise_ratios': [0.01, 0.05, 0.1, 0.2],
                'random_seed': 42
            },
            # ç³»ç»Ÿæ€§åå·®æµ‹è¯•
            {
                'name': 'systematic_test',
                'description': 'ç³»ç»Ÿæ€§åå·®æµ‹è¯•',
                'noise_types': ['systematic'],
                'noise_strengths': [-0.5, -0.2, 0.2, 0.5],
                'noise_ratios': [0.5, 1.0],
                'random_seed': 42
            }
        ]

    def execute_robustness_experiment(self,
                                    datasets: List[DatasetInfo],
                                    configs: List[OptimalConfig],
                                    save_dir: str = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œé²æ£’æ€§å®éªŒ

        Args:
            datasets: æ•°æ®é›†åˆ—è¡¨
            configs: ä¼˜åŒ–é…ç½®åˆ—è¡¨
            save_dir: ä¿å­˜ç›®å½•

        Returns:
            å®éªŒç»“æœæ‘˜è¦
        """
        # å¦‚æœæ²¡æœ‰æŒ‡å®šä¿å­˜ç›®å½•ï¼Œåˆ™ä½¿ç”¨ç»å¯¹è·¯å¾„
        if save_dir is None:
            # è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
            current_file = Path(__file__).resolve()
            # æ„å»ºä¿å­˜è·¯å¾„ - ä½¿ç”¨æ­£ç¡®çš„ç›®å½•åç§°
            save_dir = str(current_file.parent / "robustness_results")
            print(f"ä¿å­˜ç›®å½•è®¾ç½®ä¸º: {save_dir}")

        # è®¡ç®—æ€»ä»»åŠ¡æ•°
        total_tasks = len(datasets) * len(configs) * len(self.noise_experiment_configs)
        self.status.start(total_tasks)

        experiment_results = {}
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)

        # å¦‚æœæ²¡æœ‰è®¾ç½®result_managerï¼Œåˆ™åˆ›å»ºä¸€ä¸ª
        if self.result_manager is None:
            from result_manager import ResultManager
            self.result_manager = ResultManager(str(save_path))
            logger.info(f"ä¸ºå®éªŒåˆ›å»ºäº†æ–°çš„ResultManagerï¼Œä¿å­˜ç›®å½•: {save_path}")

        # ä¿å­˜å®éªŒå…ƒä¿¡æ¯
        experiment_meta = {
            'start_time': datetime.now().isoformat(),
            'datasets': [d.name for d in datasets],
            'total_tasks': total_tasks,
            'base_config': self.base_config
        }

        # å¯¹æ¯ä¸ªæ•°æ®é›†è¿è¡Œå®éªŒ
        for dataset_idx, dataset in enumerate(datasets):
            dataset_results = self._execute_dataset_experiment(
                dataset, configs, save_path, dataset_idx
            )
            experiment_results[dataset.name] = dataset_results

            # æ˜¾ç¤ºè¿›åº¦
            progress, time_info = self.status.get_progress()
            print(f"\nğŸ“Š æ€»è¿›åº¦: {progress*100:.1f}% | {time_info}")

        # ç”Ÿæˆæœ€ç»ˆæ‘˜è¦
        summary = self._generate_experiment_summary(experiment_results)

        print(f"\n{self.status.get_summary()}")
        return summary

    def _execute_dataset_experiment(self,
                                   dataset: DatasetInfo,
                                   configs: List[OptimalConfig],
                                   save_path: Path,
                                   dataset_idx: int) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªæ•°æ®é›†çš„å®éªŒ"""

        dataset_results = {
            'dataset_info': {
                'name': dataset.name,
                'display_name': dataset.display_name,
                'file_path': dataset.file_path,
                'file_size': dataset.file_size
            },
            'model_results': {},
            'experiment_summary': {}
        }

        print(f"\n{'='*80}")
        print(f"ğŸ“Š å¼€å§‹æ•°æ®é›†å®éªŒ: {dataset.display_name}")
        print(f"{'='*80}")

        try:
            # å‡†å¤‡æ•°æ®
            self.status.update(f"å‡†å¤‡æ•°æ®é›†: {dataset.display_name}")
            data_manager = self._prepare_data(dataset)

            # å¯¹æ¯ä¸ªé…ç½®è¿è¡Œå®éªŒ
            for config_idx, config in enumerate(configs):
                model_results = self._execute_model_experiment(
                    data_manager, config, save_path, dataset, config_idx
                )
                dataset_results['model_results'][config.dataset_name] = model_results

            # å¦‚æœæœ‰ç»“æœç®¡ç†å™¨ï¼Œä¿å­˜æ•°æ®é›†ç»“æœ
            if self.result_manager:
                result_path = self.result_manager.save_experiment_results(
                    dataset_results,
                    f"{dataset.name}_experiment",
                    dataset.name,
                    "robustness"
                )
                logger.info(f"æ•°æ®é›† {dataset.name} å®éªŒç»“æœå·²ä¿å­˜: {result_path}")

            return dataset_results

        except Exception as e:
            error_msg = f"æ•°æ®é›† {dataset.name} å®éªŒå¤±è´¥: {e}"
            logger.error(error_msg)
            self.status.complete_task(False, error_msg)
            return {'error': error_msg}

    def _execute_model_experiment(self,
                                 data_manager: DataManager,
                                 config: OptimalConfig,
                                 save_path: Path,
                                 dataset: DatasetInfo,
                                 config_idx: int) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªæ¨¡å‹é…ç½®çš„å®éªŒ"""

        model_results = {
            'config_info': {
                'dataset_name': config.dataset_name,
                'confidence': config.confidence,
                'source_file': config.source_file
            },
            'loss_function_results': {},
            'summary': {}
        }

        try:
            # æ„å»ºæ¨¡å‹é…ç½®
            model_configs = config.build_model_configs(config) if hasattr(config, 'build_model_configs') else self._build_model_configs_from_optimal(config)

            # å¯¹æ¯ä¸ªæŸå¤±å‡½æ•°è¿è¡Œå®éªŒ
            for loss_idx, model_config in enumerate(model_configs):
                self.status.update(f"è®­ç»ƒæ¨¡å‹: {dataset.display_name} - {model_config['name']}")

                loss_results = self._execute_loss_function_experiment(
                    data_manager, model_config, save_path, dataset, config_idx, loss_idx
                )
                model_results['loss_function_results'][model_config['name']] = loss_results

            return model_results

        except Exception as e:
            error_msg = f"æ¨¡å‹é…ç½®å®éªŒå¤±è´¥: {e}"
            logger.error(error_msg)
            self.status.complete_task(False, error_msg)
            return {'error': error_msg}

    def _execute_loss_function_experiment(self,
                                        data_manager: DataManager,
                                        model_config: Dict[str, Any],
                                        save_path: Path,
                                        dataset: DatasetInfo,
                                        config_idx: int,
                                        loss_idx: int) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªæŸå¤±å‡½æ•°çš„å®éªŒ"""

        loss_results = {
            'model_config': model_config,
            'noise_experiment_results': {},
            'training_info': {}
        }

        try:
            # è®­ç»ƒæ¨¡å‹
            model, training_info = self._train_model(data_manager, model_config)
            loss_results['training_info'] = training_info

            # åˆ›å»ºé²æ£’æ€§è¯„ä¼°å™¨
            evaluator = RobustnessEvaluator(data_manager)

            # å¯¹æ¯ä¸ªå™ªå£°å®éªŒé…ç½®è¿›è¡Œæµ‹è¯•
            for noise_exp_idx, noise_exp_config in enumerate(self.noise_experiment_configs):
                self.status.update(f"å™ªå£°æµ‹è¯•: {dataset.display_name} - {model_config['name']} - {noise_exp_config['name']}")

                # åˆ›å»ºå™ªå£°é…ç½®åˆ—è¡¨
                noise_configs = self._create_noise_configs(noise_exp_config)

                # è¯„ä¼°é²æ£’æ€§
                experiment_name = f"{dataset.name}_{model_config['name']}_{noise_exp_config['name']}"
                robustness_results = evaluator.evaluate_model_robustness(
                    model, noise_configs, experiment_name
                )

                loss_results['noise_experiment_results'][noise_exp_config['name']] = robustness_results

                # ä¿å­˜å•ä¸ªå®éªŒç»“æœ
                result_file = save_path / f"{experiment_name}_results.json"
                evaluator.save_results(result_file)

                # ä¿å­˜æŠ¥å‘Š
                report = evaluator.generate_robustness_report()
                report_file = save_path / f"{experiment_name}_report.txt"
                with open(report_file, 'w', encoding='utf-8') as f:
                    f.write(report)

                self.status.complete_task(True)

            return loss_results

        except Exception as e:
            error_msg = f"æŸå¤±å‡½æ•° {model_config['name']} å®éªŒå¤±è´¥: {e}"
            logger.error(error_msg)
            self.status.complete_task(False, error_msg)
            return {'error': error_msg}

    def _prepare_data(self, dataset: DatasetInfo) -> DataManager:
        """å‡†å¤‡æ•°æ®"""
        config = self.base_config.copy()
        data_manager = DataManager(config)
        data_manager.load_dataset(dataset.name, dataset.file_path).preprocess()
        return data_manager

    def _train_model(self, data_manager: DataManager, model_config: Dict[str, Any]) -> Tuple[Any, Dict[str, Any]]:
        """è®­ç»ƒæ¨¡å‹"""
        start_time = time.time()

        try:
            # è·å–æ•°æ®ç»´åº¦
            stats = data_manager.get_statistics()
            n_users = stats['n_users']
            n_items = stats['n_items']

            # åˆ›å»ºæŸå¤±å‡½æ•°
            loss_function = self._create_loss_function(model_config['loss_function'])

            # åˆ›å»ºæ­£åˆ™åŒ–å™¨
            regularizer = self._create_regularizer(model_config['regularizer'])

            # åˆ›å»ºåˆå§‹åŒ–å™¨
            initializer = self._create_initializer(model_config['initializer'])

            # åˆ›å»ºæ¨¡å‹
            model = MatrixFactorizationSGD(
                n_users=n_users,
                n_items=n_items,
                n_factors=model_config['n_factors'],
                learning_rate=model_config['learning_rate'],
                regularizer=regularizer,
                loss_function=loss_function,
                use_bias=model_config['use_bias'],
                global_mean=data_manager.global_mean or 0.0
            )

            # åˆå§‹åŒ–å‚æ•°
            model.initialize_parameters(initializer)

            # è·å–è®­ç»ƒæ•°æ®
            train_data, val_data, _ = data_manager.get_splits()

            # è®­ç»ƒæ¨¡å‹
            model.fit(
                train_data=train_data,
                val_data=val_data,
                n_epochs=model_config['n_epochs'],
                verbose=0  # å‡å°‘è¾“å‡º
            )

            training_time = time.time() - start_time
            training_info = {
                'training_time': training_time,
                'n_epochs': model_config['n_epochs'],
                'final_loss': getattr(model, 'train_history', {}).get('loss', [])[-1:],
                'final_val_loss': getattr(model, 'train_history', {}).get('val_loss', [])[-1:]
            }

            return model, training_info

        except Exception as e:
            raise Exception(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")

    def _create_loss_function(self, loss_config: Dict[str, Any]):
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        loss_type = loss_config['type']

        if loss_type == 'L2':
            return L2Loss()
        elif loss_type == 'L1':
            return L1Loss()
        elif loss_type == 'Huber':
            return HuberLoss(delta=loss_config.get('delta', 1.0))
        elif loss_type == 'HPL':
            return HybridPiecewiseLoss(
                delta1=loss_config.get('delta1', 0.5),
                delta2=loss_config.get('delta2', 2.0),
                l_max=loss_config.get('l_max', 3.0)
            )
        elif loss_type == 'Logcosh':
            return LogcoshLoss()
        elif loss_type == 'SigmoidLike':
            return SigmoidLikeLoss(
                alpha=loss_config.get('alpha', 1.0),
                l_max=loss_config.get('l_max', 3.0)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {loss_type}")

    def _create_regularizer(self, reg_config: Dict[str, Any]):
        """åˆ›å»ºæ­£åˆ™åŒ–å™¨"""
        reg_type = reg_config['type']

        if reg_type == 'L2':
            return L2Regularizer(lambda_reg=reg_config.get('lambda_reg', 0.01))
        elif reg_type == 'L1':
            return L1Regularizer(lambda_reg=reg_config.get('lambda_reg', 0.01))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ­£åˆ™åŒ–å™¨: {reg_type}")

    def _create_initializer(self, init_config: Dict[str, Any]):
        """åˆ›å»ºåˆå§‹åŒ–å™¨"""
        init_type = init_config['type']

        if init_type == 'Normal':
            return NormalInitializer(
                mean=init_config.get('mean', 0.0),
                std=init_config.get('std', 0.01)
            )
        elif init_type == 'Xavier':
            return XavierInitializer()
        elif init_type == 'Uniform':
            return UniformInitializer(
                low=init_config.get('low', -0.01),
                high=init_config.get('high', 0.01)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆå§‹åŒ–å™¨: {init_type}")

    def _create_noise_configs(self, exp_config: Dict[str, Any]) -> List[NoiseConfig]:
        """åˆ›å»ºå™ªå£°é…ç½®åˆ—è¡¨"""
        noise_configs = []

        for noise_type in exp_config['noise_types']:
            for strength in exp_config['noise_strengths']:
                for ratio in exp_config['noise_ratios']:
                    config = NoiseConfig(
                        noise_type=noise_type,
                        noise_strength=strength,
                        noise_ratio=ratio,
                        random_seed=exp_config.get('random_seed', 42)
                    )
                    noise_configs.append(config)

        return noise_configs

    def _build_model_configs_from_optimal(self, optimal_config: OptimalConfig) -> List[Dict[str, Any]]:
        """ä»æœ€ä¼˜é…ç½®æ„å»ºæ¨¡å‹é…ç½®ï¼ˆå…¼å®¹æ–¹æ³•ï¼‰"""
        base_config = optimal_config.get_best_config()

        # æå–åŸºç¡€å‚æ•°
        base_params = {
            'n_factors': base_config.get('latent_factors', 50),
            'learning_rate': base_config.get('learning_rate', 0.01),
            'n_epochs': 100,
            'use_bias': True,
            'regularizer': {
                'type': 'L2',
                'lambda_reg': base_config.get('lambda_reg', 0.01)
            },
            'initializer': {'type': 'Normal', 'mean': 0.0, 'std': 0.01}
        }

        # æ„å»ºä¸åŒæŸå¤±å‡½æ•°çš„é…ç½®
        model_configs = []

        # HPLä¼˜åŒ–æ¨¡å‹
        hpl_config = base_params.copy()
        hpl_config.update({
            'name': 'HPL_optimized',
            'description': 'ä¼˜åŒ–åçš„HPLæŸå¤±å‡½æ•°æ¨¡å‹',
            'loss_function': {
                'type': 'HPL',
                'delta1': base_config.get('delta1', 0.5),
                'delta2': base_config.get('delta2', 2.0),
                'l_max': base_config.get('l_max', 3.0)
            }
        })
        model_configs.append(hpl_config)

        # L2åŸºçº¿æ¨¡å‹
        l2_config = base_params.copy()
        l2_config.update({
            'name': 'L2_baseline',
            'description': 'L2æŸå¤±å‡½æ•°åŸºçº¿æ¨¡å‹',
            'loss_function': {'type': 'L2'}
        })
        model_configs.append(l2_config)

        return model_configs

    def _generate_experiment_summary(self, experiment_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå®éªŒæ‘˜è¦"""
        summary = {
            'experiment_info': {
                'total_datasets': len(experiment_results),
                'completion_time': datetime.now().isoformat(),
                'status': 'completed',
                'total_tasks': self.status.total_tasks,
                'successful_tasks': self.status.successful_tasks,
                'failed_tasks': self.status.failed_tasks
            },
            'dataset_summaries': {},
            'overall_findings': {}
        }

        # ä¸ºæ¯ä¸ªæ•°æ®é›†ç”Ÿæˆæ‘˜è¦
        for dataset_name, dataset_result in experiment_results.items():
            if 'error' not in dataset_result:
                summary['dataset_summaries'][dataset_name] = {
                    'dataset_info': dataset_result.get('dataset_info', {}),
                    'model_count': len(dataset_result.get('model_results', {})),
                    'status': 'completed'
                }
            else:
                summary['dataset_summaries'][dataset_name] = {
                    'status': 'failed',
                    'error': dataset_result['error']
                }

        return summary

    def _generate_partial_summary(self, experiment_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆéƒ¨åˆ†å®Œæˆçš„å®éªŒæ‘˜è¦"""
        summary = self._generate_experiment_summary(experiment_results)
        summary['experiment_info']['status'] = 'interrupted'
        summary['experiment_info']['completion_time'] = datetime.now().isoformat()
        return summary

    def _generate_error_summary(self, error_message: str) -> Dict[str, Any]:
        """ç”Ÿæˆé”™è¯¯æ‘˜è¦"""
        return {
            'experiment_info': {
                'status': 'failed',
                'error_time': datetime.now().isoformat(),
                'error_message': error_message,
                'successful_tasks': self.status.successful_tasks,
                'failed_tasks': self.status.failed_tasks
            }
        }


def main():
    """MovieLens 100K æ•°æ®é›†å®éªŒ"""
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    from dataset_selector import DatasetInfo
    from config_matcher import OptimalConfig

    # åˆ›å»º MovieLens 100K æ•°æ®é›†
    ml100k_dataset = DatasetInfo(
        name='movielens100k',
        file_path='dataset\small_20201202M100K_data_all_random_1percent.txt',
        display_name='MovieLens 100K Small'
    )

    # åˆ›å»ºä¼˜åŒ–é…ç½®
    ml100k_config_data = {
        'results': {
            'best_config': {
                'learning_rate': 0.01,
                'latent_factors': 20,
                'lambda_reg': 0.01,
                'delta1': 0.3,
                'delta2': 1.5,
                'l_max': 3.0
            },
            'best_score': 0.95
        }
    }
    ml100k_config = OptimalConfig('movielens100k', ml100k_config_data)

    # è¿è¡Œ MovieLens 100K å®éªŒ
    executor = ExperimentExecutor()
    results = executor.execute_robustness_experiment(
        datasets=[ml100k_dataset],
        configs=[ml100k_config],
        save_dir=None  # ä½¿ç”¨é»˜è®¤è·¯å¾„
    )

    print("MovieLens 100K å®éªŒå®Œæˆï¼ç»“æœæ‘˜è¦:")
    print(results)


if __name__ == "__main__":
    main()























