#!/usr/bin/env python3
"""
ç»“æœç®¡ç†å™¨æ¨¡å—
è´Ÿè´£å®éªŒç»“æœçš„å­˜å‚¨ã€ç»„ç»‡å’Œç®¡ç†
"""

import json
import os
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging
import hashlib
import numpy as np

logger = logging.getLogger(__name__)


class DateTimeEncoder(json.JSONEncoder):
    """è‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œç”¨äºå¤„ç†datetimeå¯¹è±¡å’Œæ•°å€¼ç±»å‹"""
    
    def default(self, obj):
        # å¤„ç†datetimeå¯¹è±¡
        if isinstance(obj, datetime):
            return obj.isoformat()
        
        # å¤„ç†NumPyæ•°å€¼ç±»å‹
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        
        # å¤„ç†å¤æ•°ç±»å‹
        elif isinstance(obj, complex):
            return {'real': obj.real, 'imag': obj.imag}
        
        # å¤„ç†setç±»å‹
        elif isinstance(obj, set):
            return list(obj)
        
        # å¤„ç†bytesç±»å‹
        elif isinstance(obj, bytes):
            return obj.decode('utf-8', errors='ignore')
            
        return super().default(obj)


class ResultManager:
    """ç»“æœç®¡ç†å™¨"""

    def __init__(self, base_dir: str):
        """
        åˆå§‹åŒ–ç»“æœç®¡ç†å™¨

        Args:
            base_dir: åŸºç¡€ç›®å½•
        """
        # ç¡®ä¿base_diræ˜¯Pathå¯¹è±¡
        self.base_dir = Path(base_dir)
        self.structure = {
            'experiments': 'experiments',      # å…·ä½“å®éªŒç»“æœ
            'summaries': 'summaries',         # å®éªŒæ‘˜è¦
            'metadata': 'metadata',           # å…ƒæ•°æ®
            'reports': 'reports',             # æŠ¥å‘Šæ–‡ä»¶
            'visualizations': 'visualizations', # å¯è§†åŒ–æ–‡ä»¶
            'backups': 'backups'              # å¤‡ä»½æ–‡ä»¶
        }
        self._create_directory_structure()

    def _create_directory_structure(self):
        """åˆ›å»ºç›®å½•ç»“æ„"""
        for dir_name in self.structure.values():
            (self.base_dir / dir_name).mkdir(exist_ok=True)

    def save_experiment_metadata(self, metadata: Dict[str, Any], dataset_name: str = None) -> str:
        """
        ä¿å­˜å®éªŒå…ƒæ•°æ®

        Args:
            metadata: å…ƒæ•°æ®å­—å…¸
            dataset_name: æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼‰

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # å¦‚æœæä¾›äº†æ•°æ®é›†åç§°ï¼Œå°†å…¶æ·»åŠ åˆ°æ–‡ä»¶åä¸­
        if dataset_name:
            filename = f"experiment_metadata_{dataset_name}_{timestamp}.json"
        else:
            filename = f"experiment_metadata_{timestamp}.json"

        # ç¡®ä¿ä½¿ç”¨Pathå¯¹è±¡è¿›è¡Œè·¯å¾„æ“ä½œ
        file_path = self.base_dir / self.structure['metadata'] / filename

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        file_path.parent.mkdir(parents=True, exist_ok=True)

        # æ·»åŠ ç³»ç»Ÿä¿¡æ¯
        enhanced_metadata = metadata.copy()
        enhanced_metadata.update({
            'save_time': datetime.now().isoformat(),
            'file_path': str(file_path),
            'version': '1.0',
            'dataset_name': dataset_name
        })

        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                # ä½¿ç”¨è‡ªå®šä¹‰ç¼–ç å™¨å¤„ç†datetimeå¯¹è±¡
                json.dump(enhanced_metadata, f, indent=2, ensure_ascii=False, cls=DateTimeEncoder)

            logger.info(f"å…ƒæ•°æ®å·²ä¿å­˜: {file_path}")
            return str(file_path)

        except Exception as e:
            logger.error(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
            raise

    def save_experiment_results(self,
                               results: Dict[str, Any],
                               experiment_name: str,
                               dataset_name: str = None,
                               experiment_type: str = "robustness") -> str:
        """
        ä¿å­˜å®éªŒç»“æœ

        Args:
            results: å®éªŒç»“æœ
            experiment_name: å®éªŒåç§°
            dataset_name: æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼‰
            experiment_type: å®éªŒç±»å‹

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # åˆ›å»ºå®éªŒç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # å¦‚æœæä¾›äº†æ•°æ®é›†åç§°ï¼Œå°†å…¶æ·»åŠ åˆ°ç›®å½•åä¸­
        if dataset_name:
            exp_dir_name = f"{experiment_type}_{dataset_name}_{experiment_name}_{timestamp}"
        else:
            exp_dir_name = f"{experiment_type}_{experiment_name}_{timestamp}"

        # ç¡®ä¿ä½¿ç”¨Pathå¯¹è±¡è¿›è¡Œè·¯å¾„æ“ä½œ
        exp_dir = self.base_dir / self.structure['experiments'] / exp_dir_name
        exp_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ä¸»è¦ç»“æœ
        main_result_file = exp_dir / "results.json"
        enhanced_results = results.copy()
        enhanced_results.update({
            'experiment_name': experiment_name,
            'experiment_type': experiment_type,
            'dataset_name': dataset_name,
            'save_time': datetime.now().isoformat(),
            'result_hash': self._calculate_result_hash(results)
        })

        try:
            with open(main_result_file, 'w', encoding='utf-8') as f:
                # ä½¿ç”¨è‡ªå®šä¹‰ç¼–ç å™¨å¤„ç†datetimeå¯¹è±¡
                json.dump(enhanced_results, f, indent=2, ensure_ascii=False, cls=DateTimeEncoder)

            # ä¿å­˜ç®€åŒ–ç‰ˆæœ¬ï¼ˆåªåŒ…å«å…³é”®ä¿¡æ¯ï¼‰
            summary_file = exp_dir / "summary.json"
            summary = self._extract_result_summary(enhanced_results)
            with open(summary_file, 'w', encoding='utf-8') as f:
                # ä½¿ç”¨è‡ªå®šä¹‰ç¼–ç å™¨å¤„ç†datetimeå¯¹è±¡
                json.dump(summary, f, indent=2, ensure_ascii=False, cls=DateTimeEncoder)

            logger.info(f"å®éªŒç»“æœå·²ä¿å­˜: {exp_dir}")
            return str(exp_dir)

        except Exception as e:
            logger.error(f"ä¿å­˜å®éªŒç»“æœå¤±è´¥: {e}")
            raise

    def save_experiment_summary(self, summary: Dict[str, Any], dataset_name: str = None) -> str:
        """
        ä¿å­˜å®éªŒæ‘˜è¦

        Args:
            summary: å®éªŒæ‘˜è¦
            dataset_name: æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼‰

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # å¦‚æœæä¾›äº†æ•°æ®é›†åç§°ï¼Œå°†å…¶æ·»åŠ åˆ°æ–‡ä»¶åä¸­
        if dataset_name:
            filename = f"experiment_summary_{dataset_name}_{timestamp}.json"
        else:
            filename = f"experiment_summary_{timestamp}.json"

        # ç¡®ä¿ä½¿ç”¨Pathå¯¹è±¡è¿›è¡Œè·¯å¾„æ“ä½œ
        file_path = self.base_dir / self.structure['summaries'] / filename

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        file_path.parent.mkdir(parents=True, exist_ok=True)

        # å¢å¼ºæ‘˜è¦ä¿¡æ¯
        enhanced_summary = summary.copy()
        enhanced_summary.update({
            'summary_generated_time': datetime.now().isoformat(),
            'summary_version': '1.0',
            'dataset_name': dataset_name
        })

        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                # ä½¿ç”¨è‡ªå®šä¹‰ç¼–ç å™¨å¤„ç†datetimeå¯¹è±¡
                json.dump(enhanced_summary, f, indent=2, ensure_ascii=False, cls=DateTimeEncoder)

            logger.info(f"å®éªŒæ‘˜è¦å·²ä¿å­˜: {file_path}")
            return str(file_path)

        except Exception as e:
            logger.error(f"ä¿å­˜å®éªŒæ‘˜è¦å¤±è´¥: {e}")
            raise

    def save_robustness_report(self,
                              report_content: str,
                              experiment_name: str,
                              report_type: str = "text",
                              dataset_name: str = None) -> str:
        """
        ä¿å­˜é²æ£’æ€§æŠ¥å‘Š

        Args:
            report_content: æŠ¥å‘Šå†…å®¹
            experiment_name: å®éªŒåç§°
            report_type: æŠ¥å‘Šç±»å‹ (text, html, markdown)
            dataset_name: æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼‰

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # å¦‚æœæä¾›äº†æ•°æ®é›†åç§°ï¼Œå°†å…¶æ·»åŠ åˆ°æ–‡ä»¶åä¸­
        if dataset_name:
            filename = f"{dataset_name}_{experiment_name}_report_{timestamp}.{report_type}"
        else:
            filename = f"{experiment_name}_report_{timestamp}.{report_type}"

        # ç¡®ä¿ä½¿ç”¨Pathå¯¹è±¡è¿›è¡Œè·¯å¾„æ“ä½œ
        file_path = self.base_dir / self.structure['reports'] / filename

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        file_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(report_content)

            logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {file_path}")
            return str(file_path)

        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            raise

    def save_visualization(self,
                          figure_object: Any,
                          experiment_name: str,
                          plot_type: str,
                          format: str = "png",
                          dataset_name: str = None) -> str:
        """
        ä¿å­˜å¯è§†åŒ–å›¾è¡¨

        Args:
            figure_object: matplotlib figureå¯¹è±¡
            experiment_name: å®éªŒåç§°
            plot_type: å›¾è¡¨ç±»å‹
            format: ä¿å­˜æ ¼å¼ (png, pdf, svg)
            dataset_name: æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼‰

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # å¦‚æœæä¾›äº†æ•°æ®é›†åç§°ï¼Œå°†å…¶æ·»åŠ åˆ°æ–‡ä»¶åä¸­
        if dataset_name:
            filename = f"{dataset_name}_{experiment_name}_{plot_type}_{timestamp}.{format}"
        else:
            filename = f"{experiment_name}_{plot_type}_{timestamp}.{format}"

        # ç¡®ä¿ä½¿ç”¨Pathå¯¹è±¡è¿›è¡Œè·¯å¾„æ“ä½œ
        file_path = self.base_dir / self.structure['visualizations'] / filename

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        file_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            figure_object.savefig(file_path, dpi=300, bbox_inches='tight')
            logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {file_path}")
            return str(file_path)

        except Exception as e:
            logger.error(f"ä¿å­˜å¯è§†åŒ–å›¾è¡¨å¤±è´¥: {e}")
            raise

    def load_experiment_results(self, experiment_path: str) -> Dict[str, Any]:
        """
        åŠ è½½å®éªŒç»“æœ

        Args:
            experiment_path: å®éªŒè·¯å¾„

        Returns:
            å®éªŒç»“æœå­—å…¸
        """
        exp_path = Path(experiment_path)

        if not exp_path.exists():
            raise FileNotFoundError(f"å®éªŒè·¯å¾„ä¸å­˜åœ¨: {experiment_path}")

        # å°è¯•åŠ è½½ä¸»è¦ç»“æœæ–‡ä»¶
        result_file = exp_path / "results.json"
        if result_file.exists():
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                logger.info(f"åŠ è½½å®éªŒç»“æœ: {result_file}")
                return results
            except Exception as e:
                logger.error(f"åŠ è½½å®éªŒç»“æœå¤±è´¥: {e}")
                raise

        # å¦‚æœä¸»è¦ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•åŠ è½½æ‘˜è¦
        summary_file = exp_path / "summary.json"
        if summary_file.exists():
            try:
                with open(summary_file, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                logger.warning(f"åªæ‰¾åˆ°æ‘˜è¦æ–‡ä»¶: {summary_file}")
                return results
            except Exception as e:
                logger.error(f"åŠ è½½æ‘˜è¦æ–‡ä»¶å¤±è´¥: {e}")
                raise

        raise FileNotFoundError(f"åœ¨ {experiment_path} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç»“æœæ–‡ä»¶")

    def list_experiments(self, experiment_type: str = None) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ‰€æœ‰å®éªŒ

        Args:
            experiment_type: å®éªŒç±»å‹è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰

        Returns:
            å®éªŒä¿¡æ¯åˆ—è¡¨
        """
        experiments = []
        exp_dir = self.base_dir / self.structure['experiments']

        if not exp_dir.exists():
            return experiments

        for exp_folder in exp_dir.iterdir():
            if exp_folder.is_dir():
                try:
                    # å°è¯•åŠ è½½å®éªŒä¿¡æ¯
                    summary_file = exp_folder / "summary.json"
                    result_file = exp_folder / "results.json"

                    exp_info = {
                        'name': exp_folder.name,
                        'path': str(exp_folder),
                        'created_time': datetime.fromtimestamp(exp_folder.stat().st_ctime).isoformat()
                    }

                    # å°è¯•ä»æ‘˜è¦æ–‡ä»¶è·å–ä¿¡æ¯
                    if summary_file.exists():
                        with open(summary_file, 'r', encoding='utf-8') as f:
                            summary = json.load(f)
                        exp_info.update({
                            'experiment_type': summary.get('experiment_type', 'unknown'),
                            'experiment_name': summary.get('experiment_name', 'unknown'),
                            'status': summary.get('status', 'unknown')
                        })
                    elif result_file.exists():
                        with open(result_file, 'r', encoding='utf-8') as f:
                            results = json.load(f)
                        exp_info.update({
                            'experiment_type': results.get('experiment_type', 'unknown'),
                            'experiment_name': results.get('experiment_name', 'unknown'),
                            'status': 'completed'
                        })

                    # åº”ç”¨ç±»å‹è¿‡æ»¤
                    if experiment_type is None or exp_info.get('experiment_type') == experiment_type:
                        experiments.append(exp_info)

                except Exception as e:
                    logger.warning(f"è¯»å–å®éªŒä¿¡æ¯å¤±è´¥ {exp_folder}: {e}")
                    continue

        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        experiments.sort(key=lambda x: x['created_time'], reverse=True)
        return experiments

    def backup_results(self, backup_name: str = None) -> str:
        """
        å¤‡ä»½å®éªŒç»“æœ

        Args:
            backup_name: å¤‡ä»½åç§°ï¼ˆå¯é€‰ï¼‰

        Returns:
            å¤‡ä»½æ–‡ä»¶è·¯å¾„
        """
        if backup_name is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_name = f"results_backup_{timestamp}"

        backup_dir = self.base_dir / self.structure['backups']
        backup_dir.mkdir(exist_ok=True)

        backup_path = backup_dir / f"{backup_name}.zip"

        try:
            # åˆ›å»ºzipå¤‡ä»½
            import zipfile
            with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(self.base_dir):
                    # è·³è¿‡å¤‡ä»½ç›®å½•æœ¬èº«
                    if 'backups' in root:
                        continue

                    for file in files:
                        file_path = os.path.join(root, file)
                        arc_name = os.path.relpath(file_path, self.base_dir)
                        zipf.write(file_path, arc_name)

            logger.info(f"ç»“æœå·²å¤‡ä»½åˆ°: {backup_path}")
            return str(backup_path)

        except Exception as e:
            logger.error(f"å¤‡ä»½å¤±è´¥: {e}")
            raise

    def cleanup_old_results(self, days_old: int = 30, keep_count: int = 10):
        """
        æ¸…ç†æ—§çš„å®éªŒç»“æœ

        Args:
            days_old: ä¿ç•™å¤©æ•°
            keep_count: æœ€å°‘ä¿ç•™çš„å®éªŒæ•°é‡
        """
        from datetime import timedelta

        cutoff_date = datetime.now() - timedelta(days=days_old)
        experiments = self.list_experiments()

        # æŒ‰æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„keep_countä¸ª
        experiments.sort(key=lambda x: x['created_time'], reverse=True)

        to_delete = []
        for i, exp in enumerate(experiments):
            exp_time = datetime.fromisoformat(exp['created_time'])
            if i >= keep_count and exp_time < cutoff_date:
                to_delete.append(exp)

        # æ‰§è¡Œåˆ é™¤
        deleted_count = 0
        for exp in to_delete:
            try:
                exp_path = Path(exp['path'])
                if exp_path.exists():
                    shutil.rmtree(exp_path)
                    deleted_count += 1
                    logger.info(f"åˆ é™¤æ—§å®éªŒ: {exp['name']}")
            except Exception as e:
                logger.warning(f"åˆ é™¤å®éªŒå¤±è´¥ {exp['name']}: {e}")

        logger.info(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ—§å®éªŒ")

    def get_experiment_statistics(self) -> Dict[str, Any]:
        """
        è·å–å®éªŒç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        experiments = self.list_experiments()

        stats = {
            'total_experiments': len(experiments),
            'experiment_types': {},
            'storage_info': {},
            'recent_activity': []
        }

        # ç»Ÿè®¡å®éªŒç±»å‹
        for exp in experiments:
            exp_type = exp.get('experiment_type', 'unknown')
            if exp_type not in stats['experiment_types']:
                stats['experiment_types'][exp_type] = 0
            stats['experiment_types'][exp_type] += 1

        # è®¡ç®—å­˜å‚¨ä¿¡æ¯
        total_size = 0
        for root, dirs, files in os.walk(self.base_dir):
            for file in files:
                file_path = os.path.join(root, file)
                try:
                    total_size += os.path.getsize(file_path)
                except:
                    pass

        stats['storage_info'] = {
            'total_size_mb': round(total_size / (1024 * 1024), 2),
            'total_size_gb': round(total_size / (1024 * 1024 * 1024), 3)
        }

        # æœ€è¿‘æ´»åŠ¨ï¼ˆæœ€æ–°5ä¸ªå®éªŒï¼‰
        stats['recent_activity'] = experiments[:5]

        return stats

    def _calculate_result_hash(self, results: Dict[str, Any]) -> str:
        """è®¡ç®—ç»“æœå“ˆå¸Œå€¼"""
        try:
            result_str = json.dumps(results, sort_keys=True, default=str)
            return hashlib.md5(result_str.encode()).hexdigest()
        except:
            return "hash_calculation_failed"

    def _extract_result_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–ç»“æœæ‘˜è¦"""
        summary = {
            'experiment_name': results.get('experiment_name'),
            'experiment_type': results.get('experiment_type'),
            'save_time': results.get('save_time'),
            'result_hash': results.get('result_hash')
        }

        # æå–å…³é”®ç»Ÿè®¡ä¿¡æ¯
        if 'experiment_info' in results:
            exp_info = results['experiment_info']
            summary['experiment_stats'] = {
                'total_datasets': exp_info.get('total_datasets'),
                'total_tasks': exp_info.get('total_tasks'),
                'successful_tasks': exp_info.get('successful_tasks'),
                'failed_tasks': exp_info.get('failed_tasks'),
                'status': exp_info.get('status')
            }

        # æå–æ•°æ®é›†æ‘˜è¦
        if 'dataset_summaries' in results:
            summary['datasets'] = list(results['dataset_summaries'].keys())

        return summary

    def export_results(self,
                      experiment_names: List[str],
                      export_format: str = "json",
                      output_file: str = None) -> str:
        """
        å¯¼å‡ºå®éªŒç»“æœ

        Args:
            experiment_names: è¦å¯¼å‡ºçš„å®éªŒåç§°åˆ—è¡¨
            export_format: å¯¼å‡ºæ ¼å¼ (json, csv, excel)
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            å¯¼å‡ºæ–‡ä»¶è·¯å¾„
        """
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"exported_results_{timestamp}.{export_format}"

        output_path = self.base_dir / output_file

        # æ”¶é›†å®éªŒæ•°æ®
        export_data = {}
        for exp_name in experiment_names:
            # æŸ¥æ‰¾åŒ¹é…çš„å®éªŒ
            experiments = self.list_experiments()
            for exp in experiments:
                if exp_name in exp['name'] or exp_name == exp['experiment_name']:
                    try:
                        results = self.load_experiment_results(exp['path'])
                        export_data[exp['name']] = results
                        break
                    except Exception as e:
                        logger.warning(f"åŠ è½½å®éªŒæ•°æ®å¤±è´¥ {exp_name}: {e}")

        # æ ¹æ®æ ¼å¼å¯¼å‡º
        try:
            if export_format.lower() == 'json':
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)

            elif export_format.lower() == 'csv':
                import pandas as pd
                # å°†åµŒå¥—æ•°æ®å±•å¹³ä¸ºCSVæ ¼å¼
                flattened_data = self._flatten_for_csv(export_data)
                df = pd.DataFrame(flattened_data)
                df.to_csv(output_path, index=False)

            elif export_format.lower() == 'excel':
                import pandas as pd
                # åˆ›å»ºå¤šä¸ªå·¥ä½œè¡¨çš„Excelæ–‡ä»¶
                with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                    for exp_name, exp_data in export_data.items():
                        flattened_data = self._flatten_for_csv({exp_name: exp_data})
                        df = pd.DataFrame(flattened_data)
                        sheet_name = exp_name[:31]  # Excelå·¥ä½œè¡¨åç§°é™åˆ¶
                        df.to_excel(writer, sheet_name=sheet_name, index=False)

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {export_format}")

            logger.info(f"ç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")
            return str(output_path)

        except Exception as e:
            logger.error(f"å¯¼å‡ºå¤±è´¥: {e}")
            raise

    def _flatten_for_csv(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å°†åµŒå¥—æ•°æ®å±•å¹³ä¸ºCSVæ ¼å¼"""
        flattened = []

        def flatten_dict(d, parent_key='', sep='_'):
            items = []
            for k, v in d.items():
                new_key = f"{parent_key}{sep}{k}" if parent_key else k
                if isinstance(v, dict):
                    items.extend(flatten_dict(v, new_key, sep=sep).items())
                elif isinstance(v, list):
                    for i, item in enumerate(v):
                        if isinstance(item, dict):
                            items.extend(flatten_dict(item, f"{new_key}{sep}{i}", sep=sep).items())
                        else:
                            items.append((f"{new_key}{sep}{i}", item))
                else:
                    items.append((new_key, v))
            return dict(items)

        for exp_name, exp_data in data.items():
            flat_data = flatten_dict(exp_data)
            flat_data['experiment_name'] = exp_name
            flattened.append(flat_data)

        return flattened

    def print_storage_summary(self):
        """æ‰“å°å­˜å‚¨æ‘˜è¦"""
        stats = self.get_experiment_statistics()

        print("\n" + "="*80)
        print("ğŸ“ å®éªŒç»“æœå­˜å‚¨æ‘˜è¦")
        print("="*80)
        print(f"å­˜å‚¨ä½ç½®: {self.base_dir}")
        print(f"æ€»å®éªŒæ•°: {stats['total_experiments']}")
        print(f"å­˜å‚¨å¤§å°: {stats['storage_info']['total_size_mb']} MB "
              f"({stats['storage_info']['total_size_gb']} GB)")

        print(f"\nå®éªŒç±»å‹åˆ†å¸ƒ:")
        for exp_type, count in stats['experiment_types'].items():
            print(f"  - {exp_type}: {count} ä¸ªå®éªŒ")

        if stats['recent_activity']:
            print(f"\næœ€è¿‘å®éªŒ:")
            for exp in stats['recent_activity']:
                print(f"  - {exp['name']} ({exp['created_time'][:10]})")

        print("="*80)


def main():
    """æµ‹è¯•å‡½æ•°"""
    # åˆ›å»ºç»“æœç®¡ç†å™¨
    manager = ResultManager("test_results")

    # æµ‹è¯•ä¿å­˜å…ƒæ•°æ®
    metadata = {
        'experiment_name': 'test_robustness',
        'datasets': ['ml100k', 'netflix'],
        'start_time': datetime.now().isoformat()
    }
    manager.save_experiment_metadata(metadata)

    # æµ‹è¯•ä¿å­˜å®éªŒç»“æœ
    results = {
        'dataset_results': {
            'ml100k': {'mae': 0.85, 'rmse': 1.02},
            'netflix': {'mae': 0.92, 'rmse': 1.15}
        },
        'summary': 'Test experiment completed successfully'
    }
    manager.save_experiment_results(results, 'test_experiment', 'robustness')

    # æ‰“å°å­˜å‚¨æ‘˜è¦
    manager.print_storage_summary()

    # åˆ—å‡ºæ‰€æœ‰å®éªŒ
    experiments = manager.list_experiments()
    print(f"\nå‘ç° {len(experiments)} ä¸ªå®éªŒ:")
    for exp in experiments:
        print(f"  - {exp['name']}")


if __name__ == "__main__":
    main()















