#!/usr/bin/env python3
"""
HPLæŸå¤±å‡½æ•°ä¸“ç”¨ä¼˜åŒ–å®éªŒ

ä¸“é—¨é’ˆå¯¹æ··åˆåˆ†æ®µæŸå¤±å‡½æ•°(HPL)çš„è¶…å‚æ•°ä¼˜åŒ–
ä¿®æ­£äº†æ•°æ®åˆ†å‰²é—®é¢˜ï¼Œå¢åŠ äº†HPLä¸“ç”¨çš„å‚æ•°ç©ºé—´å’Œä¼˜åŒ–ç­–ç•¥
"""

import sys
import os
import numpy as np
import time
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# å¯¼å…¥æ¨¡å—
try:
    from src.hyperopt.space import ParameterSpace
    from src.hyperopt.samplers import LatinHypercubeSampler
    from src.hyperopt.constraints import ConstraintManager
    from src.hyperopt.optimizer import HyperOptimizer
    from src.hyperopt.tracker import ExperimentTracker
    print("æˆåŠŸå¯¼å…¥hyperoptæ¨¡å—")
except ImportError as e:
    print(f"å¯¼å…¥hyperoptæ¨¡å—å¤±è´¥: {e}")

try:
    from data.data_manager import DataManager
    from src.models.mf_sgd import MatrixFactorizationSGD
    from src.models.initializers import NormalInitializer
    from src.models.regularizers import L2Regularizer
    from src.losses.hpl import HybridPiecewiseLoss
    from src.losses.standard import L2Loss
    print("æˆåŠŸå¯¼å…¥æ•°æ®å’Œæ¨¡å‹æ¨¡å—")
except ImportError as e:
    print(f"å¯¼å…¥æ•°æ®/æ¨¡å‹æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

try:
    from example_hyperopt import (
        SimpleParameterSpace, SimpleLatinHypercubeSampler,
        SimpleConstraintManager, SimpleHyperOptimizer, SimpleExperimentTracker
    )
    print("ç®€åŒ–ç‰ˆæœ¬ç±»å·²å‡†å¤‡å°±ç»ª")
except ImportError:
    print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥ç®€åŒ–ç‰ˆæœ¬ç±»")


class HPLFocusedObjectiveFunction:
    """ä¸“é—¨é’ˆå¯¹HPLçš„ç›®æ ‡å‡½æ•°"""

    def __init__(self, data_manager):
        self.data_manager = data_manager
        self.n_users = data_manager.get_statistics()['n_users']
        self.n_items = data_manager.get_statistics()['n_items']
        self.train_data, self.val_data, self.test_data = data_manager.get_splits()

        print(f"HPLç›®æ ‡å‡½æ•°åˆå§‹åŒ–:")
        print(f"  è®­ç»ƒé›†: {len(self.train_data)} æ¡")
        print(f"  éªŒè¯é›†: {len(self.val_data)} æ¡")
        print(f"  æµ‹è¯•é›†: {len(self.test_data)} æ¡")

    def __call__(self, config):
        """HPLä¸“ç”¨ç›®æ ‡å‡½æ•°ï¼šå¼ºåˆ¶ä½¿ç”¨HPLæŸå¤±å‡½æ•°"""
        try:
            # å¼ºåˆ¶ä½¿ç”¨HPLæŸå¤±å‡½æ•°
            loss_function = HybridPiecewiseLoss(
                delta1=config['delta1'],
                delta2=config['delta2'],
                l_max=config.get('l_max', 4.0),
                c_sigmoid=config.get('c_sigmoid', 1.0)
            )

            regularizer = L2Regularizer(lambda_reg=config['lambda_reg'])

            model = MatrixFactorizationSGD(
                n_users=self.n_users,
                n_items=self.n_items,
                n_factors=config['latent_factors'],
                learning_rate=config['learning_rate'],
                regularizer=regularizer,
                loss_function=loss_function,
                use_bias=True,
                global_mean=self.data_manager.global_mean or 0.0
            )

            initializer = NormalInitializer(mean=0.0, std=0.01)
            model.initialize_parameters(initializer)

            # å¢åŠ è®­ç»ƒæ—¶é—´ï¼ŒHPLéœ€è¦æ›´å¤šæ—¶é—´æ”¶æ•›
            model.fit(
                train_data=self.train_data,
                val_data=self.val_data,
                n_epochs=50,        # å¢åŠ åˆ°50è½®
                verbose=0,          # ä¸æ‰“å°è¯¦ç»†è®­ç»ƒè¿‡ç¨‹
                early_stopping_patience=15  # å¢åŠ è€å¿ƒ
            )

            # éªŒè¯é›†è¯„ä¼°
            val_predictions = model.predict(
                self.val_data[:, 0].astype(int),
                self.val_data[:, 1].astype(int)
            )

            # è¿˜åŸåˆ°åŸå§‹å°ºåº¦
            if self.data_manager.global_mean is not None:
                val_predictions += self.data_manager.global_mean
                val_targets = self.val_data[:, 2] + self.data_manager.global_mean
            else:
                val_targets = self.val_data[:, 2]

            # è®¡ç®—RMSE
            rmse = np.sqrt(np.mean((val_predictions - val_targets) ** 2))

            # ä¿®æ”¹æ‰“å°æ ¼å¼ï¼ŒåŒ…å«æ‰€æœ‰é‡è¦å‚æ•°
            print(f"HPLé…ç½®: å­¦ä¹ ç‡={config['learning_rate']:.4f}, "
                  f"å› å­æ•°={config['latent_factors']}, "
                  f"æ­£åˆ™åŒ–={config['lambda_reg']:.6f}, "
                  f"Î´1={config['delta1']:.3f}, Î´2={config['delta2']:.3f}, "
                  f"l_max={config.get('l_max', 4.0):.2f}, c_sig={config.get('c_sigmoid', 1.0):.2f}, "
                  f"RMSE={rmse:.4f}")

            return rmse

        except Exception as e:
            print(f"HPLé…ç½®è¯„ä¼°å¤±è´¥ {config}: {e}")
            return 10.0  # è¿”å›è¾ƒå¤§å€¼è¡¨ç¤ºå¤±è´¥


class L2FocusedObjectiveFunction:
    """ä¸“é—¨é’ˆå¯¹L2çš„ç›®æ ‡å‡½æ•°"""

    def __init__(self, data_manager):
        self.data_manager = data_manager
        self.n_users = data_manager.get_statistics()['n_users']
        self.n_items = data_manager.get_statistics()['n_items']
        self.train_data, self.val_data, self.test_data = data_manager.get_splits()

    def __call__(self, config):
        try:
            # å¼ºåˆ¶ä½¿ç”¨L2æŸå¤±å‡½æ•°
            loss_function = L2Loss()
            regularizer = L2Regularizer(lambda_reg=config['lambda_reg'])

            model = MatrixFactorizationSGD(
                n_users=self.n_users,
                n_items=self.n_items,
                n_factors=config['latent_factors'],
                learning_rate=config['learning_rate'],
                regularizer=regularizer,
                loss_function=loss_function,
                use_bias=True,
                global_mean=self.data_manager.global_mean or 0.0
            )

            initializer = NormalInitializer(mean=0.0, std=0.01)
            model.initialize_parameters(initializer)

            # ç›¸åŒçš„è®­ç»ƒè®¾ç½®
            model.fit(
                train_data=self.train_data,
                val_data=self.val_data,
                n_epochs=50,        # ä¸HPLç›¸åŒ
                verbose=0,
                early_stopping_patience=15  # ä¸HPLç›¸åŒ
            )

            # éªŒè¯é›†è¯„ä¼°
            val_predictions = model.predict(
                self.val_data[:, 0].astype(int),
                self.val_data[:, 1].astype(int)
            )

            # è¿˜åŸåˆ°åŸå§‹å°ºåº¦
            if self.data_manager.global_mean is not None:
                val_predictions += self.data_manager.global_mean
                val_targets = self.val_data[:, 2] + self.data_manager.global_mean
            else:
                val_targets = self.val_data[:, 2]

            # è®¡ç®—RMSE
            rmse = np.sqrt(np.mean((val_predictions - val_targets) ** 2))

            # ä¿®æ”¹æ‰“å°æ ¼å¼ï¼ŒåŒ…å«æ‰€æœ‰é‡è¦å‚æ•°
            print(f"L2é…ç½®: å­¦ä¹ ç‡={config['learning_rate']:.4f}, "
                  f"å› å­æ•°={config['latent_factors']}, "
                  f"æ­£åˆ™åŒ–={config['lambda_reg']:.6f}, "
                  f"RMSE={rmse:.4f}")

            return rmse

        except Exception as e:
            print(f"L2é…ç½®è¯„ä¼°å¤±è´¥ {config}: {e}")
            return 10.0  # è¿”å›è¾ƒå¤§å€¼è¡¨ç¤ºå¤±è´¥


def setup_improved_data():
    """æ”¹è¿›çš„æ•°æ®è®¾ç½®ï¼Œä¿®æ­£åˆ†å‰²æ¯”ä¾‹é—®é¢˜"""
    print("å‡†å¤‡æ”¹è¿›çš„æ•°æ®...")

    # ä¿®æ­£æ•°æ®é…ç½®
    data_config = {
        'random_seed': 42,
        'train_ratio': 0.8,          # ç¡®ä¿80%è®­ç»ƒæ•°æ®
        'val_ratio': 0.1,            # 10%éªŒè¯æ•°æ®
        'test_ratio': 0.1,           # 10%æµ‹è¯•æ•°æ®
        'batch_size': 128,
        'shuffle': True,
        'center_data': True,         # æ•°æ®ä¸­å¿ƒåŒ–æœ‰åŠ©äºHPL
        'ensure_user_in_train': True
    }

    data_manager = DataManager(data_config)
    data_manager.load_dataset('movielens100k', 'dataset/20201202M100K_data_all_random.txt')
    data_manager.preprocess()

    # éªŒè¯æ•°æ®åˆ†å‰²
    train_data, val_data, test_data = data_manager.get_splits()
    print(f"ä¿®æ­£åæ•°æ®åˆ†å‰²:")
    print(f"  è®­ç»ƒé›†: {len(train_data)} æ¡ ({len(train_data)/100000*100:.1f}%)")
    print(f"  éªŒè¯é›†: {len(val_data)} æ¡ ({len(val_data)/100000*100:.1f}%)")
    print(f"  æµ‹è¯•é›†: {len(test_data)} æ¡ ({len(test_data)/100000*100:.1f}%)")

    data_manager.print_summary()
    return data_manager


def create_hpl_parameter_space():
    """åˆ›å»ºHPLä¸“ç”¨å‚æ•°ç©ºé—´"""
    print("åˆ›å»ºHPLä¸“ç”¨å‚æ•°ç©ºé—´...")

    try:
        space = ParameterSpace()
    except:
        space = SimpleParameterSpace()

    # åŸºç¡€æ¨¡å‹å‚æ•° - åŸºäºä¹‹å‰å®éªŒç»“æœä¼˜åŒ–èŒƒå›´
    space.add_continuous('learning_rate', 0.01, 0.08, scale='log')  # ç¼©å°åˆ°æœ‰æ•ˆèŒƒå›´
    space.add_discrete('latent_factors', 15, 75, step=5)            # èšç„¦æœ‰æ•ˆåŒºé—´
    space.add_continuous('lambda_reg', 0.001, 0.02, scale='log')    # ä¸“æ³¨å¼±æ­£åˆ™åŒ–

    # HPLä¸“ç”¨å‚æ•° - æ‰©å±•æœç´¢èŒƒå›´
    space.add_continuous('delta1', 0.05, 1.5)      # æ‰©å±•ä¸‹ç•Œï¼Œå…è®¸æ›´å°çš„delta1
    space.add_continuous('delta2', 0.8, 4.0)       # æ‰©å±•ä¸Šç•Œï¼Œå…è®¸æ›´å¤§çš„delta2
    space.add_continuous('l_max', 2.5, 6.0)        # æŸå¤±å‡½æ•°ä¸Šé™
    space.add_continuous('c_sigmoid', 0.3, 3.0)    # Sigmoidå‡½æ•°é™¡å³­åº¦

    print(f"HPLå‚æ•°ç©ºé—´ç»´åº¦: {space.get_dimension()}")
    print("HPLå‚æ•°ä¿¡æ¯:")
    for name, info in space.get_parameter_info().items():
        print(f"  {name}: {info}")

    return space


def create_l2_parameter_space():
    """åˆ›å»ºL2ä¸“ç”¨å‚æ•°ç©ºé—´"""
    try:
        space = ParameterSpace()
    except:
        space = SimpleParameterSpace()

    # åªåŒ…å«åŸºç¡€æ¨¡å‹å‚æ•°ï¼Œä¸åŒ…å«HPLå‚æ•°
    space.add_continuous('learning_rate', 0.01, 0.08, scale='log')
    space.add_discrete('latent_factors', 15, 75, step=5)
    space.add_continuous('lambda_reg', 0.001, 0.02, scale='log')

    return space


def create_hpl_constraints():
    """åˆ›å»ºHPLä¸“ç”¨çº¦æŸæ¡ä»¶"""
    print("åˆ›å»ºHPLä¸“ç”¨çº¦æŸæ¡ä»¶...")

    try:
        constraints = ConstraintManager()
    except:
        constraints = SimpleConstraintManager()

    # æ ¸å¿ƒçº¦æŸï¼šdelta1 < delta2
    constraints.add_relation('delta1', 'delta2', '<')

    print(f"HPLçº¦æŸæ•°é‡: {len(constraints.constraints) if hasattr(constraints, 'constraints') else 1}")

    return constraints


def run_hpl_focused_optimization():
    """
    è¿è¡ŒHPLæŸå¤±å‡½æ•°ä¸“ç”¨ä¼˜åŒ–å®éªŒ

    è¯¥å‡½æ•°æ‰§è¡Œå®Œæ•´çš„HPLä¼˜åŒ–æµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®å‡†å¤‡ã€å‚æ•°ç©ºé—´å®šä¹‰ã€ä¼˜åŒ–å™¨é…ç½®ã€
    å‚æ•°ä¼˜åŒ–æœç´¢ä»¥åŠç»“æœåˆ†æã€‚é‡‡ç”¨åˆ†æ®µå¼ä¼˜åŒ–ç­–ç•¥ï¼Œä¼˜å…ˆä½¿ç”¨å®Œæ•´ç‰ˆhyperoptç»„ä»¶ï¼Œ
    åœ¨å¤±è´¥æ—¶è‡ªåŠ¨é™çº§ä¸ºç®€åŒ–ç‰ˆç»„ä»¶ã€‚

    Returns:
        tuple: åŒ…å«ä¸¤ä¸ªå…ƒç´ çš„å…ƒç»„
            - optimizer: ä½¿ç”¨çš„ä¼˜åŒ–å™¨å®ä¾‹ï¼ˆHyperOptimizer/SimpleHyperOptimizerï¼‰
            - best_trial: æœ€ä¼˜è¯•éªŒç»“æœå¯¹è±¡ï¼ŒåŒ…å«æœ€ä¼˜é…ç½®å’Œæ€§èƒ½æŒ‡æ ‡
    """
    print("="*60)
    print("HPLæŸå¤±å‡½æ•°ä¸“ç”¨ä¼˜åŒ–å®éªŒ")
    print("="*60)

    # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
    # åˆ›å»ºé’ˆå¯¹HPLæŸå¤±å‡½æ•°æ”¹è¿›çš„æ•°æ®å¤„ç†ç®¡é“
    data_manager = setup_improved_data()

    # æ„å»ºHPLä¸“ç”¨ç›®æ ‡å‡½æ•°
    # åŸºäºæ”¹è¿›æ•°æ®åˆ›å»ºä¸“é—¨çš„ä¼˜åŒ–ç›®æ ‡å‡½æ•°
    objective = HPLFocusedObjectiveFunction(data_manager)

    # å®šä¹‰HPLå‚æ•°æœç´¢ç©ºé—´
    # åŒ…å«å­¦ä¹ ç‡ã€æ­£åˆ™åŒ–å‚æ•°ã€æ½œåœ¨å› å­æ•°ç­‰æ ¸å¿ƒå‚æ•°çš„æœç´¢èŒƒå›´
    space = create_hpl_parameter_space()

    # é…ç½®å‚æ•°çº¦æŸæ¡ä»¶
    # è®¾ç½®å‚æ•°é—´çš„é€»è¾‘çº¦æŸå…³ç³»ï¼Œç¡®ä¿å‚æ•°ç»„åˆçš„æœ‰æ•ˆæ€§
    constraints = create_hpl_constraints()

    # ç»„ä»¶åˆå§‹åŒ–ä¸å…¼å®¹æ€§æ£€æŸ¥
    # å°è¯•åŠ è½½å®Œæ•´ç‰ˆä¼˜åŒ–ç»„ä»¶ï¼Œå¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢ç®€åŒ–ç‰ˆ
    try:
        sampler = LatinHypercubeSampler(space, seed=42)  # ä¼˜å…ˆä½¿ç”¨LHS
        tracker = ExperimentTracker('hpl_focused_optimization', backend='memory')
        optimizer = HyperOptimizer(
            objective_fn=objective,
            space=space,
            sampler=sampler,
            constraints=constraints,
            tracker=tracker,
            maximize=False,  # æœ€å°åŒ–RMSE
            seed=42
        )
        print("ä½¿ç”¨å®Œæ•´ç‰ˆhyperoptç»„ä»¶")
    except:
        sampler = SimpleLatinHypercubeSampler(space, seed=42)
        tracker = SimpleExperimentTracker('hpl_focused_optimization', backend='memory')
        optimizer = SimpleHyperOptimizer(
            objective_fn=objective,
            space=space,
            sampler=sampler,
            constraints=constraints,
            tracker=tracker,
            maximize=False,
            seed=42
        )
        print("ä½¿ç”¨ç®€åŒ–ç‰ˆhyperoptç»„ä»¶")

    print("å¼€å§‹HPLä¸“ç”¨ä¼˜åŒ–...")
    start_time = time.time()

    # æ‰§è¡Œä¼˜åŒ–æœç´¢è¿‡ç¨‹
    # ä½¿ç”¨å¢å¼ºçš„è¯•éªŒæ¬¡æ•°å’Œè€å¿ƒå€¼è¿›è¡Œå‚æ•°ä¼˜åŒ–
    best_trial = optimizer.optimize(
        n_trials=40,                 # å¢åŠ è¯•éªŒæ•°
        no_improvement_rounds=15,    # å¢åŠ è€å¿ƒ
        batch_size=1
    )

    end_time = time.time()

    # ä¼˜åŒ–ç»“æœè§£æä¸å±•ç¤º
    # è¾“å‡ºæœ€ä¼˜å‚æ•°é…ç½®ã€æ€§èƒ½æŒ‡æ ‡åŠå‚æ•°åˆç†æ€§åˆ†æ
    print("\n" + "="*60)
    print("HPLä¸“ç”¨ä¼˜åŒ–ç»“æœ")
    print("="*60)

    if best_trial:
        print(f"HPLæœ€ä½³é…ç½®: {best_trial.config}")
        print(f"HPLæœ€ä½³éªŒè¯RMSE: {best_trial.score:.4f}")
        print(f"HPLä¼˜åŒ–è€—æ—¶: {end_time - start_time:.2f}ç§’")

        # å‚æ•°ç‰¹å¾åˆ†æ
        # å¯¹å…³é”®HPLå‚æ•°è¿›è¡Œè¯¦ç»†è§£æå’Œé—´éš”åˆç†æ€§éªŒè¯
        config = best_trial.config
        print(f"\nHPLå‚æ•°åˆ†æ:")
        print(f"  å­¦ä¹ ç‡: {config['learning_rate']:.4f}")
        print(f"  æ½œåœ¨å› å­æ•°: {config['latent_factors']}")
        print(f"  æ­£åˆ™åŒ–å‚æ•°: {config['lambda_reg']:.6f}")
        print(f"  HPL delta1: {config['delta1']:.3f}")
        print(f"  HPL delta2: {config['delta2']:.3f}")
        print(f"  HPL l_max: {config.get('l_max', 4.0):.2f}")
        print(f"  HPL c_sigmoid: {config.get('c_sigmoid', 1.0):.2f}")
        print(f"  deltaå·®å€¼: {config['delta2'] - config['delta1']:.3f}")

        # å‚æ•°åˆç†æ€§éªŒè¯
        # æ£€æŸ¥deltaé—´éš”å’Œl_maxè®¾ç½®æ˜¯å¦ç¬¦åˆHPLç†è®ºè¦æ±‚
        delta_gap = config['delta2'] - config['delta1']
        l_max_val = config.get('l_max', 4.0)

        print(f"\nHPLå‚æ•°åˆç†æ€§åˆ†æ:")
        if delta_gap < 0.3:
            print(f"  âš ï¸  deltaé—´éš”è¿‡å° ({delta_gap:.3f})ï¼Œå¯èƒ½å½±å“åˆ†æ®µæ•ˆæœ")
        elif delta_gap > 2.0:
            print(f"  âš ï¸  deltaé—´éš”è¿‡å¤§ ({delta_gap:.3f})ï¼Œå¯èƒ½å¤±å»åˆ†æ®µä¼˜åŠ¿")
        else:
            print(f"  âœ… deltaé—´éš”åˆç† ({delta_gap:.3f})")

        if l_max_val <= config['delta2']:
            print(f"  âš ï¸  l_maxè¿‡å°ï¼Œåº”è¯¥ > delta2")
        else:
            print(f"  âœ… l_maxè®¾ç½®åˆç† ({l_max_val:.2f} > {config['delta2']:.3f})")
    else:
        print("HPLä¼˜åŒ–å¤±è´¥ï¼Œæœªæ‰¾åˆ°æœ‰æ•ˆé…ç½®")

    # ä¼˜åŒ–è¿‡ç¨‹ç»Ÿè®¡æ±‡æ€»
    # å±•ç¤ºè¯•éªŒæ€»æ•°ã€æˆåŠŸæ•°å’Œå¤±è´¥æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯
    results = optimizer.get_results()
    print(f"\nHPLä¼˜åŒ–ç»Ÿè®¡:")
    print(f"æ€»è¯•éªŒæ•°: {results['n_trials']}")
    print(f"æˆåŠŸè¯•éªŒæ•°: {results['n_completed']}")
    print(f"å¤±è´¥è¯•éªŒæ•°: {results['n_failed']}")

    return optimizer, best_trial


def run_l2_focused_optimization():
    """L2æŸå¤±å‡½æ•°ä¸“ç”¨ä¼˜åŒ–ï¼ˆç”¨äºå…¬å¹³å¯¹æ¯”ï¼‰"""
    print("å¼€å§‹L2ä¸“ç”¨ä¼˜åŒ–...")

    # 1. å‡†å¤‡ç›¸åŒçš„æ•°æ®
    data_manager = setup_improved_data()

    # 2. åˆ›å»ºL2ä¸“ç”¨ç›®æ ‡å‡½æ•°
    objective = L2FocusedObjectiveFunction(data_manager)

    # 3. åˆ›å»ºL2å‚æ•°ç©ºé—´ï¼ˆä¸åŒ…å«HPLå‚æ•°ï¼‰
    space = create_l2_parameter_space()

    # 4. åˆ›å»ºé‡‡æ ·å™¨å’Œä¼˜åŒ–å™¨
    try:
        sampler = LatinHypercubeSampler(space, seed=42)
        tracker = ExperimentTracker('l2_focused_optimization', backend='memory')
        optimizer = HyperOptimizer(
            objective_fn=objective,
            space=space,
            sampler=sampler,
            constraints=None,
            tracker=tracker,
            maximize=False,
            seed=42
        )
    except:
        sampler = SimpleLatinHypercubeSampler(space, seed=42)
        tracker = SimpleExperimentTracker('l2_focused_optimization', backend='memory')
        optimizer = SimpleHyperOptimizer(
            objective_fn=objective,
            space=space,
            sampler=sampler,
            constraints=None,
            tracker=tracker,
            maximize=False,
            seed=42
        )

    # 5. è¿è¡Œä¼˜åŒ–ï¼ˆç›¸åŒçš„è¯•éªŒé¢„ç®—ï¼‰
    best_trial = optimizer.optimize(
        n_trials=40,                 # ä¸HPLç›¸åŒçš„è¯•éªŒæ•°
        no_improvement_rounds=15,    # ç›¸åŒçš„è€å¿ƒ
        batch_size=1
    )

    if best_trial:
        print(f"L2æœ€ä½³RMSE: {best_trial.score:.4f}")
        print(f"L2æœ€ä½³é…ç½®: {best_trial.config}")

    return optimizer, best_trial


def run_hpl_vs_l2_comparison():
    """HPLä¸L2æŸå¤±å‡½æ•°çš„å…¬å¹³å¯¹æ¯”"""
    print("\n" + "="*60)
    print("HPL vs L2 å…¬å¹³å¯¹æ¯”å®éªŒ")
    print("="*60)

    results = {}

    # 1. HPLä¸“ç”¨ä¼˜åŒ–
    print("\n1. è¿è¡ŒHPLä¸“ç”¨ä¼˜åŒ–...")
    hpl_optimizer, hpl_best = run_hpl_focused_optimization()
    results['HPL'] = {
        'best_rmse': hpl_best.score if hpl_best else float('inf'),
        'best_config': hpl_best.config if hpl_best else None,
        'optimizer': hpl_optimizer
    }

    # 2. L2ä¸“ç”¨ä¼˜åŒ–ï¼ˆä½¿ç”¨ç›¸åŒçš„è¯•éªŒé¢„ç®—å’Œè®­ç»ƒè®¾ç½®ï¼‰
    print("\n2. è¿è¡ŒL2ä¸“ç”¨ä¼˜åŒ–...")
    l2_optimizer, l2_best = run_l2_focused_optimization()
    results['L2'] = {
        'best_rmse': l2_best.score if l2_best else float('inf'),
        'best_config': l2_best.config if l2_best else None,
        'optimizer': l2_optimizer
    }

    # 3. å¯¹æ¯”åˆ†æ
    print("\n" + "="*60)
    print("HPL vs L2 å¯¹æ¯”ç»“æœ")
    print("="*60)

    hpl_rmse = results['HPL']['best_rmse']
    l2_rmse = results['L2']['best_rmse']

    print(f"{'æŸå¤±å‡½æ•°':<12} {'æœ€ä½³RMSE':<12} {'ç›¸å¯¹è¡¨ç°':<12}")
    print("-" * 40)
    print(f"{'HPL':<12} {hpl_rmse:<12.4f} {'åŸºå‡†':<12}")
    print(f"{'L2':<12} {l2_rmse:<12.4f} {'å¯¹æ¯”':<12}")

    if hpl_rmse < l2_rmse:
        improvement = (l2_rmse - hpl_rmse) / l2_rmse * 100
        print(f"\nğŸ‰ HPLæŸå¤±å‡½æ•°è¡¨ç°æ›´ä¼˜!")
        print(f"   ç›¸æ¯”L2æ”¹è¿›äº†: {improvement:.2f}%")
        print(f"   ç»å¯¹æ”¹è¿›: {l2_rmse - hpl_rmse:.4f}")
        winner = 'HPL'
    elif l2_rmse < hpl_rmse:
        degradation = (hpl_rmse - l2_rmse) / l2_rmse * 100
        print(f"\nğŸ“Š L2æŸå¤±å‡½æ•°è¡¨ç°æ›´ä¼˜")
        print(f"   HPLç›¸æ¯”L2å·®äº†: {degradation:.2f}%")
        print(f"   ç»å¯¹å·®è·: {hpl_rmse - l2_rmse:.4f}")
        winner = 'L2'
    else:
        print(f"\nğŸ¤ ä¸¤ç§æŸå¤±å‡½æ•°è¡¨ç°ç›¸å½“")
        winner = 'Tie'

    # 4. è¯¦ç»†é…ç½®å¯¹æ¯”
    print(f"\næœ€ä½³é…ç½®å¯¹æ¯”:")
    if results['HPL']['best_config']:
        hpl_config = results['HPL']['best_config']
        print(f"\nHPLæœ€ä½³é…ç½®:")
        print(f"  å­¦ä¹ ç‡: {hpl_config['learning_rate']:.4f}")
        print(f"  å› å­æ•°: {hpl_config['latent_factors']}")
        print(f"  æ­£åˆ™åŒ–: {hpl_config['lambda_reg']:.6f}")
        print(f"  delta1: {hpl_config['delta1']:.3f}")
        print(f"  delta2: {hpl_config['delta2']:.3f}")
        print(f"  l_max: {hpl_config.get('l_max', 4.0):.2f}")
        print(f"  c_sigmoid: {hpl_config.get('c_sigmoid', 1.0):.2f}")

    if results['L2']['best_config']:
        l2_config = results['L2']['best_config']
        print(f"\nL2æœ€ä½³é…ç½®:")
        print(f"  å­¦ä¹ ç‡: {l2_config['learning_rate']:.4f}")
        print(f"  å› å­æ•°: {l2_config['latent_factors']}")
        print(f"  æ­£åˆ™åŒ–: {l2_config['lambda_reg']:.6f}")

    # 5. ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_results = {
        'winner': winner,
        'hpl_rmse': hpl_rmse,
        'l2_rmse': l2_rmse,
        'improvement_percentage': (l2_rmse - hpl_rmse) / l2_rmse * 100 if l2_rmse != 0 else 0,
        'hpl_config': results['HPL']['best_config'],
        'l2_config': results['L2']['best_config']
    }

    # ä¿å­˜è¯¦ç»†ç»“æœ
    save_comparison_results(comparison_results, "hpl_vs_l2_comparison.json")

    return results


def save_comparison_results(results, filename):
    """ä¿å­˜å¯¹æ¯”ç»“æœ"""
    try:
        def convert_numpy_types(obj):
            if isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            elif hasattr(obj, 'item'):
                return obj.item()
            elif hasattr(obj, 'tolist'):
                return obj.tolist()
            else:
                return obj

        clean_results = convert_numpy_types(results)
        clean_results['timestamp'] = time.strftime("%Y-%m-%d %H:%M:%S")

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(clean_results, f, indent=2, ensure_ascii=False)

        print(f"\nå¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {filename}")

    except Exception as e:
        print(f"ä¿å­˜å¯¹æ¯”ç»“æœå¤±è´¥: {e}")


def main():
    """HPLä¸“ç”¨ä¼˜åŒ–ä¸»å‡½æ•°"""
    try:
        print("HPLæŸå¤±å‡½æ•°æ·±åº¦ä¼˜åŒ–å®éªŒ")
        print("="*60)

        # 1. åŸºç¡€HPLä¼˜åŒ–å®éªŒ
        print("\n1. è¿è¡ŒHPLä¸“ç”¨ä¼˜åŒ–å®éªŒ...")
        hpl_optimizer, hpl_best = run_hpl_focused_optimization()

        # 2. HPL vs L2 å…¬å¹³å¯¹æ¯”
        print("\n2. è¿è¡ŒHPL vs L2å…¬å¹³å¯¹æ¯”...")
        comparison_results = run_hpl_vs_l2_comparison()

        # 3. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        print("\n" + "="*60)
        print("æœ€ç»ˆå®éªŒæ€»ç»“")
        print("="*60)

        if hpl_best:
            print(f"\nHPLä¸“ç”¨ä¼˜åŒ–æœ€ä½³RMSE: {hpl_best.score:.4f}")
            print(f"HPLæœ€ä½³é…ç½®:")
            config = hpl_best.config
            print(f"  å­¦ä¹ ç‡: {config['learning_rate']:.4f}")
            print(f"  æ½œåœ¨å› å­æ•°: {config['latent_factors']}")
            print(f"  æ­£åˆ™åŒ–: {config['lambda_reg']:.6f}")
            print(f"  Î´1: {config['delta1']:.3f}")
            print(f"  Î´2: {config['delta2']:.3f}")
            print(f"  l_max: {config.get('l_max', 4.0):.2f}")
            print(f"  c_sigmoid: {config.get('c_sigmoid', 1.0):.2f}")

        if comparison_results:
            hpl_rmse = comparison_results['HPL']['best_rmse']
            l2_rmse = comparison_results['L2']['best_rmse']
            print(f"\nå…¬å¹³å¯¹æ¯”ç»“æœ:")
            print(f"  HPLæœ€ä½³RMSE: {hpl_rmse:.4f}")
            print(f"  L2æœ€ä½³RMSE: {l2_rmse:.4f}")
            if hpl_rmse < l2_rmse:
                improvement = (l2_rmse - hpl_rmse) / l2_rmse * 100
                print(f"  ğŸ‰ HPLä¼˜äºL2ï¼Œæ”¹è¿›: {improvement:.2f}%")
            else:
                degradation = (hpl_rmse - l2_rmse) / l2_rmse * 100
                print(f"  ğŸ“Š L2ä¼˜äºHPLï¼Œå·®è·: {degradation:.2f}%")

        print("\nå®éªŒå®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°ç›¸åº”çš„JSONæ–‡ä»¶ä¸­ã€‚")

        # æä¾›æ”¹è¿›å»ºè®®
        print("\næ”¹è¿›å»ºè®®:")
        print("1. å¦‚æœHPLæ€§èƒ½ä¸ä½³ï¼Œè¯·è€ƒè™‘å¢åŠ è®­ç»ƒè½®æ•°å’Œè¯•éªŒæ•°")
        print("2. å¯ä»¥è°ƒæ•´delta1å’Œdelta2çš„æœç´¢èŒƒå›´")
        print("3. è€ƒè™‘æ·»åŠ æ›´å¤šçš„HPLé«˜çº§å‚æ•°ä¼˜åŒ–")
        print("4. å¯ä»¥å°è¯•ä¸åŒçš„æ•°æ®é›†å’Œåˆå§‹åŒ–ç­–ç•¥")

        return True

    except Exception as e:
        print(f"HPLä¼˜åŒ–å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)


