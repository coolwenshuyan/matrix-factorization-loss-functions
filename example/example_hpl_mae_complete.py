#!/usr/bin/env python3
"""
HPLæŸå¤±å‡½æ•°ä¸“ç”¨ä¼˜åŒ–å®éªŒ - MAEæŒ‡æ ‡å®Œæ•´ç‰ˆæœ¬

ä¸“é—¨é’ˆå¯¹æ··åˆåˆ†æ®µæŸå¤±å‡½æ•°(HPL)çš„è¶…å‚æ•°ä¼˜åŒ–ï¼Œä½¿ç”¨MAEä½œä¸ºä¼˜åŒ–ç›®æ ‡
å®Œæ•´çš„HPL vs L2 vs L1åœ¨MAEæŒ‡æ ‡ä¸Šçš„å¯¹æ¯”å®éªŒ
"""

import sys
import os
import numpy as np
import time
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# å¯¼å…¥æ¨¡å—
try:
    from src.hyperopt.space import ParameterSpace
    from src.hyperopt.samplers import LatinHypercubeSampler
    from src.hyperopt.constraints import ConstraintManager
    from src.hyperopt.optimizer import HyperOptimizer
    from src.hyperopt.tracker import ExperimentTracker
    print("æˆåŠŸå¯¼å…¥hyperoptæ¨¡å—")
except ImportError as e:
    print(f"å¯¼å…¥hyperoptæ¨¡å—å¤±è´¥: {e}")

try:
    from data.data_manager import DataManager
    from src.models.mf_sgd import MatrixFactorizationSGD
    from src.models.initializers import NormalInitializer
    from src.models.regularizers import L2Regularizer
    from src.losses.hpl import HybridPiecewiseLoss
    from src.losses.standard import L2Loss, L1Loss  # æ·»åŠ L1Lossç”¨äºMAEè®­ç»ƒ
    from src.evaluation.metrics import MAE, RMSE  # å¯¼å…¥MAEå’ŒRMSEè¯„ä¼°æŒ‡æ ‡
    print("æˆåŠŸå¯¼å…¥æ•°æ®å’Œæ¨¡å‹æ¨¡å—")
except ImportError as e:
    print(f"å¯¼å…¥æ•°æ®/æ¨¡å‹æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

try:
    from example_hyperopt import (
        SimpleParameterSpace, SimpleLatinHypercubeSampler,
        SimpleConstraintManager, SimpleHyperOptimizer, SimpleExperimentTracker
    )
    print("ç®€åŒ–ç‰ˆæœ¬ç±»å·²å‡†å¤‡å°±ç»ª")
except ImportError:
    print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥ç®€åŒ–ç‰ˆæœ¬ç±»")


class HPLMAEObjectiveFunction:
    """ä¸“é—¨é’ˆå¯¹HPLä¼˜åŒ–MAEçš„ç›®æ ‡å‡½æ•°"""

    def __init__(self, data_manager):
        self.data_manager = data_manager
        self.n_users = data_manager.get_statistics()['n_users']
        self.n_items = data_manager.get_statistics()['n_items']
        self.train_data, self.val_data, self.test_data = data_manager.get_splits()

        # åˆå§‹åŒ–MAEè¯„ä¼°å™¨
        self.mae_metric = MAE()

        print(f"HPL-MAEç›®æ ‡å‡½æ•°åˆå§‹åŒ–:")
        print(f"  è®­ç»ƒé›†: {len(self.train_data)} æ¡")
        print(f"  éªŒè¯é›†: {len(self.val_data)} æ¡")
        print(f"  æµ‹è¯•é›†: {len(self.test_data)} æ¡")
        print(f"  ä¼˜åŒ–ç›®æ ‡: MAE (å¹³å‡ç»å¯¹è¯¯å·®)")

    def __call__(self, config):
        """HPLä¸“ç”¨ç›®æ ‡å‡½æ•°ï¼šå¼ºåˆ¶ä½¿ç”¨HPLæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–MAEæŒ‡æ ‡"""
        try:
            # å¼ºåˆ¶ä½¿ç”¨HPLæŸå¤±å‡½æ•°
            loss_function = HybridPiecewiseLoss(
                delta1=config['delta1'],
                delta2=config['delta2'],
                l_max=config.get('l_max', 4.0),
                c_sigmoid=config.get('c_sigmoid', 1.0)
            )

            regularizer = L2Regularizer(lambda_reg=config['lambda_reg'])

            model = MatrixFactorizationSGD(
                n_users=self.n_users,
                n_items=self.n_items,
                n_factors=config['latent_factors'],
                learning_rate=config['learning_rate'],
                regularizer=regularizer,
                loss_function=loss_function,
                use_bias=True,
                global_mean=self.data_manager.global_mean or 0.0
            )

            initializer = NormalInitializer(mean=0.0, std=0.01)
            model.initialize_parameters(initializer)

            # å¢åŠ è®­ç»ƒæ—¶é—´ï¼ŒHPLéœ€è¦æ›´å¤šæ—¶é—´æ”¶æ•›
            model.fit(
                train_data=self.train_data,
                val_data=self.val_data,
                n_epochs=50,        # å¢åŠ åˆ°50è½®
                verbose=0,          # ä¸æ‰“å°è¯¦ç»†è®­ç»ƒè¿‡ç¨‹
                early_stopping_patience=15  # å¢åŠ è€å¿ƒ
            )

            # éªŒè¯é›†è¯„ä¼°
            val_predictions = model.predict(
                self.val_data[:, 0].astype(int),
                self.val_data[:, 1].astype(int)
            )

            # è¿˜åŸåˆ°åŸå§‹å°ºåº¦
            if self.data_manager.global_mean is not None:
                val_predictions += self.data_manager.global_mean
                val_targets = self.val_data[:, 2] + self.data_manager.global_mean
            else:
                val_targets = self.val_data[:, 2]

            # ğŸ¯ å…³é”®ä¿®æ”¹ï¼šè®¡ç®—MAEè€Œä¸æ˜¯RMSE
            mae = self.mae_metric.calculate(val_targets, val_predictions)

            # è®°å½•HPLç‰¹æœ‰ä¿¡æ¯
            print(f"HPLé…ç½®: Î´1={config['delta1']:.3f}, Î´2={config['delta2']:.3f}, "
                  f"l_max={config.get('l_max', 4.0):.2f}, c_sig={config.get('c_sigmoid', 1.0):.2f}, "
                  f"MAE={mae:.4f}")

            return mae

        except Exception as e:
            print(f"HPLé…ç½®è¯„ä¼°å¤±è´¥ {config}: {e}")
            return 10.0  # è¿”å›è¾ƒå¤§å€¼è¡¨ç¤ºå¤±è´¥


class L2MAEObjectiveFunction:
    """ä¸“é—¨é’ˆå¯¹L2æŸå¤±ä¼˜åŒ–MAEçš„ç›®æ ‡å‡½æ•°"""

    def __init__(self, data_manager):
        self.data_manager = data_manager
        self.n_users = data_manager.get_statistics()['n_users']
        self.n_items = data_manager.get_statistics()['n_items']
        self.train_data, self.val_data, self.test_data = data_manager.get_splits()

        # åˆå§‹åŒ–MAEè¯„ä¼°å™¨
        self.mae_metric = MAE()

    def __call__(self, config):
        try:
            # å¼ºåˆ¶ä½¿ç”¨L2æŸå¤±å‡½æ•°
            loss_function = L2Loss()
            regularizer = L2Regularizer(lambda_reg=config['lambda_reg'])

            model = MatrixFactorizationSGD(
                n_users=self.n_users,
                n_items=self.n_items,
                n_factors=config['latent_factors'],
                learning_rate=config['learning_rate'],
                regularizer=regularizer,
                loss_function=loss_function,
                use_bias=True,
                global_mean=self.data_manager.global_mean or 0.0
            )

            initializer = NormalInitializer(mean=0.0, std=0.01)
            model.initialize_parameters(initializer)

            # ç›¸åŒçš„è®­ç»ƒè®¾ç½®
            model.fit(
                train_data=self.train_data,
                val_data=self.val_data,
                n_epochs=50,        # ä¸HPLç›¸åŒ
                verbose=0,
                early_stopping_patience=15  # ä¸HPLç›¸åŒ
            )

            # éªŒè¯é›†è¯„ä¼°
            val_predictions = model.predict(
                self.val_data[:, 0].astype(int),
                self.val_data[:, 1].astype(int)
            )

            if self.data_manager.global_mean is not None:
                val_predictions += self.data_manager.global_mean
                val_targets = self.val_data[:, 2] + self.data_manager.global_mean
            else:
                val_targets = self.val_data[:, 2]

            # ğŸ¯ è®¡ç®—MAE
            mae = self.mae_metric.calculate(val_targets, val_predictions)

            print(f"L2é…ç½®: lr={config['learning_rate']:.4f}, factors={config['latent_factors']}, "
                  f"reg={config['lambda_reg']:.6f}, MAE={mae:.4f}")

            return mae

        except Exception as e:
            print(f"L2é…ç½®è¯„ä¼°å¤±è´¥ {config}: {e}")
            return 10.0


class L1MAEObjectiveFunction:
    """ä¸“é—¨é’ˆå¯¹L1æŸå¤±ï¼ˆMAEæŸå¤±ï¼‰ä¼˜åŒ–MAEçš„ç›®æ ‡å‡½æ•°"""

    def __init__(self, data_manager):
        self.data_manager = data_manager
        self.n_users = data_manager.get_statistics()['n_users']
        self.n_items = data_manager.get_statistics()['n_items']
        self.train_data, self.val_data, self.test_data = data_manager.get_splits()

        # åˆå§‹åŒ–MAEè¯„ä¼°å™¨
        self.mae_metric = MAE()

    def __call__(self, config):
        try:
            # ğŸ¯ ä½¿ç”¨L1æŸå¤±å‡½æ•°ï¼ˆMAEæŸå¤±ï¼‰è¿›è¡Œè®­ç»ƒ
            loss_function = L1Loss(epsilon=config.get('l1_epsilon', 1e-8))
            regularizer = L2Regularizer(lambda_reg=config['lambda_reg'])

            model = MatrixFactorizationSGD(
                n_users=self.n_users,
                n_items=self.n_items,
                n_factors=config['latent_factors'],
                learning_rate=config['learning_rate'],
                regularizer=regularizer,
                loss_function=loss_function,
                use_bias=True,
                global_mean=self.data_manager.global_mean or 0.0
            )

            initializer = NormalInitializer(mean=0.0, std=0.01)
            model.initialize_parameters(initializer)

            # ç›¸åŒçš„è®­ç»ƒè®¾ç½®
            model.fit(
                train_data=self.train_data,
                val_data=self.val_data,
                n_epochs=50,
                verbose=0,
                early_stopping_patience=15
            )

            # éªŒè¯é›†è¯„ä¼°
            val_predictions = model.predict(
                self.val_data[:, 0].astype(int),
                self.val_data[:, 1].astype(int)
            )

            if self.data_manager.global_mean is not None:
                val_predictions += self.data_manager.global_mean
                val_targets = self.val_data[:, 2] + self.data_manager.global_mean
            else:
                val_targets = self.val_data[:, 2]

            # è®¡ç®—MAE
            mae = self.mae_metric.calculate(val_targets, val_predictions)

            print(f"L1é…ç½®: lr={config['learning_rate']:.4f}, factors={config['latent_factors']}, "
                  f"reg={config['lambda_reg']:.6f}, eps={config.get('l1_epsilon', 1e-8):.2e}, MAE={mae:.4f}")

            return mae

        except Exception as e:
            print(f"L1é…ç½®è¯„ä¼°å¤±è´¥ {config}: {e}")
            return 10.0


def setup_improved_data():
    """æ”¹è¿›çš„æ•°æ®è®¾ç½®ï¼Œä¿®æ­£åˆ†å‰²æ¯”ä¾‹é—®é¢˜"""
    print("å‡†å¤‡æ”¹è¿›çš„æ•°æ®...")

    # ä¿®æ­£æ•°æ®é…ç½®
    data_config = {
        'random_seed': 42,
        'train_ratio': 0.8,          # ç¡®ä¿80%è®­ç»ƒæ•°æ®
        'val_ratio': 0.1,            # 10%éªŒè¯æ•°æ®
        'test_ratio': 0.1,           # 10%æµ‹è¯•æ•°æ®
        'batch_size': 128,
        'shuffle': True,
        'center_data': True,         # æ•°æ®ä¸­å¿ƒåŒ–æœ‰åŠ©äºHPL
        'ensure_user_in_train': True
    }

    data_manager = DataManager(data_config)
    data_manager.load_dataset('movielens100k', 'dataset/20201202M100K_data_all_random.txt')
    data_manager.preprocess()

    # éªŒè¯æ•°æ®åˆ†å‰²
    train_data, val_data, test_data = data_manager.get_splits()
    print(f"ä¿®æ­£åæ•°æ®åˆ†å‰²:")
    print(f"  è®­ç»ƒé›†: {len(train_data)} æ¡ ({len(train_data)/100000*100:.1f}%)")
    print(f"  éªŒè¯é›†: {len(val_data)} æ¡ ({len(val_data)/100000*100:.1f}%)")
    print(f"  æµ‹è¯•é›†: {len(test_data)} æ¡ ({len(test_data)/100000*100:.1f}%)")

    data_manager.print_summary()
    return data_manager


def create_hpl_parameter_space():
    """åˆ›å»ºHPLä¸“ç”¨å‚æ•°ç©ºé—´"""
    print("åˆ›å»ºHPLä¸“ç”¨å‚æ•°ç©ºé—´...")

    try:
        space = ParameterSpace()
    except:
        space = SimpleParameterSpace()

    # åŸºç¡€æ¨¡å‹å‚æ•° - åŸºäºä¹‹å‰å®éªŒç»“æœä¼˜åŒ–èŒƒå›´
    space.add_continuous('learning_rate', 0.01, 0.08, scale='log')  # ç¼©å°åˆ°æœ‰æ•ˆèŒƒå›´
    space.add_discrete('latent_factors', 15, 75, step=5)            # èšç„¦æœ‰æ•ˆåŒºé—´
    space.add_continuous('lambda_reg', 0.001, 0.02, scale='log')    # ä¸“æ³¨å¼±æ­£åˆ™åŒ–

    # HPLä¸“ç”¨å‚æ•° - æ‰©å±•æœç´¢èŒƒå›´
    space.add_continuous('delta1', 0.05, 1.5)      # æ‰©å±•ä¸‹ç•Œï¼Œå…è®¸æ›´å°çš„delta1
    space.add_continuous('delta2', 0.8, 4.0)       # æ‰©å±•ä¸Šç•Œï¼Œå…è®¸æ›´å¤§çš„delta2
    space.add_continuous('l_max', 2.5, 6.0)        # æŸå¤±å‡½æ•°ä¸Šé™
    space.add_continuous('c_sigmoid', 0.3, 3.0)    # Sigmoidå‡½æ•°é™¡å³­åº¦

    print(f"HPLå‚æ•°ç©ºé—´ç»´åº¦: {space.get_dimension()}")
    print("HPLå‚æ•°ä¿¡æ¯:")
    for name, info in space.get_parameter_info().items():
        print(f"  {name}: {info}")

    return space


def create_l2_parameter_space():
    """åˆ›å»ºL2ä¸“ç”¨å‚æ•°ç©ºé—´"""
    try:
        space = ParameterSpace()
    except:
        space = SimpleParameterSpace()

    # åªåŒ…å«åŸºç¡€æ¨¡å‹å‚æ•°ï¼Œä¸åŒ…å«HPLå‚æ•°
    space.add_continuous('learning_rate', 0.01, 0.08, scale='log')
    space.add_discrete('latent_factors', 15, 75, step=5)
    space.add_continuous('lambda_reg', 0.001, 0.02, scale='log')

    return space


def create_l1_parameter_space():
    """åˆ›å»ºL1ä¸“ç”¨å‚æ•°ç©ºé—´"""
    try:
        space = ParameterSpace()
    except:
        space = SimpleParameterSpace()

    # L1æŸå¤±ä¸“ç”¨å‚æ•°
    space.add_continuous('learning_rate', 0.01, 0.08, scale='log')
    space.add_discrete('latent_factors', 15, 75, step=5)
    space.add_continuous('lambda_reg', 0.001, 0.02, scale='log')
    space.add_continuous('l1_epsilon', 1e-10, 1e-6, scale='log')  # L1å¹³æ»‘å‚æ•°

    return space


def create_hpl_constraints():
    """åˆ›å»ºHPLä¸“ç”¨çº¦æŸæ¡ä»¶"""
    print("åˆ›å»ºHPLä¸“ç”¨çº¦æŸæ¡ä»¶...")

    try:
        constraints = ConstraintManager()
    except:
        constraints = SimpleConstraintManager()

    # æ ¸å¿ƒçº¦æŸï¼šdelta1 < delta2
    constraints.add_relation('delta1', 'delta2', '<')

    print(f"HPLçº¦æŸæ•°é‡: {len(constraints.constraints) if hasattr(constraints, 'constraints') else 1}")

    return constraints


def run_hpl_mae_optimization():
    """è¿è¡ŒHPLé’ˆå¯¹MAEçš„ä¸“ç”¨ä¼˜åŒ–å®éªŒ"""
    print("="*60)
    print("HPLæŸå¤±å‡½æ•°ä¸“ç”¨ä¼˜åŒ–å®éªŒ - MAEç›®æ ‡")
    print("="*60)

    # 1. å‡†å¤‡æ”¹è¿›çš„æ•°æ®
    data_manager = setup_improved_data()

    # 2. åˆ›å»ºHPLä¸“ç”¨ç›®æ ‡å‡½æ•°ï¼ˆMAEç‰ˆæœ¬ï¼‰
    objective = HPLMAEObjectiveFunction(data_manager)

    # 3. åˆ›å»ºHPLä¸“ç”¨å‚æ•°ç©ºé—´
    space = create_hpl_parameter_space()

    # 4. åˆ›å»ºHPLä¸“ç”¨çº¦æŸ
    constraints = create_hpl_constraints()

    # 5. åˆ›å»ºé‡‡æ ·å™¨å’Œä¼˜åŒ–å™¨
    try:
        sampler = LatinHypercubeSampler(space, seed=42)  # ä¼˜å…ˆä½¿ç”¨LHS
        tracker = ExperimentTracker('hpl_mae_optimization', backend='memory')
        optimizer = HyperOptimizer(
            objective_fn=objective,
            space=space,
            sampler=sampler,
            constraints=constraints,
            tracker=tracker,
            maximize=False,  # æœ€å°åŒ–MAE
            seed=42
        )
        print("ä½¿ç”¨å®Œæ•´ç‰ˆhyperoptç»„ä»¶")
    except:
        sampler = SimpleLatinHypercubeSampler(space, seed=42)
        tracker = SimpleExperimentTracker('hpl_mae_optimization', backend='memory')
        optimizer = SimpleHyperOptimizer(
            objective_fn=objective,
            space=space,
            sampler=sampler,
            constraints=constraints,
            tracker=tracker,
            maximize=False,
            seed=42
        )
        print("ä½¿ç”¨ç®€åŒ–ç‰ˆhyperoptç»„ä»¶")

    print("å¼€å§‹HPLä¸“ç”¨MAEä¼˜åŒ–...")
    start_time = time.time()

    # 6. è¿è¡Œä¼˜åŒ– - å¢åŠ è¯•éªŒæ•°å’Œè€å¿ƒ
    best_trial = optimizer.optimize(
        n_trials=40,                 # å¢åŠ è¯•éªŒæ•°
        no_improvement_rounds=15,    # å¢åŠ è€å¿ƒ
        batch_size=1
    )

    end_time = time.time()

    # 7. æ‰“å°HPLä¸“ç”¨ç»“æœ
    print("\n" + "="*60)
    print("HPLä¸“ç”¨MAEä¼˜åŒ–ç»“æœ")
    print("="*60)

    if best_trial:
        print(f"HPLæœ€ä½³é…ç½®: {best_trial.config}")
        print(f"HPLæœ€ä½³éªŒè¯MAE: {best_trial.score:.4f}")
        print(f"HPLä¼˜åŒ–è€—æ—¶: {end_time - start_time:.2f}ç§’")

        # HPLå‚æ•°åˆ†æ
        config = best_trial.config
        print(f"\nHPLå‚æ•°åˆ†æ:")
        print(f"  å­¦ä¹ ç‡: {config['learning_rate']:.4f}")
        print(f"  æ½œåœ¨å› å­æ•°: {config['latent_factors']}")
        print(f"  æ­£åˆ™åŒ–å‚æ•°: {config['lambda_reg']:.6f}")
        print(f"  HPL delta1: {config['delta1']:.3f}")
        print(f"  HPL delta2: {config['delta2']:.3f}")
        print(f"  HPL l_max: {config.get('l_max', 4.0):.2f}")
        print(f"  HPL c_sigmoid: {config.get('c_sigmoid', 1.0):.2f}")
        print(f"  deltaå·®å€¼: {config['delta2'] - config['delta1']:.3f}")

        # åˆ†æHPLå‚æ•°çš„åˆç†æ€§
        delta_gap = config['delta2'] - config['delta1']
        l_max_val = config.get('l_max', 4.0)

        print(f"\nHPLå‚æ•°åˆç†æ€§åˆ†æ:")
        if delta_gap < 0.3:
            print(f"  âš ï¸  deltaé—´éš”è¿‡å° ({delta_gap:.3f})ï¼Œå¯èƒ½å½±å“åˆ†æ®µæ•ˆæœ")
        elif delta_gap > 2.0:
            print(f"  âš ï¸  deltaé—´éš”è¿‡å¤§ ({delta_gap:.3f})ï¼Œå¯èƒ½å¤±å»åˆ†æ®µä¼˜åŠ¿")
        else:
            print(f"  âœ… deltaé—´éš”åˆç† ({delta_gap:.3f})")

        if l_max_val <= config['delta2']:
            print(f"  âš ï¸  l_maxè¿‡å°ï¼Œåº”è¯¥ > delta2")
        else:
            print(f"  âœ… l_maxè®¾ç½®åˆç† ({l_max_val:.2f} > {config['delta2']:.3f})")
    else:
        print("HPLä¼˜åŒ–å¤±è´¥ï¼Œæœªæ‰¾åˆ°æœ‰æ•ˆé…ç½®")

    # 8. è·å–ä¼˜åŒ–å†å²
    results = optimizer.get_results()
    print(f"\nHPLä¼˜åŒ–ç»Ÿè®¡:")
    print(f"æ€»è¯•éªŒæ•°: {results['n_trials']}")
    print(f"æˆåŠŸè¯•éªŒæ•°: {results['n_completed']}")
    print(f"å¤±è´¥è¯•éªŒæ•°: {results['n_failed']}")

    return optimizer, best_trial


def run_l2_mae_optimization():
    """L2æŸå¤±å‡½æ•°ä¸“ç”¨MAEä¼˜åŒ–ï¼ˆç”¨äºå…¬å¹³å¯¹æ¯”ï¼‰"""
    print("å¼€å§‹L2ä¸“ç”¨MAEä¼˜åŒ–...")

    # 1. å‡†å¤‡ç›¸åŒçš„æ•°æ®
    data_manager = setup_improved_data()

    # 2. åˆ›å»ºL2ä¸“ç”¨ç›®æ ‡å‡½æ•°ï¼ˆMAEç‰ˆæœ¬ï¼‰
    objective = L2MAEObjectiveFunction(data_manager)

    # 3. åˆ›å»ºL2å‚æ•°ç©ºé—´ï¼ˆä¸åŒ…å«HPLå‚æ•°ï¼‰
    space = create_l2_parameter_space()

    # 4. åˆ›å»ºé‡‡æ ·å™¨å’Œä¼˜åŒ–å™¨
    try:
        sampler = LatinHypercubeSampler(space, seed=42)
        tracker = ExperimentTracker('l2_mae_optimization', backend='memory')
        optimizer = HyperOptimizer(
            objective_fn=objective,
            space=space,
            sampler=sampler,
            constraints=None,
            tracker=tracker,
            maximize=False,
            seed=42
        )
    except:
        sampler = SimpleLatinHypercubeSampler(space, seed=42)
        tracker = SimpleExperimentTracker('l2_mae_optimization', backend='memory')
        optimizer = SimpleHyperOptimizer(
            objective_fn=objective,
            space=space,
            sampler=sampler,
            constraints=None,
            tracker=tracker,
            maximize=False,
            seed=42
        )

    # 5. è¿è¡Œä¼˜åŒ–ï¼ˆç›¸åŒçš„è¯•éªŒé¢„ç®—ï¼‰
    best_trial = optimizer.optimize(
        n_trials=40,                 # ä¸HPLç›¸åŒçš„è¯•éªŒæ•°
        no_improvement_rounds=15,    # ç›¸åŒçš„è€å¿ƒ
        batch_size=1
    )

    if best_trial:
        print(f"L2æœ€ä½³MAE: {best_trial.score:.4f}")
        print(f"L2æœ€ä½³é…ç½®: {best_trial.config}")

    return optimizer, best_trial


def run_l1_mae_optimization():
    """L1æŸå¤±å‡½æ•°ä¸“ç”¨MAEä¼˜åŒ–ï¼ˆç›´æ¥ä¼˜åŒ–MAEæŸå¤±ï¼‰"""
    print("å¼€å§‹L1ä¸“ç”¨MAEä¼˜åŒ–...")

    # 1. å‡†å¤‡ç›¸åŒçš„æ•°æ®
    data_manager = setup_improved_data()

    # 2. åˆ›å»ºL1ä¸“ç”¨ç›®æ ‡å‡½æ•°ï¼ˆMAEç‰ˆæœ¬ï¼‰
    objective = L1MAEObjectiveFunction(data_manager)

    # 3. åˆ›å»ºL1å‚æ•°ç©ºé—´
    space = create_l1_parameter_space()

    # 4. åˆ›å»ºé‡‡æ ·å™¨å’Œä¼˜åŒ–å™¨
    try:
        sampler = LatinHypercubeSampler(space, seed=42)
        tracker = ExperimentTracker('l1_mae_optimization', backend='memory')
        optimizer = HyperOptimizer(
            objective_fn=objective,
            space=space,
            sampler=sampler,
            constraints=None,
            tracker=tracker,
            maximize=False,
            seed=42
        )
    except:
        sampler = SimpleLatinHypercubeSampler(space, seed=42)
        tracker = SimpleExperimentTracker('l1_mae_optimization', backend='memory')
        optimizer = SimpleHyperOptimizer(
            objective_fn=objective,
            space=space,
            sampler=sampler,
            constraints=None,
            tracker=tracker,
            maximize=False,
            seed=42
        )

    # 5. è¿è¡Œä¼˜åŒ–ï¼ˆç›¸åŒçš„è¯•éªŒé¢„ç®—ï¼‰
    best_trial = optimizer.optimize(
        n_trials=40,                 # ä¸HPLç›¸åŒçš„è¯•éªŒæ•°
        no_improvement_rounds=15,    # ç›¸åŒçš„è€å¿ƒ
        batch_size=1
    )

    if best_trial:
        print(f"L1æœ€ä½³MAE: {best_trial.score:.4f}")
        print(f"L1æœ€ä½³é…ç½®: {best_trial.config}")

    return optimizer, best_trial


def run_mae_comparison():
    """HPL vs L2 vs L1 åœ¨MAEæŒ‡æ ‡ä¸Šçš„å…¬å¹³å¯¹æ¯”"""
    print("\n" + "="*60)
    print("HPL vs L2 vs L1 MAEæŒ‡æ ‡å…¬å¹³å¯¹æ¯”å®éªŒ")
    print("="*60)

    results = {}

    # 1. HPLä¸“ç”¨MAEä¼˜åŒ–
    print("\n1. è¿è¡ŒHPLä¸“ç”¨MAEä¼˜åŒ–...")
    hpl_optimizer, hpl_best = run_hpl_mae_optimization()
    results['HPL'] = {
        'best_mae': hpl_best.score if hpl_best else float('inf'),
        'best_config': hpl_best.config if hpl_best else None,
        'optimizer': hpl_optimizer
    }

    # 2. L2ä¸“ç”¨MAEä¼˜åŒ–
    print("\n2. è¿è¡ŒL2ä¸“ç”¨MAEä¼˜åŒ–...")
    l2_optimizer, l2_best = run_l2_mae_optimization()
    results['L2'] = {
        'best_mae': l2_best.score if l2_best else float('inf'),
        'best_config': l2_best.config if l2_best else None,
        'optimizer': l2_optimizer
    }

    # 3. L1ä¸“ç”¨MAEä¼˜åŒ–
    print("\n3. è¿è¡ŒL1ä¸“ç”¨MAEä¼˜åŒ–...")
    l1_optimizer, l1_best = run_l1_mae_optimization()
    results['L1'] = {
        'best_mae': l1_best.score if l1_best else float('inf'),
        'best_config': l1_best.config if l1_best else None,
        'optimizer': l1_optimizer
    }

    # 4. å¯¹æ¯”åˆ†æ
    print("\n" + "="*60)
    print("HPL vs L2 vs L1 MAEå¯¹æ¯”ç»“æœ")
    print("="*60)

    hpl_mae = results['HPL']['best_mae']
    l2_mae = results['L2']['best_mae']
    l1_mae = results['L1']['best_mae']

    # æ‰¾å‡ºæœ€ä½³è¡¨ç°
    best_mae = min(hpl_mae, l2_mae, l1_mae)

    print(f"{'æŸå¤±å‡½æ•°':<12} {'æœ€ä½³MAE':<12} {'ç›¸å¯¹è¡¨ç°':<12} {'æ”¹è¿›å¹…åº¦':<12}")
    print("-" * 50)
    print(f"{'HPL':<12} {hpl_mae:<12.4f} {'å¯¹æ¯”':<12} {((l2_mae - hpl_mae) / l2_mae * 100 if l2_mae != 0 else 0):<12.2f}%")
    print(f"{'L2':<12} {l2_mae:<12.4f} {'åŸºå‡†':<12} {'0.00':<12}%")
    print(f"{'L1(MAE)':<12} {l1_mae:<12.4f} {'å¯¹æ¯”':<12} {((l2_mae - l1_mae) / l2_mae * 100 if l2_mae != 0 else 0):<12.2f}%")

    # åˆ†æç»“æœ
    if best_mae == hpl_mae:
        print(f"\nğŸ‰ HPLæŸå¤±å‡½æ•°åœ¨MAEæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä¼˜!")
        if l2_mae != float('inf'):
            improvement = (l2_mae - hpl_mae) / l2_mae * 100
            print(f"   ç›¸æ¯”L2æ”¹è¿›äº†: {improvement:.2f}%")
        if l1_mae != float('inf'):
            improvement = (l1_mae - hpl_mae) / l1_mae * 100
            print(f"   ç›¸æ¯”L1æ”¹è¿›äº†: {improvement:.2f}%")
        winner = 'HPL'
    elif best_mae == l1_mae:
        print(f"\nğŸ¯ L1(MAE)æŸå¤±å‡½æ•°åœ¨MAEæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä¼˜!")
        print(f"   è¿™æ˜¯ç¬¦åˆé¢„æœŸçš„ï¼Œå› ä¸ºL1æŸå¤±ç›´æ¥ä¼˜åŒ–MAE")
        if l2_mae != float('inf'):
            improvement = (l2_mae - l1_mae) / l2_mae * 100
            print(f"   ç›¸æ¯”L2æ”¹è¿›äº†: {improvement:.2f}%")
        if hpl_mae != float('inf'):
            improvement = (hpl_mae - l1_mae) / hpl_mae * 100
            print(f"   ç›¸æ¯”HPLæ”¹è¿›äº†: {improvement:.2f}%")
        winner = 'L1'
    elif best_mae == l2_mae:
        print(f"\nğŸ“Š L2æŸå¤±å‡½æ•°åœ¨MAEæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä¼˜")
        winner = 'L2'

    else:
        print(f"\nğŸ¤ å¤šç§æŸå¤±å‡½æ•°è¡¨ç°ç›¸å½“")
        winner = 'Tie'

    # 5. è¯¦ç»†é…ç½®å¯¹æ¯”
    print(f"\næœ€ä½³é…ç½®å¯¹æ¯”:")
    if results['HPL']['best_config']:
        hpl_config = results['HPL']['best_config']
        print(f"\nHPLæœ€ä½³é…ç½®:")
        print(f"  å­¦ä¹ ç‡: {hpl_config['learning_rate']:.4f}")
        print(f"  å› å­æ•°: {hpl_config['latent_factors']}")
        print(f"  æ­£åˆ™åŒ–: {hpl_config['lambda_reg']:.6f}")
        print(f"  delta1: {hpl_config['delta1']:.3f}")
        print(f"  delta2: {hpl_config['delta2']:.3f}")
        print(f"  l_max: {hpl_config.get('l_max', 4.0):.2f}")
        print(f"  c_sigmoid: {hpl_config.get('c_sigmoid', 1.0):.2f}")

    if results['L2']['best_config']:
        l2_config = results['L2']['best_config']
        print(f"\nL2æœ€ä½³é…ç½®:")
        print(f"  å­¦ä¹ ç‡: {l2_config['learning_rate']:.4f}")
        print(f"  å› å­æ•°: {l2_config['latent_factors']}")
        print(f"  æ­£åˆ™åŒ–: {l2_config['lambda_reg']:.6f}")

    if results['L1']['best_config']:
        l1_config = results['L1']['best_config']
        print(f"\nL1æœ€ä½³é…ç½®:")
        print(f"  å­¦ä¹ ç‡: {l1_config['learning_rate']:.4f}")
        print(f"  å› å­æ•°: {l1_config['latent_factors']}")
        print(f"  æ­£åˆ™åŒ–: {l1_config['lambda_reg']:.6f}")
        print(f"  L1_epsilon: {l1_config.get('l1_epsilon', 1e-8):.2e}")

    # 6. ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_results = {
        'winner': winner,
        'hpl_mae': hpl_mae,
        'l2_mae': l2_mae,
        'l1_mae': l1_mae,
        'best_mae': best_mae,
        'hpl_improvement_over_l2': (l2_mae - hpl_mae) / l2_mae * 100 if l2_mae != 0 else 0,
        'l1_improvement_over_l2': (l2_mae - l1_mae) / l2_mae * 100 if l2_mae != 0 else 0,
        'hpl_config': results['HPL']['best_config'],
        'l2_config': results['L2']['best_config'],
        'l1_config': results['L1']['best_config']
    }

    # ä¿å­˜è¯¦ç»†ç»“æœ
    save_comparison_results(comparison_results, "hpl_vs_l2_vs_l1_mae_comparison.json")

    return results


def save_comparison_results(results, filename):
    """ä¿å­˜å¯¹æ¯”ç»“æœ"""
    try:
        def convert_numpy_types(obj):
            if isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            elif hasattr(obj, 'item'):
                return obj.item()
            elif hasattr(obj, 'tolist'):
                return obj.tolist()
            else:
                return obj

        clean_results = convert_numpy_types(results)
        clean_results['timestamp'] = time.strftime("%Y-%m-%d %H:%M:%S")
        clean_results['evaluation_metric'] = 'MAE'
        clean_results['description'] = 'HPL vs L2 vs L1 losses optimized for MAE metric'

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(clean_results, f, indent=2, ensure_ascii=False)

        print(f"\nå¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {filename}")

    except Exception as e:
        print(f"ä¿å­˜å¯¹æ¯”ç»“æœå¤±è´¥: {e}")


def evaluate_final_performance(data_manager, best_configs):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ€§èƒ½"""
    print("\n" + "="*60)
    print("æµ‹è¯•é›†æœ€ç»ˆæ€§èƒ½è¯„ä¼°")
    print("="*60)

    mae_metric = MAE()
    rmse_metric = RMSE()

    test_results = {}
    train_data, val_data, test_data = data_manager.get_splits()

    loss_functions = {
        'HPL': lambda config: HybridPiecewiseLoss(
            delta1=config['delta1'],
            delta2=config['delta2'],
            l_max=config.get('l_max', 4.0),
            c_sigmoid=config.get('c_sigmoid', 1.0)
        ),
        'L2': lambda config: L2Loss(),
        'L1': lambda config: L1Loss(epsilon=config.get('l1_epsilon', 1e-8))
    }

    for loss_name, config in best_configs.items():
        if config is None:
            continue

        print(f"\nè¯„ä¼° {loss_name} åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½...")

        try:
            # åˆ›å»ºæ¨¡å‹
            loss_function = loss_functions[loss_name](config)
            regularizer = L2Regularizer(lambda_reg=config['lambda_reg'])

            model = MatrixFactorizationSGD(
                n_users=data_manager.get_statistics()['n_users'],
                n_items=data_manager.get_statistics()['n_items'],
                n_factors=config['latent_factors'],
                learning_rate=config['learning_rate'],
                regularizer=regularizer,
                loss_function=loss_function,
                use_bias=True,
                global_mean=data_manager.global_mean or 0.0
            )

            # åˆå§‹åŒ–å¹¶è®­ç»ƒ
            initializer = NormalInitializer(mean=0.0, std=0.01)
            model.initialize_parameters(initializer)

            # åœ¨è®­ç»ƒ+éªŒè¯é›†ä¸Šè®­ç»ƒ
            combined_data = np.vstack([train_data, val_data])
            model.fit(
                train_data=combined_data,
                val_data=None,  # ä¸ä½¿ç”¨éªŒè¯é›†ï¼ˆå› ä¸ºè¦åœ¨æµ‹è¯•é›†è¯„ä¼°ï¼‰
                n_epochs=50,
                verbose=0
            )

            # æµ‹è¯•é›†é¢„æµ‹
            test_predictions = model.predict(
                test_data[:, 0].astype(int),
                test_data[:, 1].astype(int)
            )

            # è¿˜åŸåˆ°åŸå§‹å°ºåº¦
            if data_manager.global_mean is not None:
                test_predictions += data_manager.global_mean
                test_targets = test_data[:, 2] + data_manager.global_mean
            else:
                test_targets = test_data[:, 2]

            # è®¡ç®—æŒ‡æ ‡
            test_mae = mae_metric.calculate(test_targets, test_predictions)
            test_rmse = rmse_metric.calculate(test_targets, test_predictions)

            test_results[loss_name] = {
                'mae': test_mae,
                'rmse': test_rmse,
                'config': config
            }

            print(f"{loss_name} æµ‹è¯•ç»“æœ:")
            print(f"  MAE: {test_mae:.4f}")
            print(f"  RMSE: {test_rmse:.4f}")

        except Exception as e:
            print(f"{loss_name} æµ‹è¯•è¯„ä¼°å¤±è´¥: {e}")
            test_results[loss_name] = None

    # æ±‡æ€»æµ‹è¯•ç»“æœ
    print(f"\n{'æŸå¤±å‡½æ•°':<12} {'æµ‹è¯•MAE':<12} {'æµ‹è¯•RMSE':<12}")
    print("-" * 40)
    for loss_name, result in test_results.items():
        if result:
            print(f"{loss_name:<12} {result['mae']:<12.4f} {result['rmse']:<12.4f}")

    return test_results


def main():
    """HPLä¸“ç”¨MAEä¼˜åŒ–ä¸»å‡½æ•°"""
    try:
        print("HPLæŸå¤±å‡½æ•°MAEæŒ‡æ ‡æ·±åº¦ä¼˜åŒ–å®éªŒ")
        print("="*60)

        # 1. åŸºç¡€HPL MAEä¼˜åŒ–å®éªŒ
        print("\n1. è¿è¡ŒHPL vs L2 vs L1 MAEå¯¹æ¯”...")
        comparison_results = run_mae_comparison()

        # 2. æå–æœ€ä½³é…ç½®
        best_configs = {
            'HPL': comparison_results['HPL']['best_config'],
            'L2': comparison_results['L2']['best_config'],
            'L1': comparison_results['L1']['best_config']
        }

        # 3. æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°
        print("\n2. è¿è¡Œæµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°...")
        data_manager = setup_improved_data()
        test_results = evaluate_final_performance(data_manager, best_configs)

        # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        print("\n" + "="*60)
        print("MAEä¼˜åŒ–å®éªŒæœ€ç»ˆæ€»ç»“")
        print("="*60)

        print("\néªŒè¯é›†æœ€ä½³MAEç»“æœ:")
        if comparison_results['HPL']['best_mae'] != float('inf'):
            print(f"  HPL: {comparison_results['HPL']['best_mae']:.4f}")
        if comparison_results['L2']['best_mae'] != float('inf'):
            print(f"  L2:  {comparison_results['L2']['best_mae']:.4f}")
        if comparison_results['L1']['best_mae'] != float('inf'):
            print(f"  L1:  {comparison_results['L1']['best_mae']:.4f}")

        print("\næµ‹è¯•é›†æœ€ç»ˆç»“æœ:")
        for loss_name, result in test_results.items():
            if result:
                print(f"  {loss_name}: MAE={result['mae']:.4f}, RMSE={result['rmse']:.4f}")

        # 5. å…³é”®å‘ç°å’Œå»ºè®®
        print("\n" + "="*60)
        print("å…³é”®å‘ç°å’Œå»ºè®®")
        print("="*60)

        best_val_mae = min(
            comparison_results['HPL']['best_mae'],
            comparison_results['L2']['best_mae'],
            comparison_results['L1']['best_mae']
        )

        if best_val_mae == comparison_results['L1']['best_mae']:
            print("âœ… L1æŸå¤±åœ¨MAEä¼˜åŒ–ä¸­è¡¨ç°æœ€ä½³ï¼Œè¿™ç¬¦åˆç†è®ºé¢„æœŸ")
            print("   - L1æŸå¤±ç›´æ¥ä¼˜åŒ–MAEç›®æ ‡ï¼Œåº”è¯¥æ˜¯æœ€ä¼˜é€‰æ‹©")
            print("   - å»ºè®®åœ¨æ³¨é‡MAEæŒ‡æ ‡çš„åº”ç”¨ä¸­ä½¿ç”¨L1æŸå¤±")
        elif best_val_mae == comparison_results['HPL']['best_mae']:
            print("ğŸ‰ HPLæŸå¤±åœ¨MAEä¼˜åŒ–ä¸­è¶…è¶Šäº†L1æŸå¤±!")
            print("   - è¿™è¡¨æ˜HPLçš„åˆ†æ®µç­–ç•¥å¯¹MAEä¼˜åŒ–ä¹Ÿæœ‰å¸®åŠ©")
            print("   - HPLå¯èƒ½åœ¨å¤„ç†ä¸åŒè¯¯å·®èŒƒå›´æ—¶æ¯”ç®€å•L1æ›´æœ‰æ•ˆ")
        else:
            print("ğŸ“Š L2æŸå¤±åœ¨MAEä¼˜åŒ–ä¸­è¡¨ç°æ„å¤–åœ°å¥½")
            print("   - å¯èƒ½éœ€è¦è°ƒæ•´å…¶ä»–æŸå¤±å‡½æ•°çš„è¶…å‚æ•°èŒƒå›´")

        # 6. é…ç½®å»ºè®®
        print("\né…ç½®å»ºè®®:")
        for loss_name, config in best_configs.items():
            if config:
                print(f"\n{loss_name}æœ€ä½³é…ç½®:")
                print(f"  å­¦ä¹ ç‡: {config['learning_rate']:.4f}")
                print(f"  æ½œåœ¨å› å­: {config['latent_factors']}")
                print(f"  æ­£åˆ™åŒ–: {config['lambda_reg']:.6f}")
                if loss_name == 'HPL':
                    print(f"  delta1: {config['delta1']:.3f}")
                    print(f"  delta2: {config['delta2']:.3f}")
                elif loss_name == 'L1':
                    print(f"  epsilon: {config.get('l1_epsilon', 1e-8):.2e}")

        print("\nå®éªŒå®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°JSONæ–‡ä»¶ä¸­ã€‚")

        return True

    except Exception as e:
        print(f"MAEä¼˜åŒ–å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
