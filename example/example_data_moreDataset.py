# é…ç½®åŒ–æ•°æ®é›†ç®¡ç† - ç®€å•æ˜“ç”¨ç‰ˆæœ¬


import logging
import sys

# æ·»åŠ dataæ¨¡å—è·¯å¾„ï¼ˆæ ¹æ®ä½ çš„æ–‡ä»¶ç»“æ„è°ƒæ•´ï¼‰
sys.path.append('data')

import logging
from pathlib import Path
from data.loader import DatasetLoader
from data.dataset import MovieLens100K
from data.data_manager import DataManager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

# =====================================================
# ç¬¬ä¸€æ­¥ï¼šæ•°æ®é›†é…ç½®
# =====================================================

# æ•°æ®é›†é…ç½®å­—å…¸ - åœ¨è¿™é‡Œæ·»åŠ ä½ çš„æ‰€æœ‰æ•°æ®é›†
DATASETS_CONFIG = {
    'M100K_test': {
        'raw_data_path': 'dataset/20201202M100K_data_all_random.txt',  # æ›´æ–°ä¸ºç›¸å¯¹è·¯å¾„
        'preprocessed_dir': './preprocessed_data/M100K_test',
        'description': 'M100K_testè¯„åˆ†æ•°æ®é›†'
    },
    'Netflix': {
        'raw_data_path': 'dataset/20201202NetFlix_data_all_random.txt',  # æ›´æ–°ä¸ºç›¸å¯¹è·¯å¾„
        'preprocessed_dir': './preprocessed_data/Netflix',
        'description': 'Netflixè¯„åˆ†æ•°æ®é›†'
    },
    'MovieLens1M': {
        'raw_data_path': 'dataset/moive1M20221009randombigthan20bigthan20userbigandeq300.txt',
        'preprocessed_dir': './preprocessed_data/MovieLens1M',
        'description': 'MovieLens 1Mè¯„åˆ†æ•°æ®é›†'
    },
    'AmazonMI': {
        'raw_data_path': 'dataset/Amazon_Musical_Instruments20220608random.txt',
        'preprocessed_dir': './preprocessed_data/AmazonMI',
        'description': 'äºšé©¬é€Šä¹å™¨è¯„åˆ†æ•°æ®é›†'
    },
    'CiaoDVD': {
        'raw_data_path': 'dataset/ciaodvd20220530random.txt',
        'preprocessed_dir': './preprocessed_data/CiaoDVD',
        'description': 'CiaoDVDè¯„åˆ†æ•°æ®é›†'
    },
    'Epinions': {
        'raw_data_path': 'dataset/Epinions20220531random.txt',
        'preprocessed_dir': './preprocessed_data/Epinions',
        'description': 'Epinionsè¯„åˆ†æ•°æ®é›†'
    },
    'FilmTrust': {
        'raw_data_path': 'dataset/flimtrust20220604random.txt',
        'preprocessed_dir': './preprocessed_data/FilmTrust',
        'description': 'FilmTrustè¯„åˆ†æ•°æ®é›†'
    },
    'MovieTweetings': {
        'raw_data_path': 'dataset/moivetweetings20220511random.txt',
        'preprocessed_dir': './preprocessed_data/MovieTweetings',
        'description': 'MovieTweetingsè¯„åˆ†æ•°æ®é›†'
    }
}


# =====================================================
# ç¬¬äºŒæ­¥ï¼šæ³¨å†Œæ‰€æœ‰æ•°æ®é›†
# =====================================================

def register_all_datasets():
    """ä¸€æ¬¡æ€§æ³¨å†Œæ‰€æœ‰æ•°æ®é›†"""
    for dataset_name in DATASETS_CONFIG.keys():
        DatasetLoader.register_dataset(dataset_name, MovieLens100K)
    print(f"âœ“ å·²æ³¨å†Œ {len(DATASETS_CONFIG)} ä¸ªæ•°æ®é›†")

# =====================================================
# ç¬¬ä¸‰æ­¥ï¼šæ ¸å¿ƒç®¡ç†å‡½æ•°
# =====================================================

def preprocess_dataset(dataset_name):
    """
    é¢„å¤„ç†æŒ‡å®šæ•°æ®é›†

    Args:
        dataset_name: æ•°æ®é›†åç§°ï¼ˆåœ¨DATASETS_CONFIGä¸­å®šä¹‰ï¼‰
    """
    if dataset_name not in DATASETS_CONFIG:
        print(f"âŒ é”™è¯¯ï¼šæœªçŸ¥æ•°æ®é›† '{dataset_name}'")
        print(f"å¯ç”¨æ•°æ®é›†ï¼š{list(DATASETS_CONFIG.keys())}")
        return False

    config = DATASETS_CONFIG[dataset_name]
    print(f"ğŸ”„ å¼€å§‹é¢„å¤„ç†ï¼š{config['description']}")

    try:
        # æ£€æŸ¥åŸå§‹æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(config['raw_data_path']).exists():
            print(f"âŒ åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{config['raw_data_path']}")
            return False

        # åˆ›å»ºæ•°æ®ç®¡ç†å™¨å¹¶é¢„å¤„ç†
        data_manager = DataManager()
        data_manager.load_dataset(dataset_name, config['raw_data_path'])
        data_manager.preprocess()

        # ä¿å­˜é¢„å¤„ç†ç»“æœ
        data_manager.save_preprocessed_data(config['preprocessed_dir'])

        print(f"âœ… é¢„å¤„ç†å®Œæˆï¼š{config['preprocessed_dir']}")
        return True

    except Exception as e:
        print(f"âŒ é¢„å¤„ç†å¤±è´¥ï¼š{e}")
        return False

def load_dataset(dataset_name):
    """
    åŠ è½½æ•°æ®é›†ï¼ˆæ™ºèƒ½åŠ è½½ï¼šä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†æ–‡ä»¶ï¼Œä¸å­˜åœ¨åˆ™è‡ªåŠ¨é¢„å¤„ç†ï¼‰

    Args:
        dataset_name: æ•°æ®é›†åç§°

    Returns:
        DataManagerå¯¹è±¡ï¼Œå¤±è´¥è¿”å›None
    """
    if dataset_name not in DATASETS_CONFIG:
        print(f"âŒ é”™è¯¯ï¼šæœªçŸ¥æ•°æ®é›† '{dataset_name}'")
        print(f"å¯ç”¨æ•°æ®é›†ï¼š{list(DATASETS_CONFIG.keys())}")
        return None

    config = DATASETS_CONFIG[dataset_name]
    preprocessed_dir = config['preprocessed_dir']

    # æ£€æŸ¥é¢„å¤„ç†æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if Path(preprocessed_dir).exists():
        try:
            print(f"ğŸ“ å‘ç°é¢„å¤„ç†æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½ï¼š{config['description']}")
            data_manager = DataManager()
            data_manager.load_preprocessed_data(preprocessed_dir)
            print(f"âœ… åŠ è½½æˆåŠŸï¼š{dataset_name}")
            return data_manager
        except Exception as e:
            print(f"âš ï¸  é¢„å¤„ç†æ–‡ä»¶æŸåï¼Œé‡æ–°é¢„å¤„ç†ï¼š{e}")

    # é¢„å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨æˆ–æŸåï¼Œé‡æ–°é¢„å¤„ç†
    print(f"ğŸ”„ é¢„å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹é¢„å¤„ç†ï¼š{config['description']}")
    if preprocess_dataset(dataset_name):
        return load_dataset(dataset_name)  # é€’å½’è°ƒç”¨ï¼ŒåŠ è½½åˆšé¢„å¤„ç†çš„æ•°æ®
    else:
        return None

def list_datasets():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
    print("\nğŸ“‹ å¯ç”¨æ•°æ®é›†åˆ—è¡¨ï¼š")
    print("=" * 60)

    for i, (name, config) in enumerate(DATASETS_CONFIG.items(), 1):
        status = "âœ… å·²é¢„å¤„ç†" if Path(config['preprocessed_dir']).exists() else "â³ æœªé¢„å¤„ç†"
        print(f"{i:2d}. {name:<15} - {config['description']} [{status}]")

    print("=" * 60)

def remove_preprocessed_data(dataset_name):
    """åˆ é™¤æŒ‡å®šæ•°æ®é›†çš„é¢„å¤„ç†æ–‡ä»¶"""
    if dataset_name not in DATASETS_CONFIG:
        print(f"âŒ é”™è¯¯ï¼šæœªçŸ¥æ•°æ®é›† '{dataset_name}'")
        return False

    import shutil
    preprocessed_dir = DATASETS_CONFIG[dataset_name]['preprocessed_dir']

    if Path(preprocessed_dir).exists():
        shutil.rmtree(preprocessed_dir)
        print(f"ğŸ—‘ï¸  å·²åˆ é™¤é¢„å¤„ç†æ–‡ä»¶ï¼š{dataset_name}")
        return True
    else:
        print(f"ğŸ“ é¢„å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨ï¼š{dataset_name}")
        return False

# =====================================================
# ç¬¬å››æ­¥ï¼šä¾¿æ·ä½¿ç”¨å‡½æ•°
# =====================================================

def quick_start(dataset_name):
    """
    ä¸€é”®å¯åŠ¨ï¼šæ³¨å†Œâ†’åŠ è½½â†’è¿”å›å¯ç”¨çš„æ•°æ®ç®¡ç†å™¨

    Args:
        dataset_name: æ•°æ®é›†åç§°

    Returns:
        (data_manager, train_iterator, val_iterator, test_iterator)
    """
    print(f"ğŸš€ å¿«é€Ÿå¯åŠ¨æ•°æ®é›†ï¼š{dataset_name}")

    # æ³¨å†Œæ•°æ®é›†
    register_all_datasets()

    # åŠ è½½æ•°æ®é›†
    data_manager = load_dataset(dataset_name)
    if data_manager is None:
        return None, None, None, None

    # åˆ›å»ºè¿­ä»£å™¨
    train_iterator = data_manager.get_batch_iterator('train')
    val_iterator = data_manager.get_batch_iterator('val')
    test_iterator = data_manager.get_batch_iterator('test')

    print(f"âœ… æ•°æ®é›† {dataset_name} å‡†å¤‡å°±ç»ªï¼")
    return data_manager, train_iterator, val_iterator, test_iterator

def get_dataset_info(dataset_name):
    """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    data_manager = load_dataset(dataset_name)
    if data_manager:
        stats = data_manager.get_statistics()
        print(f"\nğŸ“Š æ•°æ®é›† {dataset_name} ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"   ç”¨æˆ·æ•°ï¼š{stats['n_users']}")
        print(f"   ç‰©å“æ•°ï¼š{stats['n_items']}")
        print(f"   è¯„åˆ†æ•°ï¼š{stats['n_total']}")
        print(f"   å¹³å‡è¯„åˆ†ï¼š{stats['rating_mean']:.2f}")
        print(f"   ç¨€ç–åº¦ï¼š{stats['sparsity']:.2%}")
        return stats
    return None

# =====================================================
# ç¬¬äº”æ­¥ï¼šä½¿ç”¨ç¤ºä¾‹
# =====================================================

def main():
    """ä¸»å‡½æ•° - å±•ç¤ºæ‰€æœ‰åŠŸèƒ½"""
    print("ğŸ¯ é…ç½®åŒ–æ•°æ®é›†ç®¡ç†ç³»ç»Ÿ")
    print("=" * 50)

    # 1. æ³¨å†Œæ‰€æœ‰æ•°æ®é›†
    register_all_datasets()

    # 2. åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†
    list_datasets()

    # 3. æ¼”ç¤ºåŠ è½½ä¸åŒæ•°æ®é›†
    test_datasets = ['Netflix', 'Amazon_Musical']  # é€‰æ‹©è¦æµ‹è¯•çš„æ•°æ®é›†

    for dataset_name in test_datasets:
        print(f"\nğŸ” æµ‹è¯•æ•°æ®é›†ï¼š{dataset_name}")
        print("-" * 30)

        # æ–¹å¼1ï¼šé€æ­¥æ“ä½œ
        data_manager = load_dataset(dataset_name)
        if data_manager:
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            get_dataset_info(dataset_name)

            # è·å–è®­ç»ƒè¿­ä»£å™¨
            train_iterator = data_manager.get_batch_iterator('train')
            print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°ï¼š{len(train_iterator)}")

            # ç¤ºä¾‹ï¼šè¿­ä»£ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
            first_batch = next(iter(train_iterator))
            print(f"   ç¬¬ä¸€æ‰¹æ¬¡å¤§å°ï¼š{len(first_batch[0])}")

def quick_usage_examples():
    """å¿«é€Ÿä½¿ç”¨ç¤ºä¾‹"""
    print("\n" + "=" * 50)
    print("ğŸš€ å¿«é€Ÿä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)

    # ç¤ºä¾‹1ï¼šä¸€é”®å¯åŠ¨
    print("\n1ï¸âƒ£ ä¸€é”®å¯åŠ¨æ•°æ®é›†ï¼š")
    dm, train_iter, val_iter, test_iter = quick_start('M100K_test')
    if dm:
        print(f"âœ… è®­ç»ƒæ‰¹æ¬¡ï¼š{len(train_iter)}, éªŒè¯æ‰¹æ¬¡ï¼š{len(val_iter)}, æµ‹è¯•æ‰¹æ¬¡ï¼š{len(test_iter)}")

    # ç¤ºä¾‹2ï¼šåˆ‡æ¢æ•°æ®é›†
    print("\n2ï¸âƒ£ åˆ‡æ¢åˆ°ä¸åŒæ•°æ®é›†ï¼š")
    for dataset in ['M100K_test', 'amazon_musical_instruments']:
        print(f"   åˆ‡æ¢åˆ°ï¼š{dataset}")
        dm = load_dataset(dataset)
        if dm:
            stats = dm.get_statistics()
            print(f"   âœ… ç”¨æˆ·æ•°ï¼š{stats['n_users']}, ç‰©å“æ•°ï¼š{stats['n_items']}")

    # ç¤ºä¾‹3ï¼šæ‰¹é‡é¢„å¤„ç†
    print("\n3ï¸âƒ£ æ‰¹é‡é¢„å¤„ç†æ‰€æœ‰æ•°æ®é›†ï¼š")
    for dataset_name in DATASETS_CONFIG.keys():
        print(f"   é¢„å¤„ç†ï¼š{dataset_name}")
        success = preprocess_dataset(dataset_name)
        print(f"   ç»“æœï¼š{'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")

# =====================================================
# å®é™…ä½¿ç”¨æ¨¡æ¿
# =====================================================

def your_training_code():
    """
    ä½ çš„è®­ç»ƒä»£ç æ¨¡æ¿
    """
    print("\nğŸ“ è®­ç»ƒä»£ç ç¤ºä¾‹ï¼š")

    # 1. åŠ è½½æ•°æ®é›†
    dataset_name = 'M100K_test'  # ä¿®æ”¹è¿™é‡Œåˆ‡æ¢æ•°æ®é›†
    data_manager = load_dataset(dataset_name)

    if data_manager is None:
        print("âŒ æ•°æ®é›†åŠ è½½å¤±è´¥")
        return

    # 2. è·å–æ•°æ®è¿­ä»£å™¨
    train_iterator = data_manager.get_batch_iterator('train', batch_size=128)
    val_iterator = data_manager.get_batch_iterator('val', batch_size=128)
    test_iterator = data_manager.get_batch_iterator('test', batch_size=128)

    # 3. å¼€å§‹è®­ç»ƒï¼ˆç¤ºä¾‹ï¼‰
    print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒï¼Œä½¿ç”¨æ•°æ®é›†ï¼š{dataset_name}")
    print(f"   è®­ç»ƒæ‰¹æ¬¡ï¼š{len(train_iterator)}")
    print(f"   éªŒè¯æ‰¹æ¬¡ï¼š{len(val_iterator)}")
    print(f"   æµ‹è¯•æ‰¹æ¬¡ï¼š{len(test_iterator)}")

    # 4. è®­ç»ƒå¾ªç¯ç¤ºä¾‹
    for epoch in range(3):  # ç¤ºä¾‹ï¼šè®­ç»ƒ3ä¸ªepoch
        print(f"   Epoch {epoch + 1}:")

        # è®­ç»ƒ
        for i, (user_ids, item_ids, ratings) in enumerate(train_iterator):
            # è¿™é‡Œæ”¾ä½ çš„æ¨¡å‹è®­ç»ƒä»£ç 
            # model.train_step(user_ids, item_ids, ratings)
            if i >= 2:  # åªæ¼”ç¤ºå‰3ä¸ªæ‰¹æ¬¡
                break

        # éªŒè¯
        for i, (user_ids, item_ids, ratings) in enumerate(val_iterator):
            # è¿™é‡Œæ”¾ä½ çš„æ¨¡å‹éªŒè¯ä»£ç 
            # loss = model.validate_step(user_ids, item_ids, ratings)
            if i >= 1:  # åªæ¼”ç¤ºå‰2ä¸ªæ‰¹æ¬¡
                break

        print(f"     âœ… Epoch {epoch + 1} å®Œæˆ")

if __name__ == "__main__":
    # è¿è¡Œä¸»ç¨‹åº
    main()

    # å¿«é€Ÿä½¿ç”¨ç¤ºä¾‹
    # quick_usage_examples()

    # è®­ç»ƒä»£ç ç¤ºä¾‹
    # your_training_code()

    print("\n" + "=" * 50)
    print("ğŸ“ ä½¿ç”¨è¯´æ˜ï¼š")
    print("1. åœ¨ DATASETS_CONFIG ä¸­æ·»åŠ ä½ çš„æ•°æ®é›†é…ç½®")
    print("2. ä½¿ç”¨ load_dataset('æ•°æ®é›†å') åŠ è½½æ•°æ®")
    print("3. ä½¿ç”¨ quick_start('æ•°æ®é›†å') ä¸€é”®å¯åŠ¨")
    print("4. æ•°æ®ä¼šè‡ªåŠ¨é¢„å¤„ç†å’Œç¼“å­˜ï¼Œä¸‹æ¬¡åŠ è½½æ›´å¿«")
    print("=" * 50)
