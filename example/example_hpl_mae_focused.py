    print("-" * 50)
    print(f"{'HPL':<12} {hpl_mae:<12.4f} {'å¯¹æ¯”':<12} {((l2_mae - hpl_mae) / l2_mae * 100 if l2_mae != 0 else 0):<12.2f}%")
    print(f"{'L2':<12} {l2_mae:<12.4f} {'åŸºå‡†':<12} {'0.00':<12}%")
    print(f"{'L1(MAE)':<12} {l1_mae:<12.4f} {'å¯¹æ¯”':<12} {((l2_mae - l1_mae) / l2_mae * 100 if l2_mae != 0 else 0):<12.2f}%")
    
    # åˆ†æžç»“æžœ
    if best_mae == hpl_mae:
        print(f"\nðŸŽ‰ HPLæŸå¤±å‡½æ•°åœ¨MAEæŒ‡æ ‡ä¸Šè¡¨çŽ°æœ€ä¼˜!")
        if l2_mae != float('inf'):
            improvement = (l2_mae - hpl_mae) / l2_mae * 100
            print(f"   ç›¸æ¯”L2æ”¹è¿›äº†: {improvement:.2f}%")
        if l1_mae != float('inf'):
            improvement = (l1_mae - hpl_mae) / l1_mae * 100
            print(f"   ç›¸æ¯”L1æ”¹è¿›äº†: {improvement:.2f}%")
        winner = 'HPL'
    elif best_mae == l1_mae:
        print(f"\nðŸŽ¯ L1(MAE)æŸå¤±å‡½æ•°åœ¨MAEæŒ‡æ ‡ä¸Šè¡¨çŽ°æœ€ä¼˜!")
        print(f"   è¿™æ˜¯ç¬¦åˆé¢„æœŸçš„ï¼Œå› ä¸ºL1æŸå¤±ç›´æŽ¥ä¼˜åŒ–MAE")
        if l2_mae != float('inf'):
            improvement = (l2_mae - l1_mae) / l2_mae * 100
            print(f"   ç›¸æ¯”L2æ”¹è¿›äº†: {improvement:.2f}%")
        if hpl_mae != float('inf'):
            improvement = (hpl_mae - l1_mae) / hpl_mae * 100
            print(f"   ç›¸æ¯”HPLæ”¹è¿›äº†: {improvement:.2f}%")
        winner = 'L1'
    elif best_mae == l2_mae:
        print(f"\nðŸ“Š L2æŸå¤±å‡½æ•°åœ¨MAEæŒ‡æ ‡ä¸Šè¡¨çŽ°æœ€ä¼˜")
        winner = 'L2'
    else:
        print(f"\nðŸ¤ å¤šç§æŸå¤±å‡½æ•°è¡¨çŽ°ç›¸å½“")
        winner = 'Tie'
    
    # 5. è¯¦ç»†é…ç½®å¯¹æ¯”
    print(f"\næœ€ä½³é…ç½®å¯¹æ¯”:")
    if results['HPL']['best_config']:
        hpl_config = results['HPL']['best_config']
        print(f"\nHPLæœ€ä½³é…ç½®:")
        print(f"  å­¦ä¹ çŽ‡: {hpl_config['learning_rate']:.4f}")
        print(f"  å› å­æ•°: {hpl_config['latent_factors']}")
        print(f"  æ­£åˆ™åŒ–: {hpl_config['lambda_reg']:.6f}")
        print(f"  delta1: {hpl_config['delta1']:.3f}")
        print(f"  delta2: {hpl_config['delta2']:.3f}")
        print(f"  l_max: {hpl_config.get('l_max', 4.0):.2f}")
        print(f"  c_sigmoid: {hpl_config.get('c_sigmoid', 1.0):.2f}")
    
    if results['L2']['best_config']:
        l2_config = results['L2']['best_config']
        print(f"\nL2æœ€ä½³é…ç½®:")
        print(f"  å­¦ä¹ çŽ‡: {l2_config['learning_rate']:.4f}")
        print(f"  å› å­æ•°: {l2_config['latent_factors']}")
        print(f"  æ­£åˆ™åŒ–: {l2_config['lambda_reg']:.6f}")
    
    if results['L1']['best_config']:
        l1_config = results['L1']['best_config']
        print(f"\nL1æœ€ä½³é…ç½®:")
        print(f"  å­¦ä¹ çŽ‡: {l1_config['learning_rate']:.4f}")
        print(f"  å› å­æ•°: {l1_config['latent_factors']}")
        print(f"  æ­£åˆ™åŒ–: {l1_config['lambda_reg']:.6f}")
        print(f"  L1_epsilon: {l1_config.get('l1_epsilon', 1e-8):.2e}")
    
    # 6. ä¿å­˜å¯¹æ¯”ç»“æžœ
    comparison_results = {
        'winner': winner,
        'hpl_mae': hpl_mae,
        'l2_mae': l2_mae,
        'l1_mae': l1_mae,
        'best_mae': best_mae,
        'hpl_improvement_over_l2': (l2_mae - hpl_mae) / l2_mae * 100 if l2_mae != 0 else 0,
        'l1_improvement_over_l2': (l2_mae - l1_mae) / l2_mae * 100 if l2_mae != 0 else 0,
        'hpl_config': results['HPL']['best_config'],
        'l2_config': results['L2']['best_config'],
        'l1_config': results['L1']['best_config']
    }
    
    # ä¿å­˜è¯¦ç»†ç»“æžœ
    save_comparison_results(comparison_results, "hpl_vs_l2_vs_l1_mae_comparison.json")
    
    return results


def save_comparison_results(results, filename):
    """ä¿å­˜å¯¹æ¯”ç»“æžœ"""
    try:
        def convert_numpy_types(obj):
            if isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            elif hasattr(obj, 'item'):
                return obj.item()
            elif hasattr(obj, 'tolist'):
                return obj.tolist()
            else:
                return obj
        
        clean_results = convert_numpy_types(results)
        clean_results['timestamp'] = time.strftime("%Y-%m-%d %H:%M:%S")
        clean_results['evaluation_metric'] = 'MAE'
        clean_results['description'] = 'HPL vs L2 vs L1 losses optimized for MAE metric'
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(clean_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nå¯¹æ¯”ç»“æžœå·²ä¿å­˜åˆ°: {filename}")
        
    except Exception as e:
        print(f"ä¿å­˜å¯¹æ¯”ç»“æžœå¤±è´¥: {e}")


def evaluate_final_performance(data_manager, best_configs):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ€§èƒ½"""
    print("\n" + "="*60)
    print("æµ‹è¯•é›†æœ€ç»ˆæ€§èƒ½è¯„ä¼°")
    print("="*60)
    
    mae_metric = MAE()
    rmse_metric = RMSE()
    
    test_results = {}
    train_data, val_data, test_data = data_manager.get_splits()
    
    loss_functions = {
        'HPL': lambda config: HybridPiecewiseLoss(
            delta1=config['delta1'],
            delta2=config['delta2'], 
            l_max=config.get('l_max', 4.0),
            c_sigmoid=config.get('c_sigmoid', 1.0)
        ),
        'L2': lambda config: L2Loss(),
        'L1': lambda config: L1Loss(epsilon=config.get('l1_epsilon', 1e-8))
    }
    
    for loss_name, config in best_configs.items():
        if config is None:
            continue
            
        print(f"\nè¯„ä¼° {loss_name} åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½...")
        
        try:
            # åˆ›å»ºæ¨¡åž‹
            loss_function = loss_functions[loss_name](config)
            regularizer = L2Regularizer(lambda_reg=config['lambda_reg'])
            
            model = MatrixFactorizationSGD(
                n_users=data_manager.get_statistics()['n_users'],
                n_items=data_manager.get_statistics()['n_items'],
                n_factors=config['latent_factors'],
                learning_rate=config['learning_rate'],
                regularizer=regularizer,
                loss_function=loss_function,
                use_bias=True,
                global_mean=data_manager.global_mean or 0.0
            )
            
            # åˆå§‹åŒ–å¹¶è®­ç»ƒ
            initializer = NormalInitializer(mean=0.0, std=0.01)
            model.initialize_parameters(initializer)
            
            # åœ¨è®­ç»ƒ+éªŒè¯é›†ä¸Šè®­ç»ƒ
            combined_data = np.vstack([train_data, val_data])
            model.fit(
                train_data=combined_data,
                val_data=None,  # ä¸ä½¿ç”¨éªŒè¯é›†ï¼ˆå› ä¸ºè¦åœ¨æµ‹è¯•é›†è¯„ä¼°ï¼‰
                n_epochs=50,
                verbose=0
            )
            
            # æµ‹è¯•é›†é¢„æµ‹
            test_predictions = model.predict(
                test_data[:, 0].astype(int),
                test_data[:, 1].astype(int)
            )
            
            # è¿˜åŽŸåˆ°åŽŸå§‹å°ºåº¦
            if data_manager.global_mean is not None:
                test_predictions += data_manager.global_mean
                test_targets = test_data[:, 2] + data_manager.global_mean
            else:
                test_targets = test_data[:, 2]
            
            # è®¡ç®—æŒ‡æ ‡
            test_mae = mae_metric.calculate(test_targets, test_predictions)
            test_rmse = rmse_metric.calculate(test_targets, test_predictions)
            
            test_results[loss_name] = {
                'mae': test_mae,
                'rmse': test_rmse,
                'config': config
            }
            
            print(f"{loss_name} æµ‹è¯•ç»“æžœ:")
            print(f"  MAE: {test_mae:.4f}")
            print(f"  RMSE: {test_rmse:.4f}")
            
        except Exception as e:
            print(f"{loss_name} æµ‹è¯•è¯„ä¼°å¤±è´¥: {e}")
            test_results[loss_name] = None
    
    # æ±‡æ€»æµ‹è¯•ç»“æžœ
    print(f"\n{'æŸå¤±å‡½æ•°':<12} {'æµ‹è¯•MAE':<12} {'æµ‹è¯•RMSE':<12}")
    print("-" * 40)
    for loss_name, result in test_results.items():
        if result:
            print(f"{loss_name:<12} {result['mae']:<12.4f} {result['rmse']:<12.4f}")
    
    return test_results


def main():
    """HPLä¸“ç”¨MAEä¼˜åŒ–ä¸»å‡½æ•°"""
    try:
        print("HPLæŸå¤±å‡½æ•°MAEæŒ‡æ ‡æ·±åº¦ä¼˜åŒ–å®žéªŒ")
        print("="*60)
        
        # 1. åŸºç¡€HPL MAEä¼˜åŒ–å®žéªŒ
        print("\n1. è¿è¡ŒHPL vs L2 vs L1 MAEå¯¹æ¯”...")
        comparison_results = run_mae_comparison()
        
        # 2. æå–æœ€ä½³é…ç½®
        best_configs = {
            'HPL': comparison_results['HPL']['best_config'],
            'L2': comparison_results['L2']['best_config'], 
            'L1': comparison_results['L1']['best_config']
        }
        
        # 3. æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°
        print("\n2. è¿è¡Œæµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°...")
        data_manager = setup_improved_data()
        test_results = evaluate_final_performance(data_manager, best_configs)
        
        # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        print("\n" + "="*60)
        print("MAEä¼˜åŒ–å®žéªŒæœ€ç»ˆæ€»ç»“")
        print("="*60)
        
        print("\néªŒè¯é›†æœ€ä½³MAEç»“æžœ:")
        if comparison_results['HPL']['best_mae'] != float('inf'):
            print(f"  HPL: {comparison_results['HPL']['best_mae']:.4f}")
        if comparison_results['L2']['best_mae'] != float('inf'):
            print(f"  L2:  {comparison_results['L2']['best_mae']:.4f}")
        if comparison_results['L1']['best_mae'] != float('inf'):
            print(f"  L1:  {comparison_results['L1']['best_mae']:.4f}")
        
        print("\næµ‹è¯•é›†æœ€ç»ˆç»“æžœ:")
        for loss_name, result in test_results.items():
            if result:
                print(f"  {loss_name}: MAE={result['mae']:.4f}, RMSE={result['rmse']:.4f}")
        
        # 5. å…³é”®å‘çŽ°å’Œå»ºè®®
        print("\n" + "="*60)
        print("å…³é”®å‘çŽ°å’Œå»ºè®®")
        print("="*60)
        
        best_val_mae = min(
            comparison_results['HPL']['best_mae'],
            comparison_results['L2']['best_mae'],
            comparison_results['L1']['best_mae']
        )
        
        if best_val_mae == comparison_results['L1']['best_mae']:
            print("âœ… L1æŸå¤±åœ¨MAEä¼˜åŒ–ä¸­è¡¨çŽ°æœ€ä½³ï¼Œè¿™ç¬¦åˆç†è®ºé¢„æœŸ")
            print("   - L1æŸå¤±ç›´æŽ¥ä¼˜åŒ–MAEç›®æ ‡ï¼Œåº”è¯¥æ˜¯æœ€ä¼˜é€‰æ‹©")
            print("   - å»ºè®®åœ¨æ³¨é‡MAEæŒ‡æ ‡çš„åº”ç”¨ä¸­ä½¿ç”¨L1æŸå¤±")
        elif best_val_mae == comparison_results['HPL']['best_mae']:
            print("ðŸŽ‰ HPLæŸå¤±åœ¨MAEä¼˜åŒ–ä¸­è¶…è¶Šäº†L1æŸå¤±!")
            print("   - è¿™è¡¨æ˜ŽHPLçš„åˆ†æ®µç­–ç•¥å¯¹MAEä¼˜åŒ–ä¹Ÿæœ‰å¸®åŠ©")
            print("   - HPLå¯èƒ½åœ¨å¤„ç†ä¸åŒè¯¯å·®èŒƒå›´æ—¶æ¯”ç®€å•L1æ›´æœ‰æ•ˆ")
        else:
            print("ðŸ“Š L2æŸå¤±åœ¨MAEä¼˜åŒ–ä¸­è¡¨çŽ°æ„å¤–åœ°å¥½")
            print("   - å¯èƒ½éœ€è¦è°ƒæ•´å…¶ä»–æŸå¤±å‡½æ•°çš„è¶…å‚æ•°èŒƒå›´")
        
        # 6. é…ç½®å»ºè®®
        print("\né…ç½®å»ºè®®:")
        for loss_name, config in best_configs.items():
            if config:
                print(f"\n{loss_name}æœ€ä½³é…ç½®:")
                print(f"  å­¦ä¹ çŽ‡: {config['learning_rate']:.4f}")
                print(f"  æ½œåœ¨å› å­: {config['latent_factors']}")
                print(f"  æ­£åˆ™åŒ–: {config['lambda_reg']:.6f}")
                if loss_name == 'HPL':
                    print(f"  delta1: {config['delta1']:.3f}")
                    print(f"  delta2: {config['delta2']:.3f}")
                elif loss_name == 'L1':
                    print(f"  epsilon: {config.get('l1_epsilon', 1e-8):.2e}")
        
        print("\nå®žéªŒå®Œæˆï¼è¯¦ç»†ç»“æžœå·²ä¿å­˜åˆ°JSONæ–‡ä»¶ä¸­ã€‚")
        
        return True
        
    except Exception as e:
        print(f"MAEä¼˜åŒ–å®žéªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
